// -----// IR Dump After mlir::iree_compiler::IREE::HAL::AssignTargetDevicesPass (iree-hal-assign-target-devices) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
    %0 = "tosa.const"() <{value = dense<-1.000000e+00> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
    %1 = "tosa.const"() <{value = dense<0.0125187514> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
    %2 = "tosa.const"() <{value = dense<2.560000e+02> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
    %3 = "tosa.const"() <{value = dense<-1.280000e+02> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
    %4 = "tosa.cast"(%arg0) : (tensor<1x2xi8>) -> tensor<1x2xf32>
    %5 = "tosa.sub"(%4, %0) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
    %6 = "tosa.mul"(%5, %1) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
    %7 = "tosa.reduce_max"(%6) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
    %8 = "tosa.sub"(%6, %7) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
    %9 = "tosa.exp"(%8) : (tensor<1x2xf32>) -> tensor<1x2xf32>
    %10 = "tosa.reduce_sum"(%9) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
    %11 = "tosa.reciprocal"(%10) : (tensor<1x1xf32>) -> tensor<1x1xf32>
    %12 = "tosa.mul"(%9, %11) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
    %13 = "tosa.mul"(%12, %2) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
    %14 = "tosa.add"(%13, %3) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
    %15 = "tosa.cast"(%14) : (tensor<1x2xf32>) -> tensor<1x2xi8>
    return %15 : tensor<1x2xi8>
  }
}


// -----// IR Dump After TosaToSCF (tosa-to-scf) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %0 = "tosa.const"() <{value = dense<-1.000000e+00> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
  %1 = "tosa.const"() <{value = dense<0.0125187514> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
  %2 = "tosa.const"() <{value = dense<2.560000e+02> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
  %3 = "tosa.const"() <{value = dense<-1.280000e+02> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
  %4 = "tosa.cast"(%arg0) : (tensor<1x2xi8>) -> tensor<1x2xf32>
  %5 = "tosa.sub"(%4, %0) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %6 = "tosa.mul"(%5, %1) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %7 = "tosa.reduce_max"(%6) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %8 = "tosa.sub"(%6, %7) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %9 = "tosa.exp"(%8) : (tensor<1x2xf32>) -> tensor<1x2xf32>
  %10 = "tosa.reduce_sum"(%9) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %11 = "tosa.reciprocal"(%10) : (tensor<1x1xf32>) -> tensor<1x1xf32>
  %12 = "tosa.mul"(%9, %11) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %13 = "tosa.mul"(%12, %2) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %14 = "tosa.add"(%13, %3) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %15 = "tosa.cast"(%14) : (tensor<1x2xf32>) -> tensor<1x2xi8>
  return %15 : tensor<1x2xi8>
}

// -----// IR Dump After TopLevelSCFToCFG (iree-top-level-scf-to-cfg) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %0 = "tosa.const"() <{value = dense<-1.000000e+00> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
  %1 = "tosa.const"() <{value = dense<0.0125187514> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
  %2 = "tosa.const"() <{value = dense<2.560000e+02> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
  %3 = "tosa.const"() <{value = dense<-1.280000e+02> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
  %4 = "tosa.cast"(%arg0) : (tensor<1x2xi8>) -> tensor<1x2xf32>
  %5 = "tosa.sub"(%4, %0) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %6 = "tosa.mul"(%5, %1) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %7 = "tosa.reduce_max"(%6) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %8 = "tosa.sub"(%6, %7) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %9 = "tosa.exp"(%8) : (tensor<1x2xf32>) -> tensor<1x2xf32>
  %10 = "tosa.reduce_sum"(%9) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %11 = "tosa.reciprocal"(%10) : (tensor<1x1xf32>) -> tensor<1x1xf32>
  %12 = "tosa.mul"(%9, %11) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %13 = "tosa.mul"(%12, %2) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %14 = "tosa.add"(%13, %3) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %15 = "tosa.cast"(%14) : (tensor<1x2xf32>) -> tensor<1x2xi8>
  return %15 : tensor<1x2xi8>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %0 = "tosa.const"() <{value = dense<-1.000000e+00> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
  %1 = "tosa.const"() <{value = dense<0.0125187514> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
  %2 = "tosa.const"() <{value = dense<2.560000e+02> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
  %3 = "tosa.const"() <{value = dense<-1.280000e+02> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
  %4 = "tosa.cast"(%arg0) : (tensor<1x2xi8>) -> tensor<1x2xf32>
  %5 = "tosa.sub"(%4, %0) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %6 = "tosa.mul"(%5, %1) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %7 = "tosa.reduce_max"(%6) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %8 = "tosa.sub"(%6, %7) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %9 = "tosa.exp"(%8) : (tensor<1x2xf32>) -> tensor<1x2xf32>
  %10 = "tosa.reduce_sum"(%9) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %11 = "tosa.reciprocal"(%10) : (tensor<1x1xf32>) -> tensor<1x1xf32>
  %12 = "tosa.mul"(%9, %11) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %13 = "tosa.mul"(%12, %2) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %14 = "tosa.add"(%13, %3) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %15 = "tosa.cast"(%14) : (tensor<1x2xf32>) -> tensor<1x2xi8>
  return %15 : tensor<1x2xi8>
}

// -----// IR Dump After Inliner (inline) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
    %0 = "tosa.const"() <{value = dense<-1.000000e+00> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
    %1 = "tosa.const"() <{value = dense<0.0125187514> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
    %2 = "tosa.const"() <{value = dense<2.560000e+02> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
    %3 = "tosa.const"() <{value = dense<-1.280000e+02> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
    %4 = "tosa.cast"(%arg0) : (tensor<1x2xi8>) -> tensor<1x2xf32>
    %5 = "tosa.sub"(%4, %0) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
    %6 = "tosa.mul"(%5, %1) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
    %7 = "tosa.reduce_max"(%6) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
    %8 = "tosa.sub"(%6, %7) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
    %9 = "tosa.exp"(%8) : (tensor<1x2xf32>) -> tensor<1x2xf32>
    %10 = "tosa.reduce_sum"(%9) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
    %11 = "tosa.reciprocal"(%10) : (tensor<1x1xf32>) -> tensor<1x1xf32>
    %12 = "tosa.mul"(%9, %11) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
    %13 = "tosa.mul"(%12, %2) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
    %14 = "tosa.add"(%13, %3) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
    %15 = "tosa.cast"(%14) : (tensor<1x2xf32>) -> tensor<1x2xi8>
    return %15 : tensor<1x2xi8>
  }
}


// -----// IR Dump After TosaMakeBroadcastable (tosa-make-broadcastable) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %0 = "tosa.const"() <{value = dense<-1.000000e+00> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
  %1 = "tosa.const"() <{value = dense<0.0125187514> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
  %2 = "tosa.const"() <{value = dense<2.560000e+02> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
  %3 = "tosa.const"() <{value = dense<-1.280000e+02> : tensor<1x1xf32>}> : () -> tensor<1x1xf32>
  %4 = "tosa.cast"(%arg0) : (tensor<1x2xi8>) -> tensor<1x2xf32>
  %5 = "tosa.sub"(%4, %0) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %6 = "tosa.mul"(%5, %1) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %7 = "tosa.reduce_max"(%6) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %8 = "tosa.sub"(%6, %7) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %9 = "tosa.exp"(%8) : (tensor<1x2xf32>) -> tensor<1x2xf32>
  %10 = "tosa.reduce_sum"(%9) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %11 = "tosa.reciprocal"(%10) : (tensor<1x1xf32>) -> tensor<1x1xf32>
  %12 = "tosa.mul"(%9, %11) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %13 = "tosa.mul"(%12, %2) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %14 = "tosa.add"(%13, %3) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %15 = "tosa.cast"(%14) : (tensor<1x2xf32>) -> tensor<1x2xi8>
  return %15 : tensor<1x2xi8>
}

// -----// IR Dump After TosaToArith (tosa-to-arith) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1x1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1x1xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<1x1xf32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<1x1xf32>
  %0 = "tosa.cast"(%arg0) : (tensor<1x2xi8>) -> tensor<1x2xf32>
  %1 = "tosa.sub"(%0, %cst) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %2 = "tosa.mul"(%1, %cst_0) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %3 = "tosa.reduce_max"(%2) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %4 = "tosa.sub"(%2, %3) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %5 = "tosa.exp"(%4) : (tensor<1x2xf32>) -> tensor<1x2xf32>
  %6 = "tosa.reduce_sum"(%5) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %7 = "tosa.reciprocal"(%6) : (tensor<1x1xf32>) -> tensor<1x1xf32>
  %8 = "tosa.mul"(%5, %7) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %9 = "tosa.mul"(%8, %cst_1) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %10 = "tosa.add"(%9, %cst_2) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %11 = "tosa.cast"(%10) : (tensor<1x2xf32>) -> tensor<1x2xi8>
  return %11 : tensor<1x2xi8>
}

// -----// IR Dump After TosaToTensor (tosa-to-tensor) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1x1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1x1xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<1x1xf32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<1x1xf32>
  %0 = "tosa.cast"(%arg0) : (tensor<1x2xi8>) -> tensor<1x2xf32>
  %1 = "tosa.sub"(%0, %cst) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %2 = "tosa.mul"(%1, %cst_0) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %3 = "tosa.reduce_max"(%2) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %4 = "tosa.sub"(%2, %3) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %5 = "tosa.exp"(%4) : (tensor<1x2xf32>) -> tensor<1x2xf32>
  %6 = "tosa.reduce_sum"(%5) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %7 = "tosa.reciprocal"(%6) : (tensor<1x1xf32>) -> tensor<1x1xf32>
  %8 = "tosa.mul"(%5, %7) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %9 = "tosa.mul"(%8, %cst_1) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %10 = "tosa.add"(%9, %cst_2) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %11 = "tosa.cast"(%10) : (tensor<1x2xf32>) -> tensor<1x2xi8>
  return %11 : tensor<1x2xi8>
}

// -----// IR Dump After TosaToLinalgExt (iree-tosa-to-linalg-ext) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1x1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1x1xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<1x1xf32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<1x1xf32>
  %0 = "tosa.cast"(%arg0) : (tensor<1x2xi8>) -> tensor<1x2xf32>
  %1 = "tosa.sub"(%0, %cst) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %2 = "tosa.mul"(%1, %cst_0) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %3 = "tosa.reduce_max"(%2) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %4 = "tosa.sub"(%2, %3) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %5 = "tosa.exp"(%4) : (tensor<1x2xf32>) -> tensor<1x2xf32>
  %6 = "tosa.reduce_sum"(%5) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %7 = "tosa.reciprocal"(%6) : (tensor<1x1xf32>) -> tensor<1x1xf32>
  %8 = "tosa.mul"(%5, %7) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %9 = "tosa.mul"(%8, %cst_1) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %10 = "tosa.add"(%9, %cst_2) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %11 = "tosa.cast"(%10) : (tensor<1x2xf32>) -> tensor<1x2xi8>
  return %11 : tensor<1x2xi8>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1x1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1x1xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<1x1xf32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<1x1xf32>
  %0 = "tosa.cast"(%arg0) : (tensor<1x2xi8>) -> tensor<1x2xf32>
  %1 = "tosa.sub"(%0, %cst) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %2 = "tosa.mul"(%1, %cst_0) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %3 = "tosa.reduce_max"(%2) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %4 = "tosa.sub"(%2, %3) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %5 = "tosa.exp"(%4) : (tensor<1x2xf32>) -> tensor<1x2xf32>
  %6 = "tosa.reduce_sum"(%5) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %7 = "tosa.reciprocal"(%6) : (tensor<1x1xf32>) -> tensor<1x1xf32>
  %8 = "tosa.mul"(%5, %7) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %9 = "tosa.mul"(%8, %cst_1) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %10 = "tosa.add"(%9, %cst_2) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %11 = "tosa.cast"(%10) : (tensor<1x2xf32>) -> tensor<1x2xi8>
  return %11 : tensor<1x2xi8>
}

// -----// IR Dump After TosaOptionalDecompositions (tosa-optional-decompositions) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1x1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1x1xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<1x1xf32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<1x1xf32>
  %0 = "tosa.cast"(%arg0) : (tensor<1x2xi8>) -> tensor<1x2xf32>
  %1 = "tosa.sub"(%0, %cst) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %2 = "tosa.mul"(%1, %cst_0) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %3 = "tosa.reduce_max"(%2) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %4 = "tosa.sub"(%2, %3) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %5 = "tosa.exp"(%4) : (tensor<1x2xf32>) -> tensor<1x2xf32>
  %6 = "tosa.reduce_sum"(%5) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %7 = "tosa.reciprocal"(%6) : (tensor<1x1xf32>) -> tensor<1x1xf32>
  %8 = "tosa.mul"(%5, %7) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %9 = "tosa.mul"(%8, %cst_1) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %10 = "tosa.add"(%9, %cst_2) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %11 = "tosa.cast"(%10) : (tensor<1x2xf32>) -> tensor<1x2xi8>
  return %11 : tensor<1x2xi8>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1x1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1x1xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<1x1xf32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<1x1xf32>
  %0 = "tosa.cast"(%arg0) : (tensor<1x2xi8>) -> tensor<1x2xf32>
  %1 = "tosa.sub"(%0, %cst) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %2 = "tosa.mul"(%1, %cst_0) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %3 = "tosa.reduce_max"(%2) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %4 = "tosa.sub"(%2, %3) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %5 = "tosa.exp"(%4) : (tensor<1x2xf32>) -> tensor<1x2xf32>
  %6 = "tosa.reduce_sum"(%5) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %7 = "tosa.reciprocal"(%6) : (tensor<1x1xf32>) -> tensor<1x1xf32>
  %8 = "tosa.mul"(%5, %7) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %9 = "tosa.mul"(%8, %cst_1) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %10 = "tosa.add"(%9, %cst_2) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %11 = "tosa.cast"(%10) : (tensor<1x2xf32>) -> tensor<1x2xi8>
  return %11 : tensor<1x2xi8>
}

// -----// IR Dump After TosaInferShapes (tosa-infer-shapes) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1x1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1x1xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<1x1xf32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<1x1xf32>
  %0 = "tosa.cast"(%arg0) : (tensor<1x2xi8>) -> tensor<1x2xf32>
  %1 = "tosa.sub"(%0, %cst) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %2 = "tosa.mul"(%1, %cst_0) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %3 = "tosa.reduce_max"(%2) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %4 = "tosa.sub"(%2, %3) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %5 = "tosa.exp"(%4) : (tensor<1x2xf32>) -> tensor<1x2xf32>
  %6 = "tosa.reduce_sum"(%5) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %7 = "tosa.reciprocal"(%6) : (tensor<1x1xf32>) -> tensor<1x1xf32>
  %8 = "tosa.mul"(%5, %7) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %9 = "tosa.mul"(%8, %cst_1) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %10 = "tosa.add"(%9, %cst_2) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %11 = "tosa.cast"(%10) : (tensor<1x2xf32>) -> tensor<1x2xi8>
  return %11 : tensor<1x2xi8>
}

// -----// IR Dump After TosaMakeBroadcastable (tosa-make-broadcastable) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1x1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1x1xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<1x1xf32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<1x1xf32>
  %0 = "tosa.cast"(%arg0) : (tensor<1x2xi8>) -> tensor<1x2xf32>
  %1 = "tosa.sub"(%0, %cst) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %2 = "tosa.mul"(%1, %cst_0) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %3 = "tosa.reduce_max"(%2) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %4 = "tosa.sub"(%2, %3) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %5 = "tosa.exp"(%4) : (tensor<1x2xf32>) -> tensor<1x2xf32>
  %6 = "tosa.reduce_sum"(%5) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %7 = "tosa.reciprocal"(%6) : (tensor<1x1xf32>) -> tensor<1x1xf32>
  %8 = "tosa.mul"(%5, %7) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %9 = "tosa.mul"(%8, %cst_1) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %10 = "tosa.add"(%9, %cst_2) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %11 = "tosa.cast"(%10) : (tensor<1x2xf32>) -> tensor<1x2xi8>
  return %11 : tensor<1x2xi8>
}

// -----// IR Dump After TosaToLinalgNamed (tosa-to-linalg-named) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1x1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1x1xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<1x1xf32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<1x1xf32>
  %0 = "tosa.cast"(%arg0) : (tensor<1x2xi8>) -> tensor<1x2xf32>
  %1 = "tosa.sub"(%0, %cst) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %2 = "tosa.mul"(%1, %cst_0) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %3 = "tosa.reduce_max"(%2) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %4 = "tosa.sub"(%2, %3) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %5 = "tosa.exp"(%4) : (tensor<1x2xf32>) -> tensor<1x2xf32>
  %6 = "tosa.reduce_sum"(%5) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %7 = "tosa.reciprocal"(%6) : (tensor<1x1xf32>) -> tensor<1x1xf32>
  %8 = "tosa.mul"(%5, %7) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %9 = "tosa.mul"(%8, %cst_1) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %10 = "tosa.add"(%9, %cst_2) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %11 = "tosa.cast"(%10) : (tensor<1x2xf32>) -> tensor<1x2xi8>
  return %11 : tensor<1x2xi8>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1x1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1x1xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<1x1xf32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<1x1xf32>
  %0 = "tosa.cast"(%arg0) : (tensor<1x2xi8>) -> tensor<1x2xf32>
  %1 = "tosa.sub"(%0, %cst) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %2 = "tosa.mul"(%1, %cst_0) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %3 = "tosa.reduce_max"(%2) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %4 = "tosa.sub"(%2, %3) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %5 = "tosa.exp"(%4) : (tensor<1x2xf32>) -> tensor<1x2xf32>
  %6 = "tosa.reduce_sum"(%5) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %7 = "tosa.reciprocal"(%6) : (tensor<1x1xf32>) -> tensor<1x1xf32>
  %8 = "tosa.mul"(%5, %7) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %9 = "tosa.mul"(%8, %cst_1) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %10 = "tosa.add"(%9, %cst_2) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %11 = "tosa.cast"(%10) : (tensor<1x2xf32>) -> tensor<1x2xi8>
  return %11 : tensor<1x2xi8>
}

// -----// IR Dump After TosaLayerwiseConstantFoldPass (tosa-layerwise-constant-fold) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1x1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1x1xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<1x1xf32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<1x1xf32>
  %0 = "tosa.cast"(%arg0) : (tensor<1x2xi8>) -> tensor<1x2xf32>
  %1 = "tosa.sub"(%0, %cst) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %2 = "tosa.mul"(%1, %cst_0) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %3 = "tosa.reduce_max"(%2) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %4 = "tosa.sub"(%2, %3) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %5 = "tosa.exp"(%4) : (tensor<1x2xf32>) -> tensor<1x2xf32>
  %6 = "tosa.reduce_sum"(%5) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %7 = "tosa.reciprocal"(%6) : (tensor<1x1xf32>) -> tensor<1x1xf32>
  %8 = "tosa.mul"(%5, %7) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %9 = "tosa.mul"(%8, %cst_1) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %10 = "tosa.add"(%9, %cst_2) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %11 = "tosa.cast"(%10) : (tensor<1x2xf32>) -> tensor<1x2xi8>
  return %11 : tensor<1x2xi8>
}

// -----// IR Dump After TosaMakeBroadcastable (tosa-make-broadcastable) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1x1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1x1xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<1x1xf32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<1x1xf32>
  %0 = "tosa.cast"(%arg0) : (tensor<1x2xi8>) -> tensor<1x2xf32>
  %1 = "tosa.sub"(%0, %cst) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %2 = "tosa.mul"(%1, %cst_0) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %3 = "tosa.reduce_max"(%2) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %4 = "tosa.sub"(%2, %3) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %5 = "tosa.exp"(%4) : (tensor<1x2xf32>) -> tensor<1x2xf32>
  %6 = "tosa.reduce_sum"(%5) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %7 = "tosa.reciprocal"(%6) : (tensor<1x1xf32>) -> tensor<1x1xf32>
  %8 = "tosa.mul"(%5, %7) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %9 = "tosa.mul"(%8, %cst_1) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %10 = "tosa.add"(%9, %cst_2) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %11 = "tosa.cast"(%10) : (tensor<1x2xf32>) -> tensor<1x2xi8>
  return %11 : tensor<1x2xi8>
}

// -----// IR Dump After TosaValidation (tosa-validate) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1x1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1x1xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<1x1xf32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<1x1xf32>
  %0 = "tosa.cast"(%arg0) : (tensor<1x2xi8>) -> tensor<1x2xf32>
  %1 = "tosa.sub"(%0, %cst) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %2 = "tosa.mul"(%1, %cst_0) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %3 = "tosa.reduce_max"(%2) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %4 = "tosa.sub"(%2, %3) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %5 = "tosa.exp"(%4) : (tensor<1x2xf32>) -> tensor<1x2xf32>
  %6 = "tosa.reduce_sum"(%5) <{axis = 1 : i64}> : (tensor<1x2xf32>) -> tensor<1x1xf32>
  %7 = "tosa.reciprocal"(%6) : (tensor<1x1xf32>) -> tensor<1x1xf32>
  %8 = "tosa.mul"(%5, %7) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %9 = "tosa.mul"(%8, %cst_1) <{shift = 0 : i32}> : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %10 = "tosa.add"(%9, %cst_2) : (tensor<1x2xf32>, tensor<1x1xf32>) -> tensor<1x2xf32>
  %11 = "tosa.cast"(%10) : (tensor<1x2xf32>) -> tensor<1x2xi8>
  return %11 : tensor<1x2xi8>
}

// -----// IR Dump After TosaToLinalg (tosa-to-linalg) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1x1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1x1xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<1x1xf32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<1x1xf32>
  %0 = tensor.empty() : tensor<1x2xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<1x2xi8>) outs(%0 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %32 = arith.sitofp %in : i8 to f32
    linalg.yield %32 : f32
  } -> tensor<1x2xf32>
  %2 = tensor.empty() : tensor<1x2xf32>
  %3 = "tosa.reshape"(%cst) <{new_shape = array<i64: 1>}> : (tensor<1x1xf32>) -> tensor<1xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%1, %3 : tensor<1x2xf32>, tensor<1xf32>) outs(%2 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_6: f32, %out: f32):
    %32 = arith.subf %in, %in_6 : f32
    linalg.yield %32 : f32
  } -> tensor<1x2xf32>
  %5 = tensor.empty() : tensor<1x2xf32>
  %6 = "tosa.reshape"(%cst_0) <{new_shape = array<i64: 1>}> : (tensor<1x1xf32>) -> tensor<1xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %6 : tensor<1x2xf32>, tensor<1xf32>) outs(%5 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_6: f32, %out: f32):
    %32 = arith.mulf %in, %in_6 : f32
    linalg.yield %32 : f32
  } -> tensor<1x2xf32>
  %8 = tensor.empty() : tensor<1xf32>
  %cst_3 = arith.constant -3.40282347E+38 : f32
  %9 = linalg.fill ins(%cst_3 : f32) outs(%8 : tensor<1xf32>) -> tensor<1xf32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%7 : tensor<1x2xf32>) outs(%9 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %32 = arith.maxf %in, %out : f32
    linalg.yield %32 : f32
  } -> tensor<1xf32>
  %expanded = tensor.expand_shape %10 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %11 = tensor.empty() : tensor<1x2xf32>
  %12 = "tosa.reshape"(%expanded) <{new_shape = array<i64: 1>}> : (tensor<1x1xf32>) -> tensor<1xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%7, %12 : tensor<1x2xf32>, tensor<1xf32>) outs(%11 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_6: f32, %out: f32):
    %32 = arith.subf %in, %in_6 : f32
    linalg.yield %32 : f32
  } -> tensor<1x2xf32>
  %14 = tensor.empty() : tensor<1x2xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%13 : tensor<1x2xf32>) outs(%14 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %32 = math.exp %in : f32
    linalg.yield %32 : f32
  } -> tensor<1x2xf32>
  %16 = tensor.empty() : tensor<1xf32>
  %cst_4 = arith.constant 0.000000e+00 : f32
  %17 = linalg.fill ins(%cst_4 : f32) outs(%16 : tensor<1xf32>) -> tensor<1xf32>
  %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%15 : tensor<1x2xf32>) outs(%17 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %32 = arith.addf %in, %out : f32
    linalg.yield %32 : f32
  } -> tensor<1xf32>
  %expanded_5 = tensor.expand_shape %18 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %19 = tensor.empty() : tensor<1x1xf32>
  %20 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded_5 : tensor<1x1xf32>) outs(%19 : tensor<1x1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %cst_6 = arith.constant 1.000000e+00 : f32
    %32 = arith.divf %cst_6, %in : f32
    linalg.yield %32 : f32
  } -> tensor<1x1xf32>
  %21 = tensor.empty() : tensor<1x2xf32>
  %22 = "tosa.reshape"(%20) <{new_shape = array<i64: 1>}> : (tensor<1x1xf32>) -> tensor<1xf32>
  %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%15, %22 : tensor<1x2xf32>, tensor<1xf32>) outs(%21 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_6: f32, %out: f32):
    %32 = arith.mulf %in, %in_6 : f32
    linalg.yield %32 : f32
  } -> tensor<1x2xf32>
  %24 = tensor.empty() : tensor<1x2xf32>
  %25 = "tosa.reshape"(%cst_1) <{new_shape = array<i64: 1>}> : (tensor<1x1xf32>) -> tensor<1xf32>
  %26 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%23, %25 : tensor<1x2xf32>, tensor<1xf32>) outs(%24 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_6: f32, %out: f32):
    %32 = arith.mulf %in, %in_6 : f32
    linalg.yield %32 : f32
  } -> tensor<1x2xf32>
  %27 = tensor.empty() : tensor<1x2xf32>
  %28 = "tosa.reshape"(%cst_2) <{new_shape = array<i64: 1>}> : (tensor<1x1xf32>) -> tensor<1xf32>
  %29 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%26, %28 : tensor<1x2xf32>, tensor<1xf32>) outs(%27 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_6: f32, %out: f32):
    %32 = arith.addf %in, %in_6 : f32
    linalg.yield %32 : f32
  } -> tensor<1x2xf32>
  %30 = tensor.empty() : tensor<1x2xi8>
  %31 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%29 : tensor<1x2xf32>) outs(%30 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %cst_6 = arith.constant -1.280000e+02 : f32
    %cst_7 = arith.constant 1.270000e+02 : f32
    %32 = math.roundeven %in : f32
    %33 = arith.minf %32, %cst_7 : f32
    %34 = arith.maxf %33, %cst_6 : f32
    %35 = arith.fptosi %34 : f32 to i8
    linalg.yield %35 : i8
  } -> tensor<1x2xi8>
  return %31 : tensor<1x2xi8>
}

// -----// IR Dump After TosaToArith (tosa-to-arith) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1x1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1x1xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<1x1xf32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<1x1xf32>
  %0 = tensor.empty() : tensor<1x2xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<1x2xi8>) outs(%0 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %28 = arith.sitofp %in : i8 to f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %2 = tensor.empty() : tensor<1x2xf32>
  %cst_3 = arith.constant dense<-1.000000e+00> : tensor<1xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%1, %cst_3 : tensor<1x2xf32>, tensor<1xf32>) outs(%2 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_10: f32, %out: f32):
    %28 = arith.subf %in, %in_10 : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %4 = tensor.empty() : tensor<1x2xf32>
  %cst_4 = arith.constant dense<0.0125187514> : tensor<1xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%4 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_10: f32, %out: f32):
    %28 = arith.mulf %in, %in_10 : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %6 = tensor.empty() : tensor<1xf32>
  %cst_5 = arith.constant -3.40282347E+38 : f32
  %7 = linalg.fill ins(%cst_5 : f32) outs(%6 : tensor<1xf32>) -> tensor<1xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%5 : tensor<1x2xf32>) outs(%7 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = arith.maxf %in, %out : f32
    linalg.yield %28 : f32
  } -> tensor<1xf32>
  %expanded = tensor.expand_shape %8 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %9 = tensor.empty() : tensor<1x2xf32>
  %10 = "tosa.reshape"(%expanded) <{new_shape = array<i64: 1>}> : (tensor<1x1xf32>) -> tensor<1xf32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %10 : tensor<1x2xf32>, tensor<1xf32>) outs(%9 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_10: f32, %out: f32):
    %28 = arith.subf %in, %in_10 : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %12 = tensor.empty() : tensor<1x2xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%11 : tensor<1x2xf32>) outs(%12 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = math.exp %in : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %14 = tensor.empty() : tensor<1xf32>
  %cst_6 = arith.constant 0.000000e+00 : f32
  %15 = linalg.fill ins(%cst_6 : f32) outs(%14 : tensor<1xf32>) -> tensor<1xf32>
  %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%13 : tensor<1x2xf32>) outs(%15 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = arith.addf %in, %out : f32
    linalg.yield %28 : f32
  } -> tensor<1xf32>
  %expanded_7 = tensor.expand_shape %16 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %17 = tensor.empty() : tensor<1x1xf32>
  %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded_7 : tensor<1x1xf32>) outs(%17 : tensor<1x1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %cst_10 = arith.constant 1.000000e+00 : f32
    %28 = arith.divf %cst_10, %in : f32
    linalg.yield %28 : f32
  } -> tensor<1x1xf32>
  %19 = tensor.empty() : tensor<1x2xf32>
  %20 = "tosa.reshape"(%18) <{new_shape = array<i64: 1>}> : (tensor<1x1xf32>) -> tensor<1xf32>
  %21 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%13, %20 : tensor<1x2xf32>, tensor<1xf32>) outs(%19 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_10: f32, %out: f32):
    %28 = arith.mulf %in, %in_10 : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %22 = tensor.empty() : tensor<1x2xf32>
  %cst_8 = arith.constant dense<2.560000e+02> : tensor<1xf32>
  %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%21, %cst_8 : tensor<1x2xf32>, tensor<1xf32>) outs(%22 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_10: f32, %out: f32):
    %28 = arith.mulf %in, %in_10 : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %24 = tensor.empty() : tensor<1x2xf32>
  %cst_9 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
  %25 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%23, %cst_9 : tensor<1x2xf32>, tensor<1xf32>) outs(%24 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_10: f32, %out: f32):
    %28 = arith.addf %in, %in_10 : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %26 = tensor.empty() : tensor<1x2xi8>
  %27 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%25 : tensor<1x2xf32>) outs(%26 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %cst_10 = arith.constant -1.280000e+02 : f32
    %cst_11 = arith.constant 1.270000e+02 : f32
    %28 = math.roundeven %in : f32
    %29 = arith.minf %28, %cst_11 : f32
    %30 = arith.maxf %29, %cst_10 : f32
    %31 = arith.fptosi %30 : f32 to i8
    linalg.yield %31 : i8
  } -> tensor<1x2xi8>
  return %27 : tensor<1x2xi8>
}

// -----// IR Dump After TosaToTensor (tosa-to-tensor) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1x1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1x1xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<1x1xf32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<1x1xf32>
  %0 = tensor.empty() : tensor<1x2xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<1x2xi8>) outs(%0 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %26 = arith.sitofp %in : i8 to f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %2 = tensor.empty() : tensor<1x2xf32>
  %cst_3 = arith.constant dense<-1.000000e+00> : tensor<1xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%1, %cst_3 : tensor<1x2xf32>, tensor<1xf32>) outs(%2 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_11: f32, %out: f32):
    %26 = arith.subf %in, %in_11 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %4 = tensor.empty() : tensor<1x2xf32>
  %cst_4 = arith.constant dense<0.0125187514> : tensor<1xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%4 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_11: f32, %out: f32):
    %26 = arith.mulf %in, %in_11 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %6 = tensor.empty() : tensor<1xf32>
  %cst_5 = arith.constant -3.40282347E+38 : f32
  %7 = linalg.fill ins(%cst_5 : f32) outs(%6 : tensor<1xf32>) -> tensor<1xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%5 : tensor<1x2xf32>) outs(%7 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.maxf %in, %out : f32
    linalg.yield %26 : f32
  } -> tensor<1xf32>
  %expanded = tensor.expand_shape %8 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %9 = tensor.empty() : tensor<1x2xf32>
  %collapsed = tensor.collapse_shape %expanded [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%9 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_11: f32, %out: f32):
    %26 = arith.subf %in, %in_11 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %11 = tensor.empty() : tensor<1x2xf32>
  %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%10 : tensor<1x2xf32>) outs(%11 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = math.exp %in : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %13 = tensor.empty() : tensor<1xf32>
  %cst_6 = arith.constant 0.000000e+00 : f32
  %14 = linalg.fill ins(%cst_6 : f32) outs(%13 : tensor<1xf32>) -> tensor<1xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x2xf32>) outs(%14 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.addf %in, %out : f32
    linalg.yield %26 : f32
  } -> tensor<1xf32>
  %expanded_7 = tensor.expand_shape %15 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %16 = tensor.empty() : tensor<1x1xf32>
  %17 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded_7 : tensor<1x1xf32>) outs(%16 : tensor<1x1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %cst_11 = arith.constant 1.000000e+00 : f32
    %26 = arith.divf %cst_11, %in : f32
    linalg.yield %26 : f32
  } -> tensor<1x1xf32>
  %18 = tensor.empty() : tensor<1x2xf32>
  %collapsed_8 = tensor.collapse_shape %17 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
  %19 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %collapsed_8 : tensor<1x2xf32>, tensor<1xf32>) outs(%18 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_11: f32, %out: f32):
    %26 = arith.mulf %in, %in_11 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %20 = tensor.empty() : tensor<1x2xf32>
  %cst_9 = arith.constant dense<2.560000e+02> : tensor<1xf32>
  %21 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%19, %cst_9 : tensor<1x2xf32>, tensor<1xf32>) outs(%20 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_11: f32, %out: f32):
    %26 = arith.mulf %in, %in_11 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %22 = tensor.empty() : tensor<1x2xf32>
  %cst_10 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
  %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%21, %cst_10 : tensor<1x2xf32>, tensor<1xf32>) outs(%22 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_11: f32, %out: f32):
    %26 = arith.addf %in, %in_11 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %24 = tensor.empty() : tensor<1x2xi8>
  %25 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%23 : tensor<1x2xf32>) outs(%24 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %cst_11 = arith.constant -1.280000e+02 : f32
    %cst_12 = arith.constant 1.270000e+02 : f32
    %26 = math.roundeven %in : f32
    %27 = arith.minf %26, %cst_12 : f32
    %28 = arith.maxf %27, %cst_11 : f32
    %29 = arith.fptosi %28 : f32 to i8
    linalg.yield %29 : i8
  } -> tensor<1x2xi8>
  return %25 : tensor<1x2xi8>
}

// -----// IR Dump After StripSignedness (iree-flow-strip-signedness) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1x1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1x1xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<1x1xf32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<1x1xf32>
  %0 = tensor.empty() : tensor<1x2xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<1x2xi8>) outs(%0 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %26 = arith.sitofp %in : i8 to f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %2 = tensor.empty() : tensor<1x2xf32>
  %cst_3 = arith.constant dense<-1.000000e+00> : tensor<1xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%1, %cst_3 : tensor<1x2xf32>, tensor<1xf32>) outs(%2 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_11: f32, %out: f32):
    %26 = arith.subf %in, %in_11 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %4 = tensor.empty() : tensor<1x2xf32>
  %cst_4 = arith.constant dense<0.0125187514> : tensor<1xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%4 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_11: f32, %out: f32):
    %26 = arith.mulf %in, %in_11 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %6 = tensor.empty() : tensor<1xf32>
  %cst_5 = arith.constant -3.40282347E+38 : f32
  %7 = linalg.fill ins(%cst_5 : f32) outs(%6 : tensor<1xf32>) -> tensor<1xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%5 : tensor<1x2xf32>) outs(%7 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.maxf %in, %out : f32
    linalg.yield %26 : f32
  } -> tensor<1xf32>
  %expanded = tensor.expand_shape %8 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %9 = tensor.empty() : tensor<1x2xf32>
  %collapsed = tensor.collapse_shape %expanded [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%9 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_11: f32, %out: f32):
    %26 = arith.subf %in, %in_11 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %11 = tensor.empty() : tensor<1x2xf32>
  %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%10 : tensor<1x2xf32>) outs(%11 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = math.exp %in : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %13 = tensor.empty() : tensor<1xf32>
  %cst_6 = arith.constant 0.000000e+00 : f32
  %14 = linalg.fill ins(%cst_6 : f32) outs(%13 : tensor<1xf32>) -> tensor<1xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x2xf32>) outs(%14 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.addf %in, %out : f32
    linalg.yield %26 : f32
  } -> tensor<1xf32>
  %expanded_7 = tensor.expand_shape %15 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %16 = tensor.empty() : tensor<1x1xf32>
  %17 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded_7 : tensor<1x1xf32>) outs(%16 : tensor<1x1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %cst_11 = arith.constant 1.000000e+00 : f32
    %26 = arith.divf %cst_11, %in : f32
    linalg.yield %26 : f32
  } -> tensor<1x1xf32>
  %18 = tensor.empty() : tensor<1x2xf32>
  %collapsed_8 = tensor.collapse_shape %17 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
  %19 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %collapsed_8 : tensor<1x2xf32>, tensor<1xf32>) outs(%18 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_11: f32, %out: f32):
    %26 = arith.mulf %in, %in_11 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %20 = tensor.empty() : tensor<1x2xf32>
  %cst_9 = arith.constant dense<2.560000e+02> : tensor<1xf32>
  %21 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%19, %cst_9 : tensor<1x2xf32>, tensor<1xf32>) outs(%20 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_11: f32, %out: f32):
    %26 = arith.mulf %in, %in_11 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %22 = tensor.empty() : tensor<1x2xf32>
  %cst_10 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
  %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%21, %cst_10 : tensor<1x2xf32>, tensor<1xf32>) outs(%22 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_11: f32, %out: f32):
    %26 = arith.addf %in, %in_11 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %24 = tensor.empty() : tensor<1x2xi8>
  %25 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%23 : tensor<1x2xf32>) outs(%24 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %cst_11 = arith.constant -1.280000e+02 : f32
    %cst_12 = arith.constant 1.270000e+02 : f32
    %26 = math.roundeven %in : f32
    %27 = arith.minf %26, %cst_12 : f32
    %28 = arith.maxf %27, %cst_11 : f32
    %29 = arith.fptosi %28 : f32 to i8
    linalg.yield %29 : i8
  } -> tensor<1x2xi8>
  return %25 : tensor<1x2xi8>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant 1.270000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
  %cst_2 = arith.constant dense<2.560000e+02> : tensor<1xf32>
  %cst_3 = arith.constant 1.000000e+00 : f32
  %cst_4 = arith.constant 0.000000e+00 : f32
  %cst_5 = arith.constant -3.40282347E+38 : f32
  %cst_6 = arith.constant dense<0.0125187514> : tensor<1xf32>
  %cst_7 = arith.constant dense<-1.000000e+00> : tensor<1xf32>
  %0 = tensor.empty() : tensor<1x2xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<1x2xi8>) outs(%0 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %26 = arith.sitofp %in : i8 to f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %2 = tensor.empty() : tensor<1x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%1, %cst_7 : tensor<1x2xf32>, tensor<1xf32>) outs(%2 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.subf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %4 = tensor.empty() : tensor<1x2xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_6 : tensor<1x2xf32>, tensor<1xf32>) outs(%4 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.mulf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %6 = tensor.empty() : tensor<1xf32>
  %7 = linalg.fill ins(%cst_5 : f32) outs(%6 : tensor<1xf32>) -> tensor<1xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%5 : tensor<1x2xf32>) outs(%7 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.maxf %in, %out : f32
    linalg.yield %26 : f32
  } -> tensor<1xf32>
  %9 = tensor.empty() : tensor<1x2xf32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %8 : tensor<1x2xf32>, tensor<1xf32>) outs(%9 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.subf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %11 = tensor.empty() : tensor<1x2xf32>
  %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%10 : tensor<1x2xf32>) outs(%11 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = math.exp %in : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %13 = tensor.empty() : tensor<1xf32>
  %14 = linalg.fill ins(%cst_4 : f32) outs(%13 : tensor<1xf32>) -> tensor<1xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x2xf32>) outs(%14 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.addf %in, %out : f32
    linalg.yield %26 : f32
  } -> tensor<1xf32>
  %expanded = tensor.expand_shape %15 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %16 = tensor.empty() : tensor<1x1xf32>
  %17 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%16 : tensor<1x1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.divf %cst_3, %in : f32
    linalg.yield %26 : f32
  } -> tensor<1x1xf32>
  %18 = tensor.empty() : tensor<1x2xf32>
  %collapsed = tensor.collapse_shape %17 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
  %19 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%18 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.mulf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %20 = tensor.empty() : tensor<1x2xf32>
  %21 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%19, %cst_2 : tensor<1x2xf32>, tensor<1xf32>) outs(%20 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.mulf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %22 = tensor.empty() : tensor<1x2xf32>
  %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%21, %cst_1 : tensor<1x2xf32>, tensor<1xf32>) outs(%22 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.addf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %24 = tensor.empty() : tensor<1x2xi8>
  %25 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%23 : tensor<1x2xf32>) outs(%24 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %26 = math.roundeven %in : f32
    %27 = arith.minf %26, %cst : f32
    %28 = arith.maxf %27, %cst_0 : f32
    %29 = arith.fptosi %28 : f32 to i8
    linalg.yield %29 : i8
  } -> tensor<1x2xi8>
  return %25 : tensor<1x2xi8>
}

// -----// IR Dump After LinalgQuantizedMatmulToMatmulPass (iree-linalg-quantized-matmul-to-matmul) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant 1.270000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
  %cst_2 = arith.constant dense<2.560000e+02> : tensor<1xf32>
  %cst_3 = arith.constant 1.000000e+00 : f32
  %cst_4 = arith.constant 0.000000e+00 : f32
  %cst_5 = arith.constant -3.40282347E+38 : f32
  %cst_6 = arith.constant dense<0.0125187514> : tensor<1xf32>
  %cst_7 = arith.constant dense<-1.000000e+00> : tensor<1xf32>
  %0 = tensor.empty() : tensor<1x2xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<1x2xi8>) outs(%0 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %26 = arith.sitofp %in : i8 to f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %2 = tensor.empty() : tensor<1x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%1, %cst_7 : tensor<1x2xf32>, tensor<1xf32>) outs(%2 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.subf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %4 = tensor.empty() : tensor<1x2xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_6 : tensor<1x2xf32>, tensor<1xf32>) outs(%4 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.mulf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %6 = tensor.empty() : tensor<1xf32>
  %7 = linalg.fill ins(%cst_5 : f32) outs(%6 : tensor<1xf32>) -> tensor<1xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%5 : tensor<1x2xf32>) outs(%7 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.maxf %in, %out : f32
    linalg.yield %26 : f32
  } -> tensor<1xf32>
  %9 = tensor.empty() : tensor<1x2xf32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %8 : tensor<1x2xf32>, tensor<1xf32>) outs(%9 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.subf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %11 = tensor.empty() : tensor<1x2xf32>
  %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%10 : tensor<1x2xf32>) outs(%11 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = math.exp %in : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %13 = tensor.empty() : tensor<1xf32>
  %14 = linalg.fill ins(%cst_4 : f32) outs(%13 : tensor<1xf32>) -> tensor<1xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x2xf32>) outs(%14 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.addf %in, %out : f32
    linalg.yield %26 : f32
  } -> tensor<1xf32>
  %expanded = tensor.expand_shape %15 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %16 = tensor.empty() : tensor<1x1xf32>
  %17 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%16 : tensor<1x1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.divf %cst_3, %in : f32
    linalg.yield %26 : f32
  } -> tensor<1x1xf32>
  %18 = tensor.empty() : tensor<1x2xf32>
  %collapsed = tensor.collapse_shape %17 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
  %19 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%18 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.mulf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %20 = tensor.empty() : tensor<1x2xf32>
  %21 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%19, %cst_2 : tensor<1x2xf32>, tensor<1xf32>) outs(%20 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.mulf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %22 = tensor.empty() : tensor<1x2xf32>
  %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%21, %cst_1 : tensor<1x2xf32>, tensor<1xf32>) outs(%22 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.addf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %24 = tensor.empty() : tensor<1x2xi8>
  %25 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%23 : tensor<1x2xf32>) outs(%24 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %26 = math.roundeven %in : f32
    %27 = arith.minf %26, %cst : f32
    %28 = arith.maxf %27, %cst_0 : f32
    %29 = arith.fptosi %28 : f32 to i8
    linalg.yield %29 : i8
  } -> tensor<1x2xi8>
  return %25 : tensor<1x2xi8>
}

// -----// IR Dump After LinalgQuantizedConvToConvPass (iree-linalg-quantized-conv-to-conv) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant 1.270000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
  %cst_2 = arith.constant dense<2.560000e+02> : tensor<1xf32>
  %cst_3 = arith.constant 1.000000e+00 : f32
  %cst_4 = arith.constant 0.000000e+00 : f32
  %cst_5 = arith.constant -3.40282347E+38 : f32
  %cst_6 = arith.constant dense<0.0125187514> : tensor<1xf32>
  %cst_7 = arith.constant dense<-1.000000e+00> : tensor<1xf32>
  %0 = tensor.empty() : tensor<1x2xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<1x2xi8>) outs(%0 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %26 = arith.sitofp %in : i8 to f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %2 = tensor.empty() : tensor<1x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%1, %cst_7 : tensor<1x2xf32>, tensor<1xf32>) outs(%2 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.subf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %4 = tensor.empty() : tensor<1x2xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_6 : tensor<1x2xf32>, tensor<1xf32>) outs(%4 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.mulf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %6 = tensor.empty() : tensor<1xf32>
  %7 = linalg.fill ins(%cst_5 : f32) outs(%6 : tensor<1xf32>) -> tensor<1xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%5 : tensor<1x2xf32>) outs(%7 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.maxf %in, %out : f32
    linalg.yield %26 : f32
  } -> tensor<1xf32>
  %9 = tensor.empty() : tensor<1x2xf32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %8 : tensor<1x2xf32>, tensor<1xf32>) outs(%9 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.subf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %11 = tensor.empty() : tensor<1x2xf32>
  %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%10 : tensor<1x2xf32>) outs(%11 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = math.exp %in : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %13 = tensor.empty() : tensor<1xf32>
  %14 = linalg.fill ins(%cst_4 : f32) outs(%13 : tensor<1xf32>) -> tensor<1xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x2xf32>) outs(%14 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.addf %in, %out : f32
    linalg.yield %26 : f32
  } -> tensor<1xf32>
  %expanded = tensor.expand_shape %15 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %16 = tensor.empty() : tensor<1x1xf32>
  %17 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%16 : tensor<1x1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.divf %cst_3, %in : f32
    linalg.yield %26 : f32
  } -> tensor<1x1xf32>
  %18 = tensor.empty() : tensor<1x2xf32>
  %collapsed = tensor.collapse_shape %17 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
  %19 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%18 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.mulf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %20 = tensor.empty() : tensor<1x2xf32>
  %21 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%19, %cst_2 : tensor<1x2xf32>, tensor<1xf32>) outs(%20 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.mulf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %22 = tensor.empty() : tensor<1x2xf32>
  %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%21, %cst_1 : tensor<1x2xf32>, tensor<1xf32>) outs(%22 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.addf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %24 = tensor.empty() : tensor<1x2xi8>
  %25 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%23 : tensor<1x2xf32>) outs(%24 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %26 = math.roundeven %in : f32
    %27 = arith.minf %26, %cst : f32
    %28 = arith.maxf %27, %cst_0 : f32
    %29 = arith.fptosi %28 : f32 to i8
    linalg.yield %29 : i8
  } -> tensor<1x2xi8>
  return %25 : tensor<1x2xi8>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant 1.270000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
  %cst_2 = arith.constant dense<2.560000e+02> : tensor<1xf32>
  %cst_3 = arith.constant 1.000000e+00 : f32
  %cst_4 = arith.constant 0.000000e+00 : f32
  %cst_5 = arith.constant -3.40282347E+38 : f32
  %cst_6 = arith.constant dense<0.0125187514> : tensor<1xf32>
  %cst_7 = arith.constant dense<-1.000000e+00> : tensor<1xf32>
  %0 = tensor.empty() : tensor<1x2xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<1x2xi8>) outs(%0 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %26 = arith.sitofp %in : i8 to f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %2 = tensor.empty() : tensor<1x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%1, %cst_7 : tensor<1x2xf32>, tensor<1xf32>) outs(%2 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.subf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %4 = tensor.empty() : tensor<1x2xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_6 : tensor<1x2xf32>, tensor<1xf32>) outs(%4 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.mulf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %6 = tensor.empty() : tensor<1xf32>
  %7 = linalg.fill ins(%cst_5 : f32) outs(%6 : tensor<1xf32>) -> tensor<1xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%5 : tensor<1x2xf32>) outs(%7 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.maxf %in, %out : f32
    linalg.yield %26 : f32
  } -> tensor<1xf32>
  %9 = tensor.empty() : tensor<1x2xf32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %8 : tensor<1x2xf32>, tensor<1xf32>) outs(%9 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.subf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %11 = tensor.empty() : tensor<1x2xf32>
  %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%10 : tensor<1x2xf32>) outs(%11 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = math.exp %in : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %13 = tensor.empty() : tensor<1xf32>
  %14 = linalg.fill ins(%cst_4 : f32) outs(%13 : tensor<1xf32>) -> tensor<1xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x2xf32>) outs(%14 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.addf %in, %out : f32
    linalg.yield %26 : f32
  } -> tensor<1xf32>
  %expanded = tensor.expand_shape %15 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %16 = tensor.empty() : tensor<1x1xf32>
  %17 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%16 : tensor<1x1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.divf %cst_3, %in : f32
    linalg.yield %26 : f32
  } -> tensor<1x1xf32>
  %18 = tensor.empty() : tensor<1x2xf32>
  %collapsed = tensor.collapse_shape %17 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
  %19 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%18 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.mulf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %20 = tensor.empty() : tensor<1x2xf32>
  %21 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%19, %cst_2 : tensor<1x2xf32>, tensor<1xf32>) outs(%20 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.mulf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %22 = tensor.empty() : tensor<1x2xf32>
  %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%21, %cst_1 : tensor<1x2xf32>, tensor<1xf32>) outs(%22 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.addf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %24 = tensor.empty() : tensor<1x2xi8>
  %25 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%23 : tensor<1x2xf32>) outs(%24 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %26 = math.roundeven %in : f32
    %27 = arith.minf %26, %cst : f32
    %28 = arith.maxf %27, %cst_0 : f32
    %29 = arith.fptosi %28 : f32 to i8
    linalg.yield %29 : i8
  } -> tensor<1x2xi8>
  return %25 : tensor<1x2xi8>
}

// -----// IR Dump After VerifyCompilerTOSAInputLegality (iree-tosa-verify-compiler-input-legality) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
    %cst = arith.constant 1.270000e+02 : f32
    %cst_0 = arith.constant -1.280000e+02 : f32
    %cst_1 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
    %cst_2 = arith.constant dense<2.560000e+02> : tensor<1xf32>
    %cst_3 = arith.constant 1.000000e+00 : f32
    %cst_4 = arith.constant 0.000000e+00 : f32
    %cst_5 = arith.constant -3.40282347E+38 : f32
    %cst_6 = arith.constant dense<0.0125187514> : tensor<1xf32>
    %cst_7 = arith.constant dense<-1.000000e+00> : tensor<1xf32>
    %0 = tensor.empty() : tensor<1x2xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<1x2xi8>) outs(%0 : tensor<1x2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %26 = arith.sitofp %in : i8 to f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %2 = tensor.empty() : tensor<1x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%1, %cst_7 : tensor<1x2xf32>, tensor<1xf32>) outs(%2 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.subf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %4 = tensor.empty() : tensor<1x2xf32>
    %5 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_6 : tensor<1x2xf32>, tensor<1xf32>) outs(%4 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %6 = tensor.empty() : tensor<1xf32>
    %7 = linalg.fill ins(%cst_5 : f32) outs(%6 : tensor<1xf32>) -> tensor<1xf32>
    %8 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%5 : tensor<1x2xf32>) outs(%7 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxf %in, %out : f32
      linalg.yield %26 : f32
    } -> tensor<1xf32>
    %9 = tensor.empty() : tensor<1x2xf32>
    %10 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %8 : tensor<1x2xf32>, tensor<1xf32>) outs(%9 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.subf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %11 = tensor.empty() : tensor<1x2xf32>
    %12 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%10 : tensor<1x2xf32>) outs(%11 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = math.exp %in : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %13 = tensor.empty() : tensor<1xf32>
    %14 = linalg.fill ins(%cst_4 : f32) outs(%13 : tensor<1xf32>) -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x2xf32>) outs(%14 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    } -> tensor<1xf32>
    %expanded = tensor.expand_shape %15 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
    %16 = tensor.empty() : tensor<1x1xf32>
    %17 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%16 : tensor<1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.divf %cst_3, %in : f32
      linalg.yield %26 : f32
    } -> tensor<1x1xf32>
    %18 = tensor.empty() : tensor<1x2xf32>
    %collapsed = tensor.collapse_shape %17 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
    %19 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%12, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%18 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %20 = tensor.empty() : tensor<1x2xf32>
    %21 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%19, %cst_2 : tensor<1x2xf32>, tensor<1xf32>) outs(%20 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %22 = tensor.empty() : tensor<1x2xf32>
    %23 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%21, %cst_1 : tensor<1x2xf32>, tensor<1xf32>) outs(%22 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.addf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %24 = tensor.empty() : tensor<1x2xi8>
    %25 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%23 : tensor<1x2xf32>) outs(%24 : tensor<1x2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %26 = math.roundeven %in : f32
      %27 = arith.minf %26, %cst : f32
      %28 = arith.maxf %27, %cst_0 : f32
      %29 = arith.fptosi %28 : f32 to i8
      linalg.yield %29 : i8
    } -> tensor<1x2xi8>
    return %25 : tensor<1x2xi8>
  }
}


// -----// IR Dump After AutoInputConversionPipeline (iree-auto-input-conversion) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
    %cst = arith.constant 1.270000e+02 : f32
    %cst_0 = arith.constant -1.280000e+02 : f32
    %cst_1 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
    %cst_2 = arith.constant dense<2.560000e+02> : tensor<1xf32>
    %cst_3 = arith.constant 1.000000e+00 : f32
    %cst_4 = arith.constant 0.000000e+00 : f32
    %cst_5 = arith.constant -3.40282347E+38 : f32
    %cst_6 = arith.constant dense<0.0125187514> : tensor<1xf32>
    %cst_7 = arith.constant dense<-1.000000e+00> : tensor<1xf32>
    %0 = tensor.empty() : tensor<1x2xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<1x2xi8>) outs(%0 : tensor<1x2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %26 = arith.sitofp %in : i8 to f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %2 = tensor.empty() : tensor<1x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%1, %cst_7 : tensor<1x2xf32>, tensor<1xf32>) outs(%2 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.subf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %4 = tensor.empty() : tensor<1x2xf32>
    %5 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_6 : tensor<1x2xf32>, tensor<1xf32>) outs(%4 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %6 = tensor.empty() : tensor<1xf32>
    %7 = linalg.fill ins(%cst_5 : f32) outs(%6 : tensor<1xf32>) -> tensor<1xf32>
    %8 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%5 : tensor<1x2xf32>) outs(%7 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxf %in, %out : f32
      linalg.yield %26 : f32
    } -> tensor<1xf32>
    %9 = tensor.empty() : tensor<1x2xf32>
    %10 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %8 : tensor<1x2xf32>, tensor<1xf32>) outs(%9 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.subf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %11 = tensor.empty() : tensor<1x2xf32>
    %12 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%10 : tensor<1x2xf32>) outs(%11 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = math.exp %in : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %13 = tensor.empty() : tensor<1xf32>
    %14 = linalg.fill ins(%cst_4 : f32) outs(%13 : tensor<1xf32>) -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x2xf32>) outs(%14 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    } -> tensor<1xf32>
    %expanded = tensor.expand_shape %15 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
    %16 = tensor.empty() : tensor<1x1xf32>
    %17 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%16 : tensor<1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.divf %cst_3, %in : f32
      linalg.yield %26 : f32
    } -> tensor<1x1xf32>
    %18 = tensor.empty() : tensor<1x2xf32>
    %collapsed = tensor.collapse_shape %17 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
    %19 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%12, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%18 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %20 = tensor.empty() : tensor<1x2xf32>
    %21 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%19, %cst_2 : tensor<1x2xf32>, tensor<1xf32>) outs(%20 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %22 = tensor.empty() : tensor<1x2xf32>
    %23 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%21, %cst_1 : tensor<1x2xf32>, tensor<1xf32>) outs(%22 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.addf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %24 = tensor.empty() : tensor<1x2xi8>
    %25 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%23 : tensor<1x2xf32>) outs(%24 : tensor<1x2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %26 = math.roundeven %in : f32
      %27 = arith.minf %26, %cst : f32
      %28 = arith.maxf %27, %cst_0 : f32
      %29 = arith.fptosi %28 : f32 to i8
      linalg.yield %29 : i8
    } -> tensor<1x2xi8>
    return %25 : tensor<1x2xi8>
  }
}


// -----// IR Dump After IREEImportPublic (iree-import-public) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
    %cst = arith.constant 1.270000e+02 : f32
    %cst_0 = arith.constant -1.280000e+02 : f32
    %cst_1 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
    %cst_2 = arith.constant dense<2.560000e+02> : tensor<1xf32>
    %cst_3 = arith.constant 1.000000e+00 : f32
    %cst_4 = arith.constant 0.000000e+00 : f32
    %cst_5 = arith.constant -3.40282347E+38 : f32
    %cst_6 = arith.constant dense<0.0125187514> : tensor<1xf32>
    %cst_7 = arith.constant dense<-1.000000e+00> : tensor<1xf32>
    %0 = tensor.empty() : tensor<1x2xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<1x2xi8>) outs(%0 : tensor<1x2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %26 = arith.sitofp %in : i8 to f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %2 = tensor.empty() : tensor<1x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%1, %cst_7 : tensor<1x2xf32>, tensor<1xf32>) outs(%2 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.subf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %4 = tensor.empty() : tensor<1x2xf32>
    %5 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_6 : tensor<1x2xf32>, tensor<1xf32>) outs(%4 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %6 = tensor.empty() : tensor<1xf32>
    %7 = linalg.fill ins(%cst_5 : f32) outs(%6 : tensor<1xf32>) -> tensor<1xf32>
    %8 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%5 : tensor<1x2xf32>) outs(%7 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxf %in, %out : f32
      linalg.yield %26 : f32
    } -> tensor<1xf32>
    %9 = tensor.empty() : tensor<1x2xf32>
    %10 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %8 : tensor<1x2xf32>, tensor<1xf32>) outs(%9 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.subf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %11 = tensor.empty() : tensor<1x2xf32>
    %12 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%10 : tensor<1x2xf32>) outs(%11 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = math.exp %in : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %13 = tensor.empty() : tensor<1xf32>
    %14 = linalg.fill ins(%cst_4 : f32) outs(%13 : tensor<1xf32>) -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x2xf32>) outs(%14 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    } -> tensor<1xf32>
    %expanded = tensor.expand_shape %15 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
    %16 = tensor.empty() : tensor<1x1xf32>
    %17 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%16 : tensor<1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.divf %cst_3, %in : f32
      linalg.yield %26 : f32
    } -> tensor<1x1xf32>
    %18 = tensor.empty() : tensor<1x2xf32>
    %collapsed = tensor.collapse_shape %17 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
    %19 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%12, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%18 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %20 = tensor.empty() : tensor<1x2xf32>
    %21 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%19, %cst_2 : tensor<1x2xf32>, tensor<1xf32>) outs(%20 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %22 = tensor.empty() : tensor<1x2xf32>
    %23 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%21, %cst_1 : tensor<1x2xf32>, tensor<1xf32>) outs(%22 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.addf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %24 = tensor.empty() : tensor<1x2xi8>
    %25 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%23 : tensor<1x2xf32>) outs(%24 : tensor<1x2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %26 = math.roundeven %in : f32
      %27 = arith.minf %26, %cst : f32
      %28 = arith.maxf %27, %cst_0 : f32
      %29 = arith.fptosi %28 : f32 to i8
      linalg.yield %29 : i8
    } -> tensor<1x2xi8>
    return %25 : tensor<1x2xi8>
  }
}


// -----// IR Dump After ImportMLProgram (iree-import-ml-program) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
    %cst = arith.constant 1.270000e+02 : f32
    %cst_0 = arith.constant -1.280000e+02 : f32
    %cst_1 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
    %cst_2 = arith.constant dense<2.560000e+02> : tensor<1xf32>
    %cst_3 = arith.constant 1.000000e+00 : f32
    %cst_4 = arith.constant 0.000000e+00 : f32
    %cst_5 = arith.constant -3.40282347E+38 : f32
    %cst_6 = arith.constant dense<0.0125187514> : tensor<1xf32>
    %cst_7 = arith.constant dense<-1.000000e+00> : tensor<1xf32>
    %0 = tensor.empty() : tensor<1x2xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<1x2xi8>) outs(%0 : tensor<1x2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %26 = arith.sitofp %in : i8 to f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %2 = tensor.empty() : tensor<1x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%1, %cst_7 : tensor<1x2xf32>, tensor<1xf32>) outs(%2 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.subf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %4 = tensor.empty() : tensor<1x2xf32>
    %5 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_6 : tensor<1x2xf32>, tensor<1xf32>) outs(%4 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %6 = tensor.empty() : tensor<1xf32>
    %7 = linalg.fill ins(%cst_5 : f32) outs(%6 : tensor<1xf32>) -> tensor<1xf32>
    %8 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%5 : tensor<1x2xf32>) outs(%7 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxf %in, %out : f32
      linalg.yield %26 : f32
    } -> tensor<1xf32>
    %9 = tensor.empty() : tensor<1x2xf32>
    %10 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %8 : tensor<1x2xf32>, tensor<1xf32>) outs(%9 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.subf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %11 = tensor.empty() : tensor<1x2xf32>
    %12 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%10 : tensor<1x2xf32>) outs(%11 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = math.exp %in : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %13 = tensor.empty() : tensor<1xf32>
    %14 = linalg.fill ins(%cst_4 : f32) outs(%13 : tensor<1xf32>) -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x2xf32>) outs(%14 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    } -> tensor<1xf32>
    %expanded = tensor.expand_shape %15 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
    %16 = tensor.empty() : tensor<1x1xf32>
    %17 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%16 : tensor<1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.divf %cst_3, %in : f32
      linalg.yield %26 : f32
    } -> tensor<1x1xf32>
    %18 = tensor.empty() : tensor<1x2xf32>
    %collapsed = tensor.collapse_shape %17 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
    %19 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%12, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%18 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %20 = tensor.empty() : tensor<1x2xf32>
    %21 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%19, %cst_2 : tensor<1x2xf32>, tensor<1xf32>) outs(%20 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %22 = tensor.empty() : tensor<1x2xf32>
    %23 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%21, %cst_1 : tensor<1x2xf32>, tensor<1xf32>) outs(%22 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.addf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %24 = tensor.empty() : tensor<1x2xi8>
    %25 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%23 : tensor<1x2xf32>) outs(%24 : tensor<1x2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %26 = math.roundeven %in : f32
      %27 = arith.minf %26, %cst : f32
      %28 = arith.maxf %27, %cst_0 : f32
      %29 = arith.fptosi %28 : f32 to i8
      linalg.yield %29 : i8
    } -> tensor<1x2xi8>
    return %25 : tensor<1x2xi8>
  }
}


// -----// IR Dump After SanitizeModuleNames (iree-sanitize-module-names) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
    %cst = arith.constant 1.270000e+02 : f32
    %cst_0 = arith.constant -1.280000e+02 : f32
    %cst_1 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
    %cst_2 = arith.constant dense<2.560000e+02> : tensor<1xf32>
    %cst_3 = arith.constant 1.000000e+00 : f32
    %cst_4 = arith.constant 0.000000e+00 : f32
    %cst_5 = arith.constant -3.40282347E+38 : f32
    %cst_6 = arith.constant dense<0.0125187514> : tensor<1xf32>
    %cst_7 = arith.constant dense<-1.000000e+00> : tensor<1xf32>
    %0 = tensor.empty() : tensor<1x2xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<1x2xi8>) outs(%0 : tensor<1x2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %26 = arith.sitofp %in : i8 to f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %2 = tensor.empty() : tensor<1x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%1, %cst_7 : tensor<1x2xf32>, tensor<1xf32>) outs(%2 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.subf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %4 = tensor.empty() : tensor<1x2xf32>
    %5 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_6 : tensor<1x2xf32>, tensor<1xf32>) outs(%4 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %6 = tensor.empty() : tensor<1xf32>
    %7 = linalg.fill ins(%cst_5 : f32) outs(%6 : tensor<1xf32>) -> tensor<1xf32>
    %8 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%5 : tensor<1x2xf32>) outs(%7 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxf %in, %out : f32
      linalg.yield %26 : f32
    } -> tensor<1xf32>
    %9 = tensor.empty() : tensor<1x2xf32>
    %10 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %8 : tensor<1x2xf32>, tensor<1xf32>) outs(%9 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.subf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %11 = tensor.empty() : tensor<1x2xf32>
    %12 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%10 : tensor<1x2xf32>) outs(%11 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = math.exp %in : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %13 = tensor.empty() : tensor<1xf32>
    %14 = linalg.fill ins(%cst_4 : f32) outs(%13 : tensor<1xf32>) -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x2xf32>) outs(%14 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    } -> tensor<1xf32>
    %expanded = tensor.expand_shape %15 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
    %16 = tensor.empty() : tensor<1x1xf32>
    %17 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%16 : tensor<1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.divf %cst_3, %in : f32
      linalg.yield %26 : f32
    } -> tensor<1x1xf32>
    %18 = tensor.empty() : tensor<1x2xf32>
    %collapsed = tensor.collapse_shape %17 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
    %19 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%12, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%18 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %20 = tensor.empty() : tensor<1x2xf32>
    %21 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%19, %cst_2 : tensor<1x2xf32>, tensor<1xf32>) outs(%20 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %22 = tensor.empty() : tensor<1x2xf32>
    %23 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%21, %cst_1 : tensor<1x2xf32>, tensor<1xf32>) outs(%22 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.addf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %24 = tensor.empty() : tensor<1x2xi8>
    %25 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%23 : tensor<1x2xf32>) outs(%24 : tensor<1x2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %26 = math.roundeven %in : f32
      %27 = arith.minf %26, %cst : f32
      %28 = arith.maxf %27, %cst_0 : f32
      %29 = arith.fptosi %28 : f32 to i8
      linalg.yield %29 : i8
    } -> tensor<1x2xi8>
    return %25 : tensor<1x2xi8>
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::ABI::ConvertStreamableOpsPass (iree-abi-convert-streamable-ops) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  func.func @main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
    %cst = arith.constant 1.270000e+02 : f32
    %cst_0 = arith.constant -1.280000e+02 : f32
    %cst_1 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
    %cst_2 = arith.constant dense<2.560000e+02> : tensor<1xf32>
    %cst_3 = arith.constant 1.000000e+00 : f32
    %cst_4 = arith.constant 0.000000e+00 : f32
    %cst_5 = arith.constant -3.40282347E+38 : f32
    %cst_6 = arith.constant dense<0.0125187514> : tensor<1xf32>
    %cst_7 = arith.constant dense<-1.000000e+00> : tensor<1xf32>
    %0 = tensor.empty() : tensor<1x2xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<1x2xi8>) outs(%0 : tensor<1x2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %26 = arith.sitofp %in : i8 to f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %2 = tensor.empty() : tensor<1x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%1, %cst_7 : tensor<1x2xf32>, tensor<1xf32>) outs(%2 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.subf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %4 = tensor.empty() : tensor<1x2xf32>
    %5 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_6 : tensor<1x2xf32>, tensor<1xf32>) outs(%4 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %6 = tensor.empty() : tensor<1xf32>
    %7 = linalg.fill ins(%cst_5 : f32) outs(%6 : tensor<1xf32>) -> tensor<1xf32>
    %8 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%5 : tensor<1x2xf32>) outs(%7 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxf %in, %out : f32
      linalg.yield %26 : f32
    } -> tensor<1xf32>
    %9 = tensor.empty() : tensor<1x2xf32>
    %10 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %8 : tensor<1x2xf32>, tensor<1xf32>) outs(%9 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.subf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %11 = tensor.empty() : tensor<1x2xf32>
    %12 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%10 : tensor<1x2xf32>) outs(%11 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = math.exp %in : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %13 = tensor.empty() : tensor<1xf32>
    %14 = linalg.fill ins(%cst_4 : f32) outs(%13 : tensor<1xf32>) -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x2xf32>) outs(%14 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    } -> tensor<1xf32>
    %expanded = tensor.expand_shape %15 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
    %16 = tensor.empty() : tensor<1x1xf32>
    %17 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%16 : tensor<1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.divf %cst_3, %in : f32
      linalg.yield %26 : f32
    } -> tensor<1x1xf32>
    %18 = tensor.empty() : tensor<1x2xf32>
    %collapsed = tensor.collapse_shape %17 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
    %19 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%12, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%18 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %20 = tensor.empty() : tensor<1x2xf32>
    %21 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%19, %cst_2 : tensor<1x2xf32>, tensor<1xf32>) outs(%20 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %22 = tensor.empty() : tensor<1x2xf32>
    %23 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%21, %cst_1 : tensor<1x2xf32>, tensor<1xf32>) outs(%22 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.addf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %24 = tensor.empty() : tensor<1x2xi8>
    %25 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%23 : tensor<1x2xf32>) outs(%24 : tensor<1x2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %26 = math.roundeven %in : f32
      %27 = arith.minf %26, %cst : f32
      %28 = arith.maxf %27, %cst_0 : f32
      %29 = arith.fptosi %28 : f32 to i8
      linalg.yield %29 : i8
    } -> tensor<1x2xi8>
    return %25 : tensor<1x2xi8>
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::ABI::WrapEntryPointsPass (iree-abi-wrap-entry-points) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = call @_main(%0) : (tensor<1x2xi8>) -> tensor<1x2xi8>
    %2 = hal.tensor.export %1 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %2 : !hal.buffer_view
  }
  func.func private @_main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
    %cst = arith.constant 1.270000e+02 : f32
    %cst_0 = arith.constant -1.280000e+02 : f32
    %cst_1 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
    %cst_2 = arith.constant dense<2.560000e+02> : tensor<1xf32>
    %cst_3 = arith.constant 1.000000e+00 : f32
    %cst_4 = arith.constant 0.000000e+00 : f32
    %cst_5 = arith.constant -3.40282347E+38 : f32
    %cst_6 = arith.constant dense<0.0125187514> : tensor<1xf32>
    %cst_7 = arith.constant dense<-1.000000e+00> : tensor<1xf32>
    %0 = tensor.empty() : tensor<1x2xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<1x2xi8>) outs(%0 : tensor<1x2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %26 = arith.sitofp %in : i8 to f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %2 = tensor.empty() : tensor<1x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%1, %cst_7 : tensor<1x2xf32>, tensor<1xf32>) outs(%2 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.subf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %4 = tensor.empty() : tensor<1x2xf32>
    %5 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_6 : tensor<1x2xf32>, tensor<1xf32>) outs(%4 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %6 = tensor.empty() : tensor<1xf32>
    %7 = linalg.fill ins(%cst_5 : f32) outs(%6 : tensor<1xf32>) -> tensor<1xf32>
    %8 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%5 : tensor<1x2xf32>) outs(%7 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.maxf %in, %out : f32
      linalg.yield %26 : f32
    } -> tensor<1xf32>
    %9 = tensor.empty() : tensor<1x2xf32>
    %10 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%5, %8 : tensor<1x2xf32>, tensor<1xf32>) outs(%9 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.subf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %11 = tensor.empty() : tensor<1x2xf32>
    %12 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%10 : tensor<1x2xf32>) outs(%11 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = math.exp %in : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %13 = tensor.empty() : tensor<1xf32>
    %14 = linalg.fill ins(%cst_4 : f32) outs(%13 : tensor<1xf32>) -> tensor<1xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x2xf32>) outs(%14 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.addf %in, %out : f32
      linalg.yield %26 : f32
    } -> tensor<1xf32>
    %expanded = tensor.expand_shape %15 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
    %16 = tensor.empty() : tensor<1x1xf32>
    %17 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%16 : tensor<1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %26 = arith.divf %cst_3, %in : f32
      linalg.yield %26 : f32
    } -> tensor<1x1xf32>
    %18 = tensor.empty() : tensor<1x2xf32>
    %collapsed = tensor.collapse_shape %17 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
    %19 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%12, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%18 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %20 = tensor.empty() : tensor<1x2xf32>
    %21 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%19, %cst_2 : tensor<1x2xf32>, tensor<1xf32>) outs(%20 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.mulf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %22 = tensor.empty() : tensor<1x2xf32>
    %23 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%21, %cst_1 : tensor<1x2xf32>, tensor<1xf32>) outs(%22 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %26 = arith.addf %in, %in_8 : f32
      linalg.yield %26 : f32
    } -> tensor<1x2xf32>
    %24 = tensor.empty() : tensor<1x2xi8>
    %25 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%23 : tensor<1x2xf32>) outs(%24 : tensor<1x2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %26 = math.roundeven %in : f32
      %27 = arith.minf %26, %cst : f32
      %28 = arith.maxf %27, %cst_0 : f32
      %29 = arith.fptosi %28 : f32 to i8
      linalg.yield %29 : i8
    } -> tensor<1x2xi8>
    return %25 : tensor<1x2xi8>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func private @_main(%arg0: tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (tensor<1x2xi8> {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) {
  %cst = arith.constant 1.270000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
  %cst_2 = arith.constant dense<2.560000e+02> : tensor<1xf32>
  %cst_3 = arith.constant 1.000000e+00 : f32
  %cst_4 = arith.constant 0.000000e+00 : f32
  %cst_5 = arith.constant -3.40282347E+38 : f32
  %cst_6 = arith.constant dense<0.0125187514> : tensor<1xf32>
  %cst_7 = arith.constant dense<-1.000000e+00> : tensor<1xf32>
  %0 = tensor.empty() : tensor<1x2xf32>
  %1 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%arg0 : tensor<1x2xi8>) outs(%0 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %26 = arith.sitofp %in : i8 to f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %2 = tensor.empty() : tensor<1x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%1, %cst_7 : tensor<1x2xf32>, tensor<1xf32>) outs(%2 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.subf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %4 = tensor.empty() : tensor<1x2xf32>
  %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_6 : tensor<1x2xf32>, tensor<1xf32>) outs(%4 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.mulf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %6 = tensor.empty() : tensor<1xf32>
  %7 = linalg.fill ins(%cst_5 : f32) outs(%6 : tensor<1xf32>) -> tensor<1xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%5 : tensor<1x2xf32>) outs(%7 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.maxf %in, %out : f32
    linalg.yield %26 : f32
  } -> tensor<1xf32>
  %9 = tensor.empty() : tensor<1x2xf32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%5, %8 : tensor<1x2xf32>, tensor<1xf32>) outs(%9 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.subf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %11 = tensor.empty() : tensor<1x2xf32>
  %12 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%10 : tensor<1x2xf32>) outs(%11 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = math.exp %in : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %13 = tensor.empty() : tensor<1xf32>
  %14 = linalg.fill ins(%cst_4 : f32) outs(%13 : tensor<1xf32>) -> tensor<1xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%12 : tensor<1x2xf32>) outs(%14 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.addf %in, %out : f32
    linalg.yield %26 : f32
  } -> tensor<1xf32>
  %expanded = tensor.expand_shape %15 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %16 = tensor.empty() : tensor<1x1xf32>
  %17 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%16 : tensor<1x1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %26 = arith.divf %cst_3, %in : f32
    linalg.yield %26 : f32
  } -> tensor<1x1xf32>
  %18 = tensor.empty() : tensor<1x2xf32>
  %collapsed = tensor.collapse_shape %17 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
  %19 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%12, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%18 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.mulf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %20 = tensor.empty() : tensor<1x2xf32>
  %21 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%19, %cst_2 : tensor<1x2xf32>, tensor<1xf32>) outs(%20 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.mulf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %22 = tensor.empty() : tensor<1x2xf32>
  %23 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%21, %cst_1 : tensor<1x2xf32>, tensor<1xf32>) outs(%22 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %26 = arith.addf %in, %in_8 : f32
    linalg.yield %26 : f32
  } -> tensor<1x2xf32>
  %24 = tensor.empty() : tensor<1x2xi8>
  %25 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%23 : tensor<1x2xf32>) outs(%24 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %26 = math.roundeven %in : f32
    %27 = arith.minf %26, %cst : f32
    %28 = arith.maxf %27, %cst_0 : f32
    %29 = arith.fptosi %28 : f32 to i8
    linalg.yield %29 : i8
  } -> tensor<1x2xi8>
  return %25 : tensor<1x2xi8>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = call @_main(%0) : (tensor<1x2xi8>) -> tensor<1x2xi8>
  %2 = hal.tensor.export %1 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %2 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
  %cst_1 = arith.constant -3.40282347E+38 : f32
  %cst_2 = arith.constant 0.000000e+00 : f32
  %cst_3 = arith.constant 1.000000e+00 : f32
  %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
  %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
  %cst_6 = arith.constant -1.280000e+02 : f32
  %cst_7 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = tensor.empty() : tensor<1x2xf32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %28 = arith.sitofp %in : i8 to f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %3 = tensor.empty() : tensor<1x2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%3 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.subf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %5 = tensor.empty() : tensor<1x2xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%5 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.mulf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %7 = tensor.empty() : tensor<1xf32>
  %8 = linalg.fill ins(%cst_1 : f32) outs(%7 : tensor<1xf32>) -> tensor<1xf32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%6 : tensor<1x2xf32>) outs(%8 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = arith.maxf %in, %out : f32
    linalg.yield %28 : f32
  } -> tensor<1xf32>
  %10 = tensor.empty() : tensor<1x2xf32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%6, %9 : tensor<1x2xf32>, tensor<1xf32>) outs(%10 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.subf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %12 = tensor.empty() : tensor<1x2xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%11 : tensor<1x2xf32>) outs(%12 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = math.exp %in : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %14 = tensor.empty() : tensor<1xf32>
  %15 = linalg.fill ins(%cst_2 : f32) outs(%14 : tensor<1xf32>) -> tensor<1xf32>
  %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%13 : tensor<1x2xf32>) outs(%15 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = arith.addf %in, %out : f32
    linalg.yield %28 : f32
  } -> tensor<1xf32>
  %expanded = tensor.expand_shape %16 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %17 = tensor.empty() : tensor<1x1xf32>
  %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%17 : tensor<1x1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = arith.divf %cst_3, %in : f32
    linalg.yield %28 : f32
  } -> tensor<1x1xf32>
  %19 = tensor.empty() : tensor<1x2xf32>
  %collapsed = tensor.collapse_shape %18 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
  %20 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%13, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%19 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.mulf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %21 = tensor.empty() : tensor<1x2xf32>
  %22 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%20, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%21 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.mulf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %23 = tensor.empty() : tensor<1x2xf32>
  %24 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%22, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%23 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.addf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %25 = tensor.empty() : tensor<1x2xi8>
  %26 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%24 : tensor<1x2xf32>) outs(%25 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %28 = math.roundeven %in : f32
    %29 = arith.minf %28, %cst_7 : f32
    %30 = arith.maxf %29, %cst_6 : f32
    %31 = arith.fptosi %30 : f32 to i8
    linalg.yield %31 : i8
  } -> tensor<1x2xi8>
  %27 = hal.tensor.export %26 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %27 : !hal.buffer_view
}

// -----// IR Dump After Inliner (inline) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
    %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
    %cst_1 = arith.constant -3.40282347E+38 : f32
    %cst_2 = arith.constant 0.000000e+00 : f32
    %cst_3 = arith.constant 1.000000e+00 : f32
    %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
    %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
    %cst_6 = arith.constant -1.280000e+02 : f32
    %cst_7 = arith.constant 1.270000e+02 : f32
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = tensor.empty() : tensor<1x2xf32>
    %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %28 = arith.sitofp %in : i8 to f32
      linalg.yield %28 : f32
    } -> tensor<1x2xf32>
    %3 = tensor.empty() : tensor<1x2xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%3 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %28 = arith.subf %in, %in_8 : f32
      linalg.yield %28 : f32
    } -> tensor<1x2xf32>
    %5 = tensor.empty() : tensor<1x2xf32>
    %6 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%5 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %28 = arith.mulf %in, %in_8 : f32
      linalg.yield %28 : f32
    } -> tensor<1x2xf32>
    %7 = tensor.empty() : tensor<1xf32>
    %8 = linalg.fill ins(%cst_1 : f32) outs(%7 : tensor<1xf32>) -> tensor<1xf32>
    %9 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%6 : tensor<1x2xf32>) outs(%8 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %28 = arith.maxf %in, %out : f32
      linalg.yield %28 : f32
    } -> tensor<1xf32>
    %10 = tensor.empty() : tensor<1x2xf32>
    %11 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%6, %9 : tensor<1x2xf32>, tensor<1xf32>) outs(%10 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %28 = arith.subf %in, %in_8 : f32
      linalg.yield %28 : f32
    } -> tensor<1x2xf32>
    %12 = tensor.empty() : tensor<1x2xf32>
    %13 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%11 : tensor<1x2xf32>) outs(%12 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %out: f32):
      %28 = math.exp %in : f32
      linalg.yield %28 : f32
    } -> tensor<1x2xf32>
    %14 = tensor.empty() : tensor<1xf32>
    %15 = linalg.fill ins(%cst_2 : f32) outs(%14 : tensor<1xf32>) -> tensor<1xf32>
    %16 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%13 : tensor<1x2xf32>) outs(%15 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %28 = arith.addf %in, %out : f32
      linalg.yield %28 : f32
    } -> tensor<1xf32>
    %expanded = tensor.expand_shape %16 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
    %17 = tensor.empty() : tensor<1x1xf32>
    %18 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%17 : tensor<1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %28 = arith.divf %cst_3, %in : f32
      linalg.yield %28 : f32
    } -> tensor<1x1xf32>
    %19 = tensor.empty() : tensor<1x2xf32>
    %collapsed = tensor.collapse_shape %18 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
    %20 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%13, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%19 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %28 = arith.mulf %in, %in_8 : f32
      linalg.yield %28 : f32
    } -> tensor<1x2xf32>
    %21 = tensor.empty() : tensor<1x2xf32>
    %22 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%20, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%21 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %28 = arith.mulf %in, %in_8 : f32
      linalg.yield %28 : f32
    } -> tensor<1x2xf32>
    %23 = tensor.empty() : tensor<1x2xf32>
    %24 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%22, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%23 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %28 = arith.addf %in, %in_8 : f32
      linalg.yield %28 : f32
    } -> tensor<1x2xf32>
    %25 = tensor.empty() : tensor<1x2xi8>
    %26 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%24 : tensor<1x2xf32>) outs(%25 : tensor<1x2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %28 = math.roundeven %in : f32
      %29 = arith.minf %28, %cst_7 : f32
      %30 = arith.maxf %29, %cst_6 : f32
      %31 = arith.fptosi %30 : f32 to i8
      linalg.yield %31 : i8
    } -> tensor<1x2xi8>
    %27 = hal.tensor.export %26 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %27 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
  %cst_1 = arith.constant -3.40282347E+38 : f32
  %cst_2 = arith.constant 0.000000e+00 : f32
  %cst_3 = arith.constant 1.000000e+00 : f32
  %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
  %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
  %cst_6 = arith.constant -1.280000e+02 : f32
  %cst_7 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = tensor.empty() : tensor<1x2xf32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %28 = arith.sitofp %in : i8 to f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %3 = tensor.empty() : tensor<1x2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%3 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.subf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %5 = tensor.empty() : tensor<1x2xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%5 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.mulf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %7 = tensor.empty() : tensor<1xf32>
  %8 = linalg.fill ins(%cst_1 : f32) outs(%7 : tensor<1xf32>) -> tensor<1xf32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%6 : tensor<1x2xf32>) outs(%8 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = arith.maxf %in, %out : f32
    linalg.yield %28 : f32
  } -> tensor<1xf32>
  %10 = tensor.empty() : tensor<1x2xf32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%6, %9 : tensor<1x2xf32>, tensor<1xf32>) outs(%10 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.subf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %12 = tensor.empty() : tensor<1x2xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%11 : tensor<1x2xf32>) outs(%12 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = math.exp %in : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %14 = tensor.empty() : tensor<1xf32>
  %15 = linalg.fill ins(%cst_2 : f32) outs(%14 : tensor<1xf32>) -> tensor<1xf32>
  %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%13 : tensor<1x2xf32>) outs(%15 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = arith.addf %in, %out : f32
    linalg.yield %28 : f32
  } -> tensor<1xf32>
  %expanded = tensor.expand_shape %16 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %17 = tensor.empty() : tensor<1x1xf32>
  %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%17 : tensor<1x1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = arith.divf %cst_3, %in : f32
    linalg.yield %28 : f32
  } -> tensor<1x1xf32>
  %19 = tensor.empty() : tensor<1x2xf32>
  %collapsed = tensor.collapse_shape %18 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
  %20 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%13, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%19 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.mulf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %21 = tensor.empty() : tensor<1x2xf32>
  %22 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%20, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%21 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.mulf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %23 = tensor.empty() : tensor<1x2xf32>
  %24 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%22, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%23 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.addf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<1x2xf32>
  %25 = tensor.empty() : tensor<1x2xi8>
  %26 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%24 : tensor<1x2xf32>) outs(%25 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %28 = math.roundeven %in : f32
    %29 = arith.minf %28, %cst_7 : f32
    %30 = arith.maxf %29, %cst_6 : f32
    %31 = arith.fptosi %30 : f32 to i8
    linalg.yield %31 : i8
  } -> tensor<1x2xi8>
  %27 = hal.tensor.export %26 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %27 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
  %cst_1 = arith.constant -3.40282347E+38 : f32
  %cst_2 = arith.constant 0.000000e+00 : f32
  %cst_3 = arith.constant 1.000000e+00 : f32
  %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
  %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
  %cst_6 = arith.constant -1.280000e+02 : f32
  %cst_7 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = tensor.empty() : tensor<1x2xf32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %20 = arith.sitofp %in : i8 to f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.subf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %5 = tensor.empty() : tensor<1xf32>
  %6 = linalg.fill ins(%cst_1 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x2xf32>) outs(%6 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.maxf %in, %out : f32
    linalg.yield %20 : f32
  } -> tensor<1xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %7 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.subf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<1x2xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = math.exp %in : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %10 = linalg.fill ins(%cst_2 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x2xf32>) outs(%10 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.addf %in, %out : f32
    linalg.yield %20 : f32
  } -> tensor<1xf32>
  %expanded = tensor.expand_shape %11 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %12 = tensor.empty() : tensor<1x1xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%12 : tensor<1x1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.divf %cst_3, %in : f32
    linalg.yield %20 : f32
  } -> tensor<1x1xf32>
  %collapsed = tensor.collapse_shape %13 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
  %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%9, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%14, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%15, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.addf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %17 = tensor.empty() : tensor<1x2xi8>
  %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%16 : tensor<1x2xf32>) outs(%17 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %20 = math.roundeven %in : f32
    %21 = arith.minf %20, %cst_7 : f32
    %22 = arith.maxf %21, %cst_6 : f32
    %23 = arith.fptosi %22 : f32 to i8
    linalg.yield %23 : i8
  } -> tensor<1x2xi8>
  %19 = hal.tensor.export %18 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %19 : !hal.buffer_view
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
    %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
    %cst_1 = arith.constant -3.40282347E+38 : f32
    %cst_2 = arith.constant 0.000000e+00 : f32
    %cst_3 = arith.constant 1.000000e+00 : f32
    %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
    %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
    %cst_6 = arith.constant -1.280000e+02 : f32
    %cst_7 = arith.constant 1.270000e+02 : f32
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = tensor.empty() : tensor<1x2xf32>
    %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %20 = arith.sitofp %in : i8 to f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %5 = tensor.empty() : tensor<1xf32>
    %6 = linalg.fill ins(%cst_1 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x2xf32>) outs(%6 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.maxf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %8 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %7 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %9 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<1x2xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = math.exp %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %10 = linalg.fill ins(%cst_2 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x2xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.addf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %expanded = tensor.expand_shape %11 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
    %12 = tensor.empty() : tensor<1x1xf32>
    %13 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%12 : tensor<1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.divf %cst_3, %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x1xf32>
    %collapsed = tensor.collapse_shape %13 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%14, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %16 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%15, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.addf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %17 = tensor.empty() : tensor<1x2xi8>
    %18 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%16 : tensor<1x2xf32>) outs(%17 : tensor<1x2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %20 = math.roundeven %in : f32
      %21 = arith.minf %20, %cst_7 : f32
      %22 = arith.maxf %21, %cst_6 : f32
      %23 = arith.fptosi %22 : f32 to i8
      linalg.yield %23 : i8
    } -> tensor<1x2xi8>
    %19 = hal.tensor.export %18 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %19 : !hal.buffer_view
  }
}


// -----// IR Dump After DemoteF64ToF32 (iree-util-demote-f64-to-f32) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
    %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
    %cst_1 = arith.constant -3.40282347E+38 : f32
    %cst_2 = arith.constant 0.000000e+00 : f32
    %cst_3 = arith.constant 1.000000e+00 : f32
    %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
    %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
    %cst_6 = arith.constant -1.280000e+02 : f32
    %cst_7 = arith.constant 1.270000e+02 : f32
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = tensor.empty() : tensor<1x2xf32>
    %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %20 = arith.sitofp %in : i8 to f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %5 = tensor.empty() : tensor<1xf32>
    %6 = linalg.fill ins(%cst_1 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x2xf32>) outs(%6 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.maxf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %8 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %7 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %9 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<1x2xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = math.exp %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %10 = linalg.fill ins(%cst_2 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x2xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.addf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %expanded = tensor.expand_shape %11 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
    %12 = tensor.empty() : tensor<1x1xf32>
    %13 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%12 : tensor<1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.divf %cst_3, %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x1xf32>
    %collapsed = tensor.collapse_shape %13 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%14, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %16 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%15, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.addf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %17 = tensor.empty() : tensor<1x2xi8>
    %18 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%16 : tensor<1x2xf32>) outs(%17 : tensor<1x2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %20 = math.roundeven %in : f32
      %21 = arith.minf %20, %cst_7 : f32
      %22 = arith.maxf %21, %cst_6 : f32
      %23 = arith.fptosi %22 : f32 to i8
      linalg.yield %23 : i8
    } -> tensor<1x2xi8>
    %19 = hal.tensor.export %18 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %19 : !hal.buffer_view
  }
}


// -----// IR Dump After DetachElementwiseFromNamedOps (iree-flow-detach-elementwise-from-named-ops) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
  %cst_1 = arith.constant -3.40282347E+38 : f32
  %cst_2 = arith.constant 0.000000e+00 : f32
  %cst_3 = arith.constant 1.000000e+00 : f32
  %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
  %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
  %cst_6 = arith.constant -1.280000e+02 : f32
  %cst_7 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = tensor.empty() : tensor<1x2xf32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %20 = arith.sitofp %in : i8 to f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.subf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %5 = tensor.empty() : tensor<1xf32>
  %6 = linalg.fill ins(%cst_1 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x2xf32>) outs(%6 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.maxf %in, %out : f32
    linalg.yield %20 : f32
  } -> tensor<1xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %7 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.subf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<1x2xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = math.exp %in : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %10 = linalg.fill ins(%cst_2 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x2xf32>) outs(%10 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.addf %in, %out : f32
    linalg.yield %20 : f32
  } -> tensor<1xf32>
  %expanded = tensor.expand_shape %11 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %12 = tensor.empty() : tensor<1x1xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%12 : tensor<1x1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.divf %cst_3, %in : f32
    linalg.yield %20 : f32
  } -> tensor<1x1xf32>
  %collapsed = tensor.collapse_shape %13 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
  %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%9, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%14, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%15, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.addf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %17 = tensor.empty() : tensor<1x2xi8>
  %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%16 : tensor<1x2xf32>) outs(%17 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %20 = math.roundeven %in : f32
    %21 = arith.minf %20, %cst_7 : f32
    %22 = arith.maxf %21, %cst_6 : f32
    %23 = arith.fptosi %22 : f32 to i8
    linalg.yield %23 : i8
  } -> tensor<1x2xi8>
  %19 = hal.tensor.export %18 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %19 : !hal.buffer_view
}

// -----// IR Dump After LinalgNamedOpConversion (linalg-named-op-conversion) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
  %cst_1 = arith.constant -3.40282347E+38 : f32
  %cst_2 = arith.constant 0.000000e+00 : f32
  %cst_3 = arith.constant 1.000000e+00 : f32
  %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
  %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
  %cst_6 = arith.constant -1.280000e+02 : f32
  %cst_7 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = tensor.empty() : tensor<1x2xf32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %20 = arith.sitofp %in : i8 to f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.subf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %5 = tensor.empty() : tensor<1xf32>
  %6 = linalg.fill ins(%cst_1 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x2xf32>) outs(%6 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.maxf %in, %out : f32
    linalg.yield %20 : f32
  } -> tensor<1xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %7 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.subf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<1x2xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = math.exp %in : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %10 = linalg.fill ins(%cst_2 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x2xf32>) outs(%10 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.addf %in, %out : f32
    linalg.yield %20 : f32
  } -> tensor<1xf32>
  %expanded = tensor.expand_shape %11 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %12 = tensor.empty() : tensor<1x1xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%12 : tensor<1x1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.divf %cst_3, %in : f32
    linalg.yield %20 : f32
  } -> tensor<1x1xf32>
  %collapsed = tensor.collapse_shape %13 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
  %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%9, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%14, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%15, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.addf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %17 = tensor.empty() : tensor<1x2xi8>
  %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%16 : tensor<1x2xf32>) outs(%17 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %20 = math.roundeven %in : f32
    %21 = arith.minf %20, %cst_7 : f32
    %22 = arith.maxf %21, %cst_6 : f32
    %23 = arith.fptosi %22 : f32 to i8
    linalg.yield %23 : i8
  } -> tensor<1x2xi8>
  %19 = hal.tensor.export %18 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %19 : !hal.buffer_view
}

// -----// IR Dump After Convert1X1FilterConv2DToMatmul (iree-flow-convert-1x1-filter-conv2d-to-matmul) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
  %cst_1 = arith.constant -3.40282347E+38 : f32
  %cst_2 = arith.constant 0.000000e+00 : f32
  %cst_3 = arith.constant 1.000000e+00 : f32
  %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
  %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
  %cst_6 = arith.constant -1.280000e+02 : f32
  %cst_7 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = tensor.empty() : tensor<1x2xf32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %20 = arith.sitofp %in : i8 to f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.subf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %5 = tensor.empty() : tensor<1xf32>
  %6 = linalg.fill ins(%cst_1 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x2xf32>) outs(%6 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.maxf %in, %out : f32
    linalg.yield %20 : f32
  } -> tensor<1xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %7 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.subf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<1x2xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = math.exp %in : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %10 = linalg.fill ins(%cst_2 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x2xf32>) outs(%10 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.addf %in, %out : f32
    linalg.yield %20 : f32
  } -> tensor<1xf32>
  %expanded = tensor.expand_shape %11 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %12 = tensor.empty() : tensor<1x1xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%12 : tensor<1x1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.divf %cst_3, %in : f32
    linalg.yield %20 : f32
  } -> tensor<1x1xf32>
  %collapsed = tensor.collapse_shape %13 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
  %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%9, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%14, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%15, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.addf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %17 = tensor.empty() : tensor<1x2xi8>
  %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%16 : tensor<1x2xf32>) outs(%17 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %20 = math.roundeven %in : f32
    %21 = arith.minf %20, %cst_7 : f32
    %22 = arith.maxf %21, %cst_6 : f32
    %23 = arith.fptosi %22 : f32 to i8
    linalg.yield %23 : i8
  } -> tensor<1x2xi8>
  %19 = hal.tensor.export %18 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %19 : !hal.buffer_view
}

// -----// IR Dump After EraseUnusedLinalgOperands (iree-flow-erase-unused-linalg-operands) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
    %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
    %cst_1 = arith.constant -3.40282347E+38 : f32
    %cst_2 = arith.constant 0.000000e+00 : f32
    %cst_3 = arith.constant 1.000000e+00 : f32
    %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
    %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
    %cst_6 = arith.constant -1.280000e+02 : f32
    %cst_7 = arith.constant 1.270000e+02 : f32
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = tensor.empty() : tensor<1x2xf32>
    %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %20 = arith.sitofp %in : i8 to f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %5 = tensor.empty() : tensor<1xf32>
    %6 = linalg.fill ins(%cst_1 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x2xf32>) outs(%6 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.maxf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %8 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %7 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %9 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<1x2xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = math.exp %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %10 = linalg.fill ins(%cst_2 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x2xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.addf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %expanded = tensor.expand_shape %11 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
    %12 = tensor.empty() : tensor<1x1xf32>
    %13 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%12 : tensor<1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.divf %cst_3, %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x1xf32>
    %collapsed = tensor.collapse_shape %13 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%14, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %16 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%15, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.addf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %17 = tensor.empty() : tensor<1x2xi8>
    %18 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%16 : tensor<1x2xf32>) outs(%17 : tensor<1x2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %20 = math.roundeven %in : f32
      %21 = arith.minf %20, %cst_7 : f32
      %22 = arith.maxf %21, %cst_6 : f32
      %23 = arith.fptosi %22 : f32 to i8
      linalg.yield %23 : i8
    } -> tensor<1x2xi8>
    %19 = hal.tensor.export %18 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %19 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyInputLegality (iree-verify-input-legality) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
    %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
    %cst_1 = arith.constant -3.40282347E+38 : f32
    %cst_2 = arith.constant 0.000000e+00 : f32
    %cst_3 = arith.constant 1.000000e+00 : f32
    %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
    %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
    %cst_6 = arith.constant -1.280000e+02 : f32
    %cst_7 = arith.constant 1.270000e+02 : f32
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = tensor.empty() : tensor<1x2xf32>
    %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %20 = arith.sitofp %in : i8 to f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %5 = tensor.empty() : tensor<1xf32>
    %6 = linalg.fill ins(%cst_1 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x2xf32>) outs(%6 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.maxf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %8 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %7 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %9 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<1x2xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = math.exp %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %10 = linalg.fill ins(%cst_2 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x2xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.addf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %expanded = tensor.expand_shape %11 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
    %12 = tensor.empty() : tensor<1x1xf32>
    %13 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%12 : tensor<1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.divf %cst_3, %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x1xf32>
    %collapsed = tensor.collapse_shape %13 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%14, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %16 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%15, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.addf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %17 = tensor.empty() : tensor<1x2xi8>
    %18 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%16 : tensor<1x2xf32>) outs(%17 : tensor<1x2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %20 = math.roundeven %in : f32
      %21 = arith.minf %20, %cst_7 : f32
      %22 = arith.maxf %21, %cst_6 : f32
      %23 = arith.fptosi %22 : f32 to i8
      linalg.yield %23 : i8
    } -> tensor<1x2xi8>
    %19 = hal.tensor.export %18 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %19 : !hal.buffer_view
  }
}


// -----// IR Dump After ExpandTensorShapes (iree-flow-expand-tensor-shapes) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
    %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
    %cst_1 = arith.constant -3.40282347E+38 : f32
    %cst_2 = arith.constant 0.000000e+00 : f32
    %cst_3 = arith.constant 1.000000e+00 : f32
    %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
    %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
    %cst_6 = arith.constant -1.280000e+02 : f32
    %cst_7 = arith.constant 1.270000e+02 : f32
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = tensor.empty() : tensor<1x2xf32>
    %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %20 = arith.sitofp %in : i8 to f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %5 = tensor.empty() : tensor<1xf32>
    %6 = linalg.fill ins(%cst_1 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x2xf32>) outs(%6 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.maxf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %8 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %7 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %9 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<1x2xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = math.exp %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %10 = linalg.fill ins(%cst_2 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x2xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.addf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %expanded = tensor.expand_shape %11 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
    %12 = tensor.empty() : tensor<1x1xf32>
    %13 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%12 : tensor<1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.divf %cst_3, %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x1xf32>
    %collapsed = tensor.collapse_shape %13 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%14, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %16 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%15, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.addf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %17 = tensor.empty() : tensor<1x2xi8>
    %18 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%16 : tensor<1x2xf32>) outs(%17 : tensor<1x2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %20 = math.roundeven %in : f32
      %21 = arith.minf %20, %cst_7 : f32
      %22 = arith.maxf %21, %cst_6 : f32
      %23 = arith.fptosi %22 : f32 to i8
      linalg.yield %23 : i8
    } -> tensor<1x2xi8>
    %19 = hal.tensor.export %18 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %19 : !hal.buffer_view
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
  %cst_1 = arith.constant -3.40282347E+38 : f32
  %cst_2 = arith.constant 0.000000e+00 : f32
  %cst_3 = arith.constant 1.000000e+00 : f32
  %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
  %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
  %cst_6 = arith.constant -1.280000e+02 : f32
  %cst_7 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = tensor.empty() : tensor<1x2xf32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %20 = arith.sitofp %in : i8 to f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.subf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %5 = tensor.empty() : tensor<1xf32>
  %6 = linalg.fill ins(%cst_1 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x2xf32>) outs(%6 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.maxf %in, %out : f32
    linalg.yield %20 : f32
  } -> tensor<1xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %7 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.subf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<1x2xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = math.exp %in : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %10 = linalg.fill ins(%cst_2 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x2xf32>) outs(%10 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.addf %in, %out : f32
    linalg.yield %20 : f32
  } -> tensor<1xf32>
  %expanded = tensor.expand_shape %11 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %12 = tensor.empty() : tensor<1x1xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%12 : tensor<1x1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.divf %cst_3, %in : f32
    linalg.yield %20 : f32
  } -> tensor<1x1xf32>
  %collapsed = tensor.collapse_shape %13 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
  %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%9, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%14, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%15, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.addf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %17 = tensor.empty() : tensor<1x2xi8>
  %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%16 : tensor<1x2xf32>) outs(%17 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %20 = math.roundeven %in : f32
    %21 = arith.minf %20, %cst_7 : f32
    %22 = arith.maxf %21, %cst_6 : f32
    %23 = arith.fptosi %22 : f32 to i8
    linalg.yield %23 : i8
  } -> tensor<1x2xi8>
  %19 = hal.tensor.export %18 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %19 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 0 : index} {
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
    %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
    %cst_1 = arith.constant -3.40282347E+38 : f32
    %cst_2 = arith.constant 0.000000e+00 : f32
    %cst_3 = arith.constant 1.000000e+00 : f32
    %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
    %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
    %cst_6 = arith.constant -1.280000e+02 : f32
    %cst_7 = arith.constant 1.270000e+02 : f32
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = tensor.empty() : tensor<1x2xf32>
    %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %20 = arith.sitofp %in : i8 to f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %5 = tensor.empty() : tensor<1xf32>
    %6 = linalg.fill ins(%cst_1 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x2xf32>) outs(%6 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.maxf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %8 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %7 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %9 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<1x2xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = math.exp %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %10 = linalg.fill ins(%cst_2 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x2xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.addf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %expanded = tensor.expand_shape %11 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
    %12 = tensor.empty() : tensor<1x1xf32>
    %13 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%12 : tensor<1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.divf %cst_3, %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x1xf32>
    %collapsed = tensor.collapse_shape %13 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%14, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %16 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%15, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.addf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %17 = tensor.empty() : tensor<1x2xi8>
    %18 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%16 : tensor<1x2xf32>) outs(%17 : tensor<1x2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %20 = math.roundeven %in : f32
      %21 = arith.minf %20, %cst_7 : f32
      %22 = arith.maxf %21, %cst_6 : f32
      %23 = arith.fptosi %22 : f32 to i8
      linalg.yield %23 : i8
    } -> tensor<1x2xi8>
    %19 = hal.tensor.export %18 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %19 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 0 : index} {
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
    %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
    %cst_1 = arith.constant -3.40282347E+38 : f32
    %cst_2 = arith.constant 0.000000e+00 : f32
    %cst_3 = arith.constant 1.000000e+00 : f32
    %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
    %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
    %cst_6 = arith.constant -1.280000e+02 : f32
    %cst_7 = arith.constant 1.270000e+02 : f32
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = tensor.empty() : tensor<1x2xf32>
    %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %20 = arith.sitofp %in : i8 to f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %5 = tensor.empty() : tensor<1xf32>
    %6 = linalg.fill ins(%cst_1 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x2xf32>) outs(%6 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.maxf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %8 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %7 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %9 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<1x2xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = math.exp %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %10 = linalg.fill ins(%cst_2 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x2xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.addf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %expanded = tensor.expand_shape %11 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
    %12 = tensor.empty() : tensor<1x1xf32>
    %13 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%12 : tensor<1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.divf %cst_3, %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x1xf32>
    %collapsed = tensor.collapse_shape %13 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%14, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %16 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%15, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.addf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %17 = tensor.empty() : tensor<1x2xi8>
    %18 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%16 : tensor<1x2xf32>) outs(%17 : tensor<1x2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %20 = math.roundeven %in : f32
      %21 = arith.minf %20, %cst_7 : f32
      %22 = arith.maxf %21, %cst_6 : f32
      %23 = arith.fptosi %22 : f32 to i8
      linalg.yield %23 : i8
    } -> tensor<1x2xi8>
    %19 = hal.tensor.export %18 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %19 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 0 : index} {
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
    %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
    %cst_1 = arith.constant -3.40282347E+38 : f32
    %cst_2 = arith.constant 0.000000e+00 : f32
    %cst_3 = arith.constant 1.000000e+00 : f32
    %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
    %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
    %cst_6 = arith.constant -1.280000e+02 : f32
    %cst_7 = arith.constant 1.270000e+02 : f32
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = tensor.empty() : tensor<1x2xf32>
    %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %20 = arith.sitofp %in : i8 to f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %5 = tensor.empty() : tensor<1xf32>
    %6 = linalg.fill ins(%cst_1 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x2xf32>) outs(%6 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.maxf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %8 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %7 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %9 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<1x2xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = math.exp %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %10 = linalg.fill ins(%cst_2 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x2xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.addf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %expanded = tensor.expand_shape %11 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
    %12 = tensor.empty() : tensor<1x1xf32>
    %13 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%12 : tensor<1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.divf %cst_3, %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x1xf32>
    %collapsed = tensor.collapse_shape %13 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%14, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %16 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%15, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.addf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %17 = tensor.empty() : tensor<1x2xi8>
    %18 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%16 : tensor<1x2xf32>) outs(%17 : tensor<1x2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %20 = math.roundeven %in : f32
      %21 = arith.minf %20, %cst_7 : f32
      %22 = arith.maxf %21, %cst_6 : f32
      %23 = arith.fptosi %22 : f32 to i8
      linalg.yield %23 : i8
    } -> tensor<1x2xi8>
    %19 = hal.tensor.export %18 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %19 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
  %cst_1 = arith.constant -3.40282347E+38 : f32
  %cst_2 = arith.constant 0.000000e+00 : f32
  %cst_3 = arith.constant 1.000000e+00 : f32
  %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
  %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
  %cst_6 = arith.constant -1.280000e+02 : f32
  %cst_7 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = tensor.empty() : tensor<1x2xf32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %20 = arith.sitofp %in : i8 to f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.subf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %5 = tensor.empty() : tensor<1xf32>
  %6 = linalg.fill ins(%cst_1 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x2xf32>) outs(%6 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.maxf %in, %out : f32
    linalg.yield %20 : f32
  } -> tensor<1xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %7 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.subf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<1x2xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = math.exp %in : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %10 = linalg.fill ins(%cst_2 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x2xf32>) outs(%10 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.addf %in, %out : f32
    linalg.yield %20 : f32
  } -> tensor<1xf32>
  %expanded = tensor.expand_shape %11 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %12 = tensor.empty() : tensor<1x1xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%12 : tensor<1x1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.divf %cst_3, %in : f32
    linalg.yield %20 : f32
  } -> tensor<1x1xf32>
  %collapsed = tensor.collapse_shape %13 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
  %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%9, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%14, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%15, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.addf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %17 = tensor.empty() : tensor<1x2xi8>
  %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%16 : tensor<1x2xf32>) outs(%17 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %20 = math.roundeven %in : f32
    %21 = arith.minf %20, %cst_7 : f32
    %22 = arith.maxf %21, %cst_6 : f32
    %23 = arith.fptosi %22 : f32 to i8
    linalg.yield %23 : i8
  } -> tensor<1x2xi8>
  %19 = hal.tensor.export %18 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %19 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
  %cst_1 = arith.constant -3.40282347E+38 : f32
  %cst_2 = arith.constant 0.000000e+00 : f32
  %cst_3 = arith.constant 1.000000e+00 : f32
  %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
  %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
  %cst_6 = arith.constant -1.280000e+02 : f32
  %cst_7 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = tensor.empty() : tensor<1x2xf32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %20 = arith.sitofp %in : i8 to f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.subf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %5 = tensor.empty() : tensor<1xf32>
  %6 = linalg.fill ins(%cst_1 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x2xf32>) outs(%6 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.maxf %in, %out : f32
    linalg.yield %20 : f32
  } -> tensor<1xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %7 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.subf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<1x2xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = math.exp %in : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %10 = linalg.fill ins(%cst_2 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x2xf32>) outs(%10 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.addf %in, %out : f32
    linalg.yield %20 : f32
  } -> tensor<1xf32>
  %expanded = tensor.expand_shape %11 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %12 = tensor.empty() : tensor<1x1xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%12 : tensor<1x1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.divf %cst_3, %in : f32
    linalg.yield %20 : f32
  } -> tensor<1x1xf32>
  %collapsed = tensor.collapse_shape %13 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
  %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%9, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%14, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%15, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.addf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %17 = tensor.empty() : tensor<1x2xi8>
  %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%16 : tensor<1x2xf32>) outs(%17 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %20 = math.roundeven %in : f32
    %21 = arith.minf %20, %cst_7 : f32
    %22 = arith.maxf %21, %cst_6 : f32
    %23 = arith.fptosi %22 : f32 to i8
    linalg.yield %23 : i8
  } -> tensor<1x2xi8>
  %19 = hal.tensor.export %18 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %19 : !hal.buffer_view
}

// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
    %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
    %cst_1 = arith.constant -3.40282347E+38 : f32
    %cst_2 = arith.constant 0.000000e+00 : f32
    %cst_3 = arith.constant 1.000000e+00 : f32
    %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
    %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
    %cst_6 = arith.constant -1.280000e+02 : f32
    %cst_7 = arith.constant 1.270000e+02 : f32
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = tensor.empty() : tensor<1x2xf32>
    %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %20 = arith.sitofp %in : i8 to f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %5 = tensor.empty() : tensor<1xf32>
    %6 = linalg.fill ins(%cst_1 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x2xf32>) outs(%6 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.maxf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %8 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %7 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %9 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<1x2xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = math.exp %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %10 = linalg.fill ins(%cst_2 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x2xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.addf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %expanded = tensor.expand_shape %11 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
    %12 = tensor.empty() : tensor<1x1xf32>
    %13 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%12 : tensor<1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.divf %cst_3, %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x1xf32>
    %collapsed = tensor.collapse_shape %13 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%14, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %16 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%15, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.addf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %17 = tensor.empty() : tensor<1x2xi8>
    %18 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%16 : tensor<1x2xf32>) outs(%17 : tensor<1x2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %20 = math.roundeven %in : f32
      %21 = arith.minf %20, %cst_7 : f32
      %22 = arith.maxf %21, %cst_6 : f32
      %23 = arith.fptosi %22 : f32 to i8
      linalg.yield %23 : i8
    } -> tensor<1x2xi8>
    %19 = hal.tensor.export %18 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %19 : !hal.buffer_view
  }
}


// -----// IR Dump After TensorPadToTensorInsertSlice (iree-flow-tensor-pad-to-tensor-insert-slice) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0, d1) -> (d0, d1)>
#map1 = affine_map<(d0, d1) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
    %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
    %cst_1 = arith.constant -3.40282347E+38 : f32
    %cst_2 = arith.constant 0.000000e+00 : f32
    %cst_3 = arith.constant 1.000000e+00 : f32
    %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
    %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
    %cst_6 = arith.constant -1.280000e+02 : f32
    %cst_7 = arith.constant 1.270000e+02 : f32
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = tensor.empty() : tensor<1x2xf32>
    %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %20 = arith.sitofp %in : i8 to f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %3 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %4 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %5 = tensor.empty() : tensor<1xf32>
    %6 = linalg.fill ins(%cst_1 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %7 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x2xf32>) outs(%6 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.maxf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %8 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%4, %7 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.subf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %9 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<1x2xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = math.exp %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %10 = linalg.fill ins(%cst_2 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
    %11 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x2xf32>) outs(%10 : tensor<1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.addf %in, %out : f32
      linalg.yield %20 : f32
    } -> tensor<1xf32>
    %expanded = tensor.expand_shape %11 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
    %12 = tensor.empty() : tensor<1x1xf32>
    %13 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%12 : tensor<1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      %20 = arith.divf %cst_3, %in : f32
      linalg.yield %20 : f32
    } -> tensor<1x1xf32>
    %collapsed = tensor.collapse_shape %13 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
    %14 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%9, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %15 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%14, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.mulf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %16 = linalg.generic {indexing_maps = [#map, #map1, #map], iterator_types = ["parallel", "parallel"]} ins(%15, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
    ^bb0(%in: f32, %in_8: f32, %out: f32):
      %20 = arith.addf %in, %in_8 : f32
      linalg.yield %20 : f32
    } -> tensor<1x2xf32>
    %17 = tensor.empty() : tensor<1x2xi8>
    %18 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel", "parallel"]} ins(%16 : tensor<1x2xf32>) outs(%17 : tensor<1x2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %20 = math.roundeven %in : f32
      %21 = arith.minf %20, %cst_7 : f32
      %22 = arith.maxf %21, %cst_6 : f32
      %23 = arith.fptosi %22 : f32 to i8
      linalg.yield %23 : i8
    } -> tensor<1x2xi8>
    %19 = hal.tensor.export %18 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %19 : !hal.buffer_view
  }
}


// -----// IR Dump After ConvertElementwiseToLinalg (convert-elementwise-to-linalg) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant dense<-1.000000e+00> : tensor<1xf32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<1xf32>
  %cst_1 = arith.constant -3.40282347E+38 : f32
  %cst_2 = arith.constant 0.000000e+00 : f32
  %cst_3 = arith.constant 1.000000e+00 : f32
  %cst_4 = arith.constant dense<2.560000e+02> : tensor<1xf32>
  %cst_5 = arith.constant dense<-1.280000e+02> : tensor<1xf32>
  %cst_6 = arith.constant -1.280000e+02 : f32
  %cst_7 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = tensor.empty() : tensor<1x2xf32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %20 = arith.sitofp %in : i8 to f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%2, %cst : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.subf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%3, %cst_0 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %5 = tensor.empty() : tensor<1xf32>
  %6 = linalg.fill ins(%cst_1 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%4 : tensor<1x2xf32>) outs(%6 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.maxf %in, %out : f32
    linalg.yield %20 : f32
  } -> tensor<1xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%4, %7 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.subf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%8 : tensor<1x2xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = math.exp %in : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %10 = linalg.fill ins(%cst_2 : f32) outs(%5 : tensor<1xf32>) -> tensor<1xf32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>], iterator_types = ["parallel", "reduction"]} ins(%9 : tensor<1x2xf32>) outs(%10 : tensor<1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.addf %in, %out : f32
    linalg.yield %20 : f32
  } -> tensor<1xf32>
  %expanded = tensor.expand_shape %11 [[0, 1]] : tensor<1xf32> into tensor<1x1xf32>
  %12 = tensor.empty() : tensor<1x1xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x1xf32>) outs(%12 : tensor<1x1xf32>) {
  ^bb0(%in: f32, %out: f32):
    %20 = arith.divf %cst_3, %in : f32
    linalg.yield %20 : f32
  } -> tensor<1x1xf32>
  %collapsed = tensor.collapse_shape %13 [[0, 1]] : tensor<1x1xf32> into tensor<1xf32>
  %14 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%9, %collapsed : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %15 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%14, %cst_4 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.mulf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %16 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%15, %cst_5 : tensor<1x2xf32>, tensor<1xf32>) outs(%1 : tensor<1x2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %20 = arith.addf %in, %in_8 : f32
    linalg.yield %20 : f32
  } -> tensor<1x2xf32>
  %17 = tensor.empty() : tensor<1x2xi8>
  %18 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%16 : tensor<1x2xf32>) outs(%17 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %20 = math.roundeven %in : f32
    %21 = arith.minf %20, %cst_7 : f32
    %22 = arith.maxf %21, %cst_6 : f32
    %23 = arith.fptosi %22 : f32 to i8
    linalg.yield %23 : i8
  } -> tensor<1x2xi8>
  %19 = hal.tensor.export %18 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %19 : !hal.buffer_view
}

// -----// IR Dump After LinalgFoldUnitExtentDims (linalg-fold-unit-extent-dims) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant dense<-1.000000e+00> : tensor<f32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<f32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<f32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<f32>
  %cst_3 = arith.constant -3.40282347E+38 : f32
  %cst_4 = arith.constant 0.000000e+00 : f32
  %cst_5 = arith.constant 1.000000e+00 : f32
  %cst_6 = arith.constant -1.280000e+02 : f32
  %cst_7 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %collapsed = tensor.collapse_shape %0 [[0, 1]] : tensor<1x2xi8> into tensor<2xi8>
  %1 = tensor.empty() : tensor<2xf32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed : tensor<2xi8>) outs(%1 : tensor<2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %28 = arith.sitofp %in : i8 to f32
    linalg.yield %28 : f32
  } -> tensor<2xf32>
  %3 = tensor.empty() : tensor<2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2, %cst : tensor<2xf32>, tensor<f32>) outs(%3 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.subf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<2xf32>
  %5 = tensor.empty() : tensor<2xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%4, %cst_0 : tensor<2xf32>, tensor<f32>) outs(%5 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.mulf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<2xf32>
  %7 = tensor.empty() : tensor<f32>
  %8 = linalg.fill ins(%cst_3 : f32) outs(%7 : tensor<f32>) -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%6 : tensor<2xf32>) outs(%8 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = arith.maxf %in, %out : f32
    linalg.yield %28 : f32
  } -> tensor<f32>
  %10 = tensor.empty() : tensor<2xf32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%6, %9 : tensor<2xf32>, tensor<f32>) outs(%10 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.subf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<2xf32>
  %12 = tensor.empty() : tensor<2xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%11 : tensor<2xf32>) outs(%12 : tensor<2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = math.exp %in : f32
    linalg.yield %28 : f32
  } -> tensor<2xf32>
  %14 = tensor.empty() : tensor<f32>
  %15 = linalg.fill ins(%cst_4 : f32) outs(%14 : tensor<f32>) -> tensor<f32>
  %16 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%13 : tensor<2xf32>) outs(%15 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = arith.addf %in, %out : f32
    linalg.yield %28 : f32
  } -> tensor<f32>
  %17 = tensor.empty() : tensor<f32>
  %18 = linalg.generic {indexing_maps = [affine_map<() -> ()>, affine_map<() -> ()>], iterator_types = []} ins(%16 : tensor<f32>) outs(%17 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = arith.divf %cst_5, %in : f32
    linalg.yield %28 : f32
  } -> tensor<f32>
  %19 = tensor.empty() : tensor<2xf32>
  %20 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%13, %18 : tensor<2xf32>, tensor<f32>) outs(%19 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.mulf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<2xf32>
  %21 = tensor.empty() : tensor<2xf32>
  %22 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%20, %cst_1 : tensor<2xf32>, tensor<f32>) outs(%21 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.mulf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<2xf32>
  %23 = tensor.empty() : tensor<2xf32>
  %24 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%22, %cst_2 : tensor<2xf32>, tensor<f32>) outs(%23 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.addf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<2xf32>
  %25 = tensor.empty() : tensor<2xi8>
  %26 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%24 : tensor<2xf32>) outs(%25 : tensor<2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %28 = math.roundeven %in : f32
    %29 = arith.minf %28, %cst_7 : f32
    %30 = arith.maxf %29, %cst_6 : f32
    %31 = arith.fptosi %30 : f32 to i8
    linalg.yield %31 : i8
  } -> tensor<2xi8>
  %expanded = tensor.expand_shape %26 [[0, 1]] : tensor<2xi8> into tensor<1x2xi8>
  %27 = hal.tensor.export %expanded "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %27 : !hal.buffer_view
}

// -----// IR Dump After RaiseSpecialOps (iree-flow-raise-special-ops) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant dense<-1.000000e+00> : tensor<f32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<f32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<f32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<f32>
  %cst_3 = arith.constant -3.40282347E+38 : f32
  %cst_4 = arith.constant 0.000000e+00 : f32
  %cst_5 = arith.constant 1.000000e+00 : f32
  %cst_6 = arith.constant -1.280000e+02 : f32
  %cst_7 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %collapsed = tensor.collapse_shape %0 [[0, 1]] : tensor<1x2xi8> into tensor<2xi8>
  %1 = tensor.empty() : tensor<2xf32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed : tensor<2xi8>) outs(%1 : tensor<2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %28 = arith.sitofp %in : i8 to f32
    linalg.yield %28 : f32
  } -> tensor<2xf32>
  %3 = tensor.empty() : tensor<2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2, %cst : tensor<2xf32>, tensor<f32>) outs(%3 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.subf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<2xf32>
  %5 = tensor.empty() : tensor<2xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%4, %cst_0 : tensor<2xf32>, tensor<f32>) outs(%5 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.mulf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<2xf32>
  %7 = tensor.empty() : tensor<f32>
  %8 = linalg.fill ins(%cst_3 : f32) outs(%7 : tensor<f32>) -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%6 : tensor<2xf32>) outs(%8 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = arith.maxf %in, %out : f32
    linalg.yield %28 : f32
  } -> tensor<f32>
  %10 = tensor.empty() : tensor<2xf32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%6, %9 : tensor<2xf32>, tensor<f32>) outs(%10 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.subf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<2xf32>
  %12 = tensor.empty() : tensor<2xf32>
  %13 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%11 : tensor<2xf32>) outs(%12 : tensor<2xf32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = math.exp %in : f32
    linalg.yield %28 : f32
  } -> tensor<2xf32>
  %14 = tensor.empty() : tensor<f32>
  %15 = linalg.fill ins(%cst_4 : f32) outs(%14 : tensor<f32>) -> tensor<f32>
  %16 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%13 : tensor<2xf32>) outs(%15 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = arith.addf %in, %out : f32
    linalg.yield %28 : f32
  } -> tensor<f32>
  %17 = tensor.empty() : tensor<f32>
  %18 = linalg.generic {indexing_maps = [affine_map<() -> ()>, affine_map<() -> ()>], iterator_types = []} ins(%16 : tensor<f32>) outs(%17 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %28 = arith.divf %cst_5, %in : f32
    linalg.yield %28 : f32
  } -> tensor<f32>
  %19 = tensor.empty() : tensor<2xf32>
  %20 = iree_linalg_ext.softmax dimension(0) ins(%6 : tensor<2xf32>) outs(%19 : tensor<2xf32>) -> tensor<2xf32>
  %21 = tensor.empty() : tensor<2xf32>
  %22 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%20, %cst_1 : tensor<2xf32>, tensor<f32>) outs(%21 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.mulf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<2xf32>
  %23 = tensor.empty() : tensor<2xf32>
  %24 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%22, %cst_2 : tensor<2xf32>, tensor<f32>) outs(%23 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_8: f32, %out: f32):
    %28 = arith.addf %in, %in_8 : f32
    linalg.yield %28 : f32
  } -> tensor<2xf32>
  %25 = tensor.empty() : tensor<2xi8>
  %26 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%24 : tensor<2xf32>) outs(%25 : tensor<2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %28 = math.roundeven %in : f32
    %29 = arith.minf %28, %cst_7 : f32
    %30 = arith.maxf %29, %cst_6 : f32
    %31 = arith.fptosi %30 : f32 to i8
    linalg.yield %31 : i8
  } -> tensor<2xi8>
  %expanded = tensor.expand_shape %26 [[0, 1]] : tensor<2xi8> into tensor<1x2xi8>
  %27 = hal.tensor.export %expanded "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %27 : !hal.buffer_view
}

// -----// IR Dump After InterchangeGenericOps (iree-flow-interchange-generic-ops) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant dense<-1.000000e+00> : tensor<f32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<f32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<f32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<f32>
  %cst_3 = arith.constant -1.280000e+02 : f32
  %cst_4 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %collapsed = tensor.collapse_shape %0 [[0, 1]] : tensor<1x2xi8> into tensor<2xi8>
  %1 = tensor.empty() : tensor<2xf32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed : tensor<2xi8>) outs(%1 : tensor<2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %16 = arith.sitofp %in : i8 to f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %3 = tensor.empty() : tensor<2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2, %cst : tensor<2xf32>, tensor<f32>) outs(%3 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %16 = arith.subf %in, %in_5 : f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %5 = tensor.empty() : tensor<2xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%4, %cst_0 : tensor<2xf32>, tensor<f32>) outs(%5 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %16 = arith.mulf %in, %in_5 : f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %7 = tensor.empty() : tensor<2xf32>
  %8 = iree_linalg_ext.softmax dimension(0) ins(%6 : tensor<2xf32>) outs(%7 : tensor<2xf32>) -> tensor<2xf32>
  %9 = tensor.empty() : tensor<2xf32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%8, %cst_1 : tensor<2xf32>, tensor<f32>) outs(%9 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %16 = arith.mulf %in, %in_5 : f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %11 = tensor.empty() : tensor<2xf32>
  %12 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%10, %cst_2 : tensor<2xf32>, tensor<f32>) outs(%11 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %16 = arith.addf %in, %in_5 : f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %13 = tensor.empty() : tensor<2xi8>
  %14 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%12 : tensor<2xf32>) outs(%13 : tensor<2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %16 = math.roundeven %in : f32
    %17 = arith.minf %16, %cst_4 : f32
    %18 = arith.maxf %17, %cst_3 : f32
    %19 = arith.fptosi %18 : f32 to i8
    linalg.yield %19 : i8
  } -> tensor<2xi8>
  %expanded = tensor.expand_shape %14 [[0, 1]] : tensor<2xi8> into tensor<1x2xi8>
  %15 = hal.tensor.export %expanded "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %15 : !hal.buffer_view
}

// -----// IR Dump After CollapseDims (iree-flow-collapse-dims) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant dense<-1.000000e+00> : tensor<f32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<f32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<f32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<f32>
  %cst_3 = arith.constant -1.280000e+02 : f32
  %cst_4 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %collapsed = tensor.collapse_shape %0 [[0, 1]] : tensor<1x2xi8> into tensor<2xi8>
  %1 = tensor.empty() : tensor<2xf32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed : tensor<2xi8>) outs(%1 : tensor<2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %16 = arith.sitofp %in : i8 to f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %3 = tensor.empty() : tensor<2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2, %cst : tensor<2xf32>, tensor<f32>) outs(%3 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %16 = arith.subf %in, %in_5 : f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %5 = tensor.empty() : tensor<2xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%4, %cst_0 : tensor<2xf32>, tensor<f32>) outs(%5 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %16 = arith.mulf %in, %in_5 : f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %7 = tensor.empty() : tensor<2xf32>
  %8 = iree_linalg_ext.softmax dimension(0) ins(%6 : tensor<2xf32>) outs(%7 : tensor<2xf32>) -> tensor<2xf32>
  %9 = tensor.empty() : tensor<2xf32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%8, %cst_1 : tensor<2xf32>, tensor<f32>) outs(%9 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %16 = arith.mulf %in, %in_5 : f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %11 = tensor.empty() : tensor<2xf32>
  %12 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%10, %cst_2 : tensor<2xf32>, tensor<f32>) outs(%11 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %16 = arith.addf %in, %in_5 : f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %13 = tensor.empty() : tensor<2xi8>
  %14 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%12 : tensor<2xf32>) outs(%13 : tensor<2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %16 = math.roundeven %in : f32
    %17 = arith.minf %16, %cst_4 : f32
    %18 = arith.maxf %17, %cst_3 : f32
    %19 = arith.fptosi %18 : f32 to i8
    linalg.yield %19 : i8
  } -> tensor<2xi8>
  %expanded = tensor.expand_shape %14 [[0, 1]] : tensor<2xi8> into tensor<1x2xi8>
  %15 = hal.tensor.export %expanded "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %15 : !hal.buffer_view
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant dense<-1.000000e+00> : tensor<f32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<f32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<f32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<f32>
  %cst_3 = arith.constant -1.280000e+02 : f32
  %cst_4 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %collapsed = tensor.collapse_shape %0 [[0, 1]] : tensor<1x2xi8> into tensor<2xi8>
  %1 = tensor.empty() : tensor<2xf32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed : tensor<2xi8>) outs(%1 : tensor<2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %16 = arith.sitofp %in : i8 to f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %3 = tensor.empty() : tensor<2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2, %cst : tensor<2xf32>, tensor<f32>) outs(%3 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %16 = arith.subf %in, %in_5 : f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %5 = tensor.empty() : tensor<2xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%4, %cst_0 : tensor<2xf32>, tensor<f32>) outs(%5 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %16 = arith.mulf %in, %in_5 : f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %7 = tensor.empty() : tensor<2xf32>
  %8 = iree_linalg_ext.softmax dimension(0) ins(%6 : tensor<2xf32>) outs(%7 : tensor<2xf32>) -> tensor<2xf32>
  %9 = tensor.empty() : tensor<2xf32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%8, %cst_1 : tensor<2xf32>, tensor<f32>) outs(%9 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %16 = arith.mulf %in, %in_5 : f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %11 = tensor.empty() : tensor<2xf32>
  %12 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%10, %cst_2 : tensor<2xf32>, tensor<f32>) outs(%11 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %16 = arith.addf %in, %in_5 : f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %13 = tensor.empty() : tensor<2xi8>
  %14 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%12 : tensor<2xf32>) outs(%13 : tensor<2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %16 = math.roundeven %in : f32
    %17 = arith.minf %16, %cst_4 : f32
    %18 = arith.maxf %17, %cst_3 : f32
    %19 = arith.fptosi %18 : f32 to i8
    linalg.yield %19 : i8
  } -> tensor<2xi8>
  %expanded = tensor.expand_shape %14 [[0, 1]] : tensor<2xi8> into tensor<1x2xi8>
  %15 = hal.tensor.export %expanded "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %15 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant dense<-1.000000e+00> : tensor<f32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<f32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<f32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<f32>
  %cst_3 = arith.constant -1.280000e+02 : f32
  %cst_4 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %collapsed = tensor.collapse_shape %0 [[0, 1]] : tensor<1x2xi8> into tensor<2xi8>
  %1 = tensor.empty() : tensor<2xf32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed : tensor<2xi8>) outs(%1 : tensor<2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %16 = arith.sitofp %in : i8 to f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %3 = tensor.empty() : tensor<2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2, %cst : tensor<2xf32>, tensor<f32>) outs(%3 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %16 = arith.subf %in, %in_5 : f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %5 = tensor.empty() : tensor<2xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%4, %cst_0 : tensor<2xf32>, tensor<f32>) outs(%5 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %16 = arith.mulf %in, %in_5 : f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %7 = tensor.empty() : tensor<2xf32>
  %8 = iree_linalg_ext.softmax dimension(0) ins(%6 : tensor<2xf32>) outs(%7 : tensor<2xf32>) -> tensor<2xf32>
  %9 = tensor.empty() : tensor<2xf32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%8, %cst_1 : tensor<2xf32>, tensor<f32>) outs(%9 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %16 = arith.mulf %in, %in_5 : f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %11 = tensor.empty() : tensor<2xf32>
  %12 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%10, %cst_2 : tensor<2xf32>, tensor<f32>) outs(%11 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %16 = arith.addf %in, %in_5 : f32
    linalg.yield %16 : f32
  } -> tensor<2xf32>
  %13 = tensor.empty() : tensor<2xi8>
  %14 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%12 : tensor<2xf32>) outs(%13 : tensor<2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %16 = math.roundeven %in : f32
    %17 = arith.minf %16, %cst_4 : f32
    %18 = arith.maxf %17, %cst_3 : f32
    %19 = arith.fptosi %18 : f32 to i8
    linalg.yield %19 : i8
  } -> tensor<2xi8>
  %expanded = tensor.expand_shape %14 [[0, 1]] : tensor<2xi8> into tensor<1x2xi8>
  %15 = hal.tensor.export %expanded "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %15 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant dense<-1.000000e+00> : tensor<f32>
  %cst_0 = arith.constant dense<0.0125187514> : tensor<f32>
  %cst_1 = arith.constant dense<2.560000e+02> : tensor<f32>
  %cst_2 = arith.constant dense<-1.280000e+02> : tensor<f32>
  %cst_3 = arith.constant -1.280000e+02 : f32
  %cst_4 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %collapsed = tensor.collapse_shape %0 [[0, 1]] : tensor<1x2xi8> into tensor<2xi8>
  %1 = tensor.empty() : tensor<2xf32>
  %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed : tensor<2xi8>) outs(%1 : tensor<2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %11 = arith.sitofp %in : i8 to f32
    linalg.yield %11 : f32
  } -> tensor<2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2, %cst : tensor<2xf32>, tensor<f32>) outs(%1 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %11 = arith.subf %in, %in_5 : f32
    linalg.yield %11 : f32
  } -> tensor<2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %cst_0 : tensor<2xf32>, tensor<f32>) outs(%1 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %11 = arith.mulf %in, %in_5 : f32
    linalg.yield %11 : f32
  } -> tensor<2xf32>
  %5 = iree_linalg_ext.softmax dimension(0) ins(%4 : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5, %cst_1 : tensor<2xf32>, tensor<f32>) outs(%1 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %11 = arith.mulf %in, %in_5 : f32
    linalg.yield %11 : f32
  } -> tensor<2xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%6, %cst_2 : tensor<2xf32>, tensor<f32>) outs(%1 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_5: f32, %out: f32):
    %11 = arith.addf %in, %in_5 : f32
    linalg.yield %11 : f32
  } -> tensor<2xf32>
  %8 = tensor.empty() : tensor<2xi8>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7 : tensor<2xf32>) outs(%8 : tensor<2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %11 = math.roundeven %in : f32
    %12 = arith.minf %11, %cst_4 : f32
    %13 = arith.maxf %12, %cst_3 : f32
    %14 = arith.fptosi %13 : f32 to i8
    linalg.yield %14 : i8
  } -> tensor<2xi8>
  %expanded = tensor.expand_shape %9 [[0, 1]] : tensor<2xi8> into tensor<1x2xi8>
  %10 = hal.tensor.export %expanded "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %10 : !hal.buffer_view
}

// -----// IR Dump After FusionOfTensorOps (iree-flow-fusion-of-tensor-ops) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %cst_1 = arith.constant 2.560000e+02 : f32
  %cst_2 = arith.constant -1.280000e+02 : f32
  %cst_3 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = tensor.empty() : tensor<2xf32>
  %2 = tensor.empty() : tensor<1x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%2 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %8 = arith.sitofp %in : i8 to f32
    %9 = arith.subf %8, %cst : f32
    %10 = arith.mulf %9, %cst_0 : f32
    linalg.yield %10 : f32
  } -> tensor<1x2xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x2xf32> into tensor<2xf32>
  %4 = iree_linalg_ext.softmax dimension(0) ins(%collapsed : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
  %expanded = tensor.expand_shape %4 [[0, 1]] : tensor<2xf32> into tensor<1x2xf32>
  %5 = tensor.empty() : tensor<1x2xi8>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x2xf32>) outs(%5 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %8 = arith.mulf %in, %cst_1 : f32
    %9 = arith.addf %8, %cst_2 : f32
    %10 = math.roundeven %9 : f32
    %11 = arith.minf %10, %cst_3 : f32
    %12 = arith.maxf %11, %cst_2 : f32
    %13 = arith.fptosi %12 : f32 to i8
    linalg.yield %13 : i8
  } -> tensor<1x2xi8>
  %7 = hal.tensor.export %6 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %7 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %cst_1 = arith.constant 2.560000e+02 : f32
  %cst_2 = arith.constant -1.280000e+02 : f32
  %cst_3 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = tensor.empty() : tensor<2xf32>
  %2 = tensor.empty() : tensor<1x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%2 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %8 = arith.sitofp %in : i8 to f32
    %9 = arith.subf %8, %cst : f32
    %10 = arith.mulf %9, %cst_0 : f32
    linalg.yield %10 : f32
  } -> tensor<1x2xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x2xf32> into tensor<2xf32>
  %4 = iree_linalg_ext.softmax dimension(0) ins(%collapsed : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
  %expanded = tensor.expand_shape %4 [[0, 1]] : tensor<2xf32> into tensor<1x2xf32>
  %5 = tensor.empty() : tensor<1x2xi8>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x2xf32>) outs(%5 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %8 = arith.mulf %in, %cst_1 : f32
    %9 = arith.addf %8, %cst_2 : f32
    %10 = math.roundeven %9 : f32
    %11 = arith.minf %10, %cst_3 : f32
    %12 = arith.maxf %11, %cst_2 : f32
    %13 = arith.fptosi %12 : f32 to i8
    linalg.yield %13 : i8
  } -> tensor<1x2xi8>
  %7 = hal.tensor.export %6 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %7 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %cst_1 = arith.constant 2.560000e+02 : f32
  %cst_2 = arith.constant -1.280000e+02 : f32
  %cst_3 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = tensor.empty() : tensor<2xf32>
  %2 = tensor.empty() : tensor<1x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%2 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %8 = arith.sitofp %in : i8 to f32
    %9 = arith.subf %8, %cst : f32
    %10 = arith.mulf %9, %cst_0 : f32
    linalg.yield %10 : f32
  } -> tensor<1x2xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x2xf32> into tensor<2xf32>
  %4 = iree_linalg_ext.softmax dimension(0) ins(%collapsed : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
  %expanded = tensor.expand_shape %4 [[0, 1]] : tensor<2xf32> into tensor<1x2xf32>
  %5 = tensor.empty() : tensor<1x2xi8>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x2xf32>) outs(%5 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %8 = arith.mulf %in, %cst_1 : f32
    %9 = arith.addf %8, %cst_2 : f32
    %10 = math.roundeven %9 : f32
    %11 = arith.minf %10, %cst_3 : f32
    %12 = arith.maxf %11, %cst_2 : f32
    %13 = arith.fptosi %12 : f32 to i8
    linalg.yield %13 : i8
  } -> tensor<1x2xi8>
  %7 = hal.tensor.export %6 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %7 : !hal.buffer_view
}

// -----// IR Dump After SplitReduction (iree-flow-split-reduction-ops) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %cst_1 = arith.constant 2.560000e+02 : f32
  %cst_2 = arith.constant -1.280000e+02 : f32
  %cst_3 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = tensor.empty() : tensor<2xf32>
  %2 = tensor.empty() : tensor<1x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%2 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %8 = arith.sitofp %in : i8 to f32
    %9 = arith.subf %8, %cst : f32
    %10 = arith.mulf %9, %cst_0 : f32
    linalg.yield %10 : f32
  } -> tensor<1x2xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x2xf32> into tensor<2xf32>
  %4 = iree_linalg_ext.softmax dimension(0) ins(%collapsed : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
  %expanded = tensor.expand_shape %4 [[0, 1]] : tensor<2xf32> into tensor<1x2xf32>
  %5 = tensor.empty() : tensor<1x2xi8>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x2xf32>) outs(%5 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %8 = arith.mulf %in, %cst_1 : f32
    %9 = arith.addf %8, %cst_2 : f32
    %10 = math.roundeven %9 : f32
    %11 = arith.minf %10, %cst_3 : f32
    %12 = arith.maxf %11, %cst_2 : f32
    %13 = arith.fptosi %12 : f32 to i8
    linalg.yield %13 : i8
  } -> tensor<1x2xi8>
  %7 = hal.tensor.export %6 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %7 : !hal.buffer_view
}

// -----// IR Dump After InterchangeGenericOps (iree-flow-interchange-generic-ops) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %cst_1 = arith.constant 2.560000e+02 : f32
  %cst_2 = arith.constant -1.280000e+02 : f32
  %cst_3 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = tensor.empty() : tensor<2xf32>
  %2 = tensor.empty() : tensor<1x2xf32>
  %3 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%2 : tensor<1x2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %8 = arith.sitofp %in : i8 to f32
    %9 = arith.subf %8, %cst : f32
    %10 = arith.mulf %9, %cst_0 : f32
    linalg.yield %10 : f32
  } -> tensor<1x2xf32>
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x2xf32> into tensor<2xf32>
  %4 = iree_linalg_ext.softmax dimension(0) ins(%collapsed : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
  %expanded = tensor.expand_shape %4 [[0, 1]] : tensor<2xf32> into tensor<1x2xf32>
  %5 = tensor.empty() : tensor<1x2xi8>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x2xf32>) outs(%5 : tensor<1x2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %8 = arith.mulf %in, %cst_1 : f32
    %9 = arith.addf %8, %cst_2 : f32
    %10 = math.roundeven %9 : f32
    %11 = arith.minf %10, %cst_3 : f32
    %12 = arith.maxf %11, %cst_2 : f32
    %13 = arith.fptosi %12 : f32 to i8
    linalg.yield %13 : i8
  } -> tensor<1x2xi8>
  %7 = hal.tensor.export %6 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %7 : !hal.buffer_view
}

// -----// IR Dump After FormDispatchRegions (iree-flow-form-dispatch-regions) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %cst_1 = arith.constant 2.560000e+02 : f32
  %cst_2 = arith.constant -1.280000e+02 : f32
  %cst_3 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = tensor.empty() : tensor<2xf32>
  %2 = tensor.empty() : tensor<1x2xf32>
  %3 = flow.dispatch.region -> (tensor<1x2xf32>) {
    %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%0 : tensor<1x2xi8>) outs(%2 : tensor<1x2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %9 = arith.sitofp %in : i8 to f32
      %10 = arith.subf %9, %cst : f32
      %11 = arith.mulf %10, %cst_0 : f32
      linalg.yield %11 : f32
    } -> tensor<1x2xf32>
    flow.return %8 : tensor<1x2xf32>
  }
  %collapsed = tensor.collapse_shape %3 [[0, 1]] : tensor<1x2xf32> into tensor<2xf32>
  %4 = flow.dispatch.region -> (tensor<2xf32>) {
    %8 = iree_linalg_ext.softmax dimension(0) ins(%collapsed : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
    flow.return %8 : tensor<2xf32>
  }
  %expanded = tensor.expand_shape %4 [[0, 1]] : tensor<2xf32> into tensor<1x2xf32>
  %5 = tensor.empty() : tensor<1x2xi8>
  %6 = flow.dispatch.region -> (tensor<1x2xi8>) {
    %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1) -> (d0, d1)>, affine_map<(d0, d1) -> (d0, d1)>], iterator_types = ["parallel", "parallel"]} ins(%expanded : tensor<1x2xf32>) outs(%5 : tensor<1x2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %9 = arith.mulf %in, %cst_1 : f32
      %10 = arith.addf %9, %cst_2 : f32
      %11 = math.roundeven %10 : f32
      %12 = arith.minf %11, %cst_3 : f32
      %13 = arith.maxf %12, %cst_2 : f32
      %14 = arith.fptosi %13 : f32 to i8
      linalg.yield %14 : i8
    } -> tensor<1x2xi8>
    flow.return %8 : tensor<1x2xi8>
  }
  %7 = hal.tensor.export %6 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %7 : !hal.buffer_view
}

// -----// IR Dump After CollapseDimensions (iree-flow-collapse-dimensions) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %cst_1 = arith.constant 2.560000e+02 : f32
  %cst_2 = arith.constant -1.280000e+02 : f32
  %cst_3 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = tensor.empty() : tensor<2xf32>
  %collapsed = tensor.collapse_shape %0 [[0, 1]] : tensor<1x2xi8> into tensor<2xi8>
  %2 = tensor.empty() : tensor<2xf32>
  %3 = flow.dispatch.region -> (tensor<2xf32>) {
    %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed : tensor<2xi8>) outs(%2 : tensor<2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %9 = arith.sitofp %in : i8 to f32
      %10 = arith.subf %9, %cst : f32
      %11 = arith.mulf %10, %cst_0 : f32
      linalg.yield %11 : f32
    } -> tensor<2xf32>
    flow.return %8 : tensor<2xf32>
  }
  %4 = flow.dispatch.region -> (tensor<2xf32>) {
    %8 = iree_linalg_ext.softmax dimension(0) ins(%3 : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
    flow.return %8 : tensor<2xf32>
  }
  %5 = tensor.empty() : tensor<2xi8>
  %6 = flow.dispatch.region -> (tensor<2xi8>) {
    %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%4 : tensor<2xf32>) outs(%5 : tensor<2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %9 = arith.mulf %in, %cst_1 : f32
      %10 = arith.addf %9, %cst_2 : f32
      %11 = math.roundeven %10 : f32
      %12 = arith.minf %11, %cst_3 : f32
      %13 = arith.maxf %12, %cst_2 : f32
      %14 = arith.fptosi %13 : f32 to i8
      linalg.yield %14 : i8
    } -> tensor<2xi8>
    flow.return %8 : tensor<2xi8>
  }
  %expanded = tensor.expand_shape %6 [[0, 1]] : tensor<2xi8> into tensor<1x2xi8>
  %7 = hal.tensor.export %expanded "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %7 : !hal.buffer_view
}

// -----// IR Dump After CloneProducersIntoDispatchRegions (iree-flow-clone-producers-into-dispatch-regions) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %cst_1 = arith.constant 2.560000e+02 : f32
  %cst_2 = arith.constant -1.280000e+02 : f32
  %cst_3 = arith.constant 1.270000e+02 : f32
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = tensor.empty() : tensor<2xf32>
  %collapsed = tensor.collapse_shape %0 [[0, 1]] : tensor<1x2xi8> into tensor<2xi8>
  %2 = tensor.empty() : tensor<2xf32>
  %3 = flow.dispatch.region -> (tensor<2xf32>) {
    %8 = tensor.empty() : tensor<2xf32>
    %cst_4 = arith.constant 0.0125187514 : f32
    %cst_5 = arith.constant -1.000000e+00 : f32
    %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%collapsed : tensor<2xi8>) outs(%8 : tensor<2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %10 = arith.sitofp %in : i8 to f32
      %11 = arith.subf %10, %cst_5 : f32
      %12 = arith.mulf %11, %cst_4 : f32
      linalg.yield %12 : f32
    } -> tensor<2xf32>
    flow.return %9 : tensor<2xf32>
  }
  %4 = flow.dispatch.region -> (tensor<2xf32>) {
    %8 = tensor.empty() : tensor<2xf32>
    %9 = iree_linalg_ext.softmax dimension(0) ins(%3 : tensor<2xf32>) outs(%8 : tensor<2xf32>) -> tensor<2xf32>
    flow.return %9 : tensor<2xf32>
  }
  %5 = tensor.empty() : tensor<2xi8>
  %6 = flow.dispatch.region -> (tensor<2xi8>) {
    %8 = tensor.empty() : tensor<2xi8>
    %cst_4 = arith.constant 1.270000e+02 : f32
    %cst_5 = arith.constant -1.280000e+02 : f32
    %cst_6 = arith.constant 2.560000e+02 : f32
    %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%4 : tensor<2xf32>) outs(%8 : tensor<2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %10 = arith.mulf %in, %cst_6 : f32
      %11 = arith.addf %10, %cst_5 : f32
      %12 = math.roundeven %11 : f32
      %13 = arith.minf %12, %cst_4 : f32
      %14 = arith.maxf %13, %cst_5 : f32
      %15 = arith.fptosi %14 : f32 to i8
      linalg.yield %15 : i8
    } -> tensor<2xi8>
    flow.return %9 : tensor<2xi8>
  }
  %expanded = tensor.expand_shape %6 [[0, 1]] : tensor<2xi8> into tensor<1x2xi8>
  %7 = hal.tensor.export %expanded "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %7 : !hal.buffer_view
}

// -----// IR Dump After FormDispatchWorkgroups (iree-flow-form-dispatch-workgroups) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
  %2 = flow.dispatch.workgroups(%1) : (tensor<2xi8>) -> tensor<2xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<2xi8>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
    %cst = arith.constant -1.000000e+00 : f32
    %cst_0 = arith.constant 0.0125187514 : f32
    %7 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %8 = tensor.empty() : tensor<2xf32>
    %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7 : tensor<2xi8>) outs(%8 : tensor<2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %10 = arith.sitofp %in : i8 to f32
      %11 = arith.subf %10, %cst : f32
      %12 = arith.mulf %11, %cst_0 : f32
      linalg.yield %12 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %9, %arg2, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %3 = flow.dispatch.workgroups(%2) : (tensor<2xf32>) -> tensor<2xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
    %7 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %8 = tensor.empty() : tensor<2xf32>
    %9 = iree_linalg_ext.softmax dimension(0) ins(%7 : tensor<2xf32>) outs(%8 : tensor<2xf32>) -> tensor<2xf32>
    flow.dispatch.tensor.store %9, %arg2, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %4 = flow.dispatch.workgroups(%3) : (tensor<2xf32>) -> tensor<2xi8> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<2xi8>>) {
    %cst = arith.constant 2.560000e+02 : f32
    %cst_0 = arith.constant -1.280000e+02 : f32
    %cst_1 = arith.constant 1.270000e+02 : f32
    %7 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %8 = tensor.empty() : tensor<2xi8>
    %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7 : tensor<2xf32>) outs(%8 : tensor<2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %10 = arith.mulf %in, %cst : f32
      %11 = arith.addf %10, %cst_0 : f32
      %12 = math.roundeven %11 : f32
      %13 = arith.minf %12, %cst_1 : f32
      %14 = arith.maxf %13, %cst_0 : f32
      %15 = arith.fptosi %14 : f32 to i8
      linalg.yield %15 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %9, %arg2, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
  %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CaptureDispatchDynamicDims (iree-flow-capture-dispatch-dynamic-dims) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
  %2 = flow.dispatch.workgroups(%1) : (tensor<2xi8>) -> tensor<2xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<2xi8>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
    %cst = arith.constant -1.000000e+00 : f32
    %cst_0 = arith.constant 0.0125187514 : f32
    %7 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %8 = tensor.empty() : tensor<2xf32>
    %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7 : tensor<2xi8>) outs(%8 : tensor<2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %10 = arith.sitofp %in : i8 to f32
      %11 = arith.subf %10, %cst : f32
      %12 = arith.mulf %11, %cst_0 : f32
      linalg.yield %12 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %9, %arg2, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %3 = flow.dispatch.workgroups(%2) : (tensor<2xf32>) -> tensor<2xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
    %7 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %8 = tensor.empty() : tensor<2xf32>
    %9 = iree_linalg_ext.softmax dimension(0) ins(%7 : tensor<2xf32>) outs(%8 : tensor<2xf32>) -> tensor<2xf32>
    flow.dispatch.tensor.store %9, %arg2, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %4 = flow.dispatch.workgroups(%3) : (tensor<2xf32>) -> tensor<2xi8> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<2xi8>>) {
    %cst = arith.constant 2.560000e+02 : f32
    %cst_0 = arith.constant -1.280000e+02 : f32
    %cst_1 = arith.constant 1.270000e+02 : f32
    %7 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %8 = tensor.empty() : tensor<2xi8>
    %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7 : tensor<2xf32>) outs(%8 : tensor<2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %10 = arith.mulf %in, %cst : f32
      %11 = arith.addf %10, %cst_0 : f32
      %12 = math.roundeven %11 : f32
      %13 = arith.minf %12, %cst_1 : f32
      %14 = arith.maxf %13, %cst_0 : f32
      %15 = arith.fptosi %14 : f32 to i8
      linalg.yield %15 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %9, %arg2, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
  %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
  %2 = flow.dispatch.workgroups(%1) : (tensor<2xi8>) -> tensor<2xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<2xi8>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
    %cst = arith.constant -1.000000e+00 : f32
    %cst_0 = arith.constant 0.0125187514 : f32
    %7 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %8 = tensor.empty() : tensor<2xf32>
    %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7 : tensor<2xi8>) outs(%8 : tensor<2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %10 = arith.sitofp %in : i8 to f32
      %11 = arith.subf %10, %cst : f32
      %12 = arith.mulf %11, %cst_0 : f32
      linalg.yield %12 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %9, %arg2, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %3 = flow.dispatch.workgroups(%2) : (tensor<2xf32>) -> tensor<2xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
    %7 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %8 = tensor.empty() : tensor<2xf32>
    %9 = iree_linalg_ext.softmax dimension(0) ins(%7 : tensor<2xf32>) outs(%8 : tensor<2xf32>) -> tensor<2xf32>
    flow.dispatch.tensor.store %9, %arg2, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %4 = flow.dispatch.workgroups(%3) : (tensor<2xf32>) -> tensor<2xi8> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<2xi8>>) {
    %cst = arith.constant 2.560000e+02 : f32
    %cst_0 = arith.constant -1.280000e+02 : f32
    %cst_1 = arith.constant 1.270000e+02 : f32
    %7 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %8 = tensor.empty() : tensor<2xi8>
    %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7 : tensor<2xf32>) outs(%8 : tensor<2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %10 = arith.mulf %in, %cst : f32
      %11 = arith.addf %10, %cst_0 : f32
      %12 = math.roundeven %11 : f32
      %13 = arith.minf %12, %cst_1 : f32
      %14 = arith.maxf %13, %cst_0 : f32
      %15 = arith.fptosi %14 : f32 to i8
      linalg.yield %15 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %9, %arg2, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
  %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
  %2 = flow.dispatch.workgroups(%1) : (tensor<2xi8>) -> tensor<2xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<2xi8>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
    %cst = arith.constant -1.000000e+00 : f32
    %cst_0 = arith.constant 0.0125187514 : f32
    %7 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %8 = tensor.empty() : tensor<2xf32>
    %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7 : tensor<2xi8>) outs(%8 : tensor<2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %10 = arith.sitofp %in : i8 to f32
      %11 = arith.subf %10, %cst : f32
      %12 = arith.mulf %11, %cst_0 : f32
      linalg.yield %12 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %9, %arg2, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %3 = flow.dispatch.workgroups(%2) : (tensor<2xf32>) -> tensor<2xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
    %7 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %8 = tensor.empty() : tensor<2xf32>
    %9 = iree_linalg_ext.softmax dimension(0) ins(%7 : tensor<2xf32>) outs(%8 : tensor<2xf32>) -> tensor<2xf32>
    flow.dispatch.tensor.store %9, %arg2, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %4 = flow.dispatch.workgroups(%3) : (tensor<2xf32>) -> tensor<2xi8> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<2xi8>>) {
    %cst = arith.constant 2.560000e+02 : f32
    %cst_0 = arith.constant -1.280000e+02 : f32
    %cst_1 = arith.constant 1.270000e+02 : f32
    %7 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %8 = tensor.empty() : tensor<2xi8>
    %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7 : tensor<2xf32>) outs(%8 : tensor<2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %10 = arith.mulf %in, %cst : f32
      %11 = arith.addf %10, %cst_0 : f32
      %12 = math.roundeven %11 : f32
      %13 = arith.minf %12, %cst_1 : f32
      %14 = arith.maxf %13, %cst_0 : f32
      %15 = arith.fptosi %14 : f32 to i8
      linalg.yield %15 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %9, %arg2, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
  %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After InitializeEmptyTensors (iree-flow-initialize-empty-tensors) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
  %2 = flow.dispatch.workgroups(%1) : (tensor<2xi8>) -> tensor<2xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<2xi8>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
    %cst = arith.constant -1.000000e+00 : f32
    %cst_0 = arith.constant 0.0125187514 : f32
    %7 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %8 = tensor.empty() : tensor<2xf32>
    %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7 : tensor<2xi8>) outs(%8 : tensor<2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %10 = arith.sitofp %in : i8 to f32
      %11 = arith.subf %10, %cst : f32
      %12 = arith.mulf %11, %cst_0 : f32
      linalg.yield %12 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %9, %arg2, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %3 = flow.dispatch.workgroups(%2) : (tensor<2xf32>) -> tensor<2xf32> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
    %7 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %8 = tensor.empty() : tensor<2xf32>
    %9 = iree_linalg_ext.softmax dimension(0) ins(%7 : tensor<2xf32>) outs(%8 : tensor<2xf32>) -> tensor<2xf32>
    flow.dispatch.tensor.store %9, %arg2, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %4 = flow.dispatch.workgroups(%3) : (tensor<2xf32>) -> tensor<2xi8> =
      (%arg1: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg2: !flow.dispatch.tensor<writeonly:tensor<2xi8>>) {
    %cst = arith.constant 2.560000e+02 : f32
    %cst_0 = arith.constant -1.280000e+02 : f32
    %cst_1 = arith.constant 1.270000e+02 : f32
    %7 = flow.dispatch.tensor.load %arg1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %8 = tensor.empty() : tensor<2xi8>
    %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%7 : tensor<2xf32>) outs(%8 : tensor<2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %10 = arith.mulf %in, %cst : f32
      %11 = arith.addf %10, %cst_0 : f32
      %12 = math.roundeven %11 : f32
      %13 = arith.minf %12, %cst_1 : f32
      %14 = arith.maxf %13, %cst_0 : f32
      %15 = arith.fptosi %14 : f32 to i8
      linalg.yield %15 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %9, %arg2, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
  %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After OutlineDispatchRegions (iree-flow-outline-dispatch-regions) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xi8>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %1 = tensor.empty() : tensor<2xf32>
        %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%0 : tensor<2xi8>) outs(%1 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %3 = arith.sitofp %in : i8 to f32
          %4 = arith.subf %3, %cst : f32
          %5 = arith.mulf %4, %cst_0 : f32
          linalg.yield %5 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  flow.executable private @main_dispatch_1 {
    flow.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %1 = tensor.empty() : tensor<2xf32>
        %2 = iree_linalg_ext.softmax dimension(0) ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  flow.executable private @main_dispatch_2 {
    flow.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xi8>>) {
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %1 = tensor.empty() : tensor<2xi8>
        %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %3 = arith.mulf %in, %cst : f32
          %4 = arith.addf %3, %cst_0 : f32
          %5 = math.roundeven %4 : f32
          %6 = arith.minf %5, %cst_1 : f32
          %7 = arith.maxf %6, %cst_0 : f32
          %8 = arith.fptosi %7 : f32 to i8
          linalg.yield %8 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
    %2 = flow.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1) : (tensor<2xi8>) -> tensor<2xf32>
    %3 = flow.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2) : (tensor<2xf32>) -> tensor<2xf32>
    %4 = flow.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3) : (tensor<2xf32>) -> tensor<2xi8>
    %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
    %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
  %2 = flow.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1) : (tensor<2xi8>) -> tensor<2xf32>
  %3 = flow.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2) : (tensor<2xf32>) -> tensor<2xf32>
  %4 = flow.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3) : (tensor<2xf32>) -> tensor<2xi8>
  %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
  %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After StripDebugOps (iree-util-strip-debug-ops) //----- //
flow.executable private @main_dispatch_0 {
  flow.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xi8>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
      %cst = arith.constant -1.000000e+00 : f32
      %cst_0 = arith.constant 0.0125187514 : f32
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
      %1 = tensor.empty() : tensor<2xf32>
      %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%0 : tensor<2xi8>) outs(%1 : tensor<2xf32>) {
      ^bb0(%in: i8, %out: f32):
        %3 = arith.sitofp %in : i8 to f32
        %4 = arith.subf %3, %cst : f32
        %5 = arith.mulf %4, %cst_0 : f32
        linalg.yield %5 : f32
      } -> tensor<2xf32>
      flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
      return
    }
  }
}

// -----// IR Dump After StripDebugOps (iree-util-strip-debug-ops) //----- //
flow.executable private @main_dispatch_2 {
  flow.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xi8>>) {
      %cst = arith.constant 2.560000e+02 : f32
      %cst_0 = arith.constant -1.280000e+02 : f32
      %cst_1 = arith.constant 1.270000e+02 : f32
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
      %1 = tensor.empty() : tensor<2xi8>
      %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xi8>) {
      ^bb0(%in: f32, %out: i8):
        %3 = arith.mulf %in, %cst : f32
        %4 = arith.addf %3, %cst_0 : f32
        %5 = math.roundeven %4 : f32
        %6 = arith.minf %5, %cst_1 : f32
        %7 = arith.maxf %6, %cst_0 : f32
        %8 = arith.fptosi %7 : f32 to i8
        linalg.yield %8 : i8
      } -> tensor<2xi8>
      flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
      return
    }
  }
}

// -----// IR Dump After StripDebugOps (iree-util-strip-debug-ops) //----- //
flow.executable private @main_dispatch_1 {
  flow.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_1_softmax_2xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
      %1 = tensor.empty() : tensor<2xf32>
      %2 = iree_linalg_ext.softmax dimension(0) ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
      flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
      return
    }
  }
}

// -----// IR Dump After DeduplicateExecutables (iree-flow-deduplicate-executables) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xi8>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %1 = tensor.empty() : tensor<2xf32>
        %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%0 : tensor<2xi8>) outs(%1 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %3 = arith.sitofp %in : i8 to f32
          %4 = arith.subf %3, %cst : f32
          %5 = arith.mulf %4, %cst_0 : f32
          linalg.yield %5 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  flow.executable private @main_dispatch_1 {
    flow.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %1 = tensor.empty() : tensor<2xf32>
        %2 = iree_linalg_ext.softmax dimension(0) ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  flow.executable private @main_dispatch_2 {
    flow.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xi8>>) {
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %1 = tensor.empty() : tensor<2xi8>
        %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %3 = arith.mulf %in, %cst : f32
          %4 = arith.addf %3, %cst_0 : f32
          %5 = math.roundeven %4 : f32
          %6 = arith.minf %5, %cst_1 : f32
          %7 = arith.maxf %6, %cst_0 : f32
          %8 = arith.fptosi %7 : f32 to i8
          linalg.yield %8 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
    %2 = flow.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1) : (tensor<2xi8>) -> tensor<2xf32>
    %3 = flow.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2) : (tensor<2xf32>) -> tensor<2xf32>
    %4 = flow.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3) : (tensor<2xf32>) -> tensor<2xi8>
    %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
    %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After CleanupTensorShapes (iree-flow-cleanup-tensor-shapes) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
  %2 = flow.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1) : (tensor<2xi8>) -> tensor<2xf32>
  %3 = flow.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2) : (tensor<2xf32>) -> tensor<2xf32>
  %4 = flow.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3) : (tensor<2xf32>) -> tensor<2xi8>
  %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
  %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
flow.executable private @main_dispatch_1 {
  flow.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_1_softmax_2xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
      %1 = tensor.empty() : tensor<2xf32>
      %2 = iree_linalg_ext.softmax dimension(0) ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
      flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
      return
    }
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
flow.executable private @main_dispatch_2 {
  flow.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xi8>>) {
      %cst = arith.constant 2.560000e+02 : f32
      %cst_0 = arith.constant -1.280000e+02 : f32
      %cst_1 = arith.constant 1.270000e+02 : f32
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
      %1 = tensor.empty() : tensor<2xi8>
      %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xi8>) {
      ^bb0(%in: f32, %out: i8):
        %3 = arith.mulf %in, %cst : f32
        %4 = arith.addf %3, %cst_0 : f32
        %5 = math.roundeven %4 : f32
        %6 = arith.minf %5, %cst_1 : f32
        %7 = arith.maxf %6, %cst_0 : f32
        %8 = arith.fptosi %7 : f32 to i8
        linalg.yield %8 : i8
      } -> tensor<2xi8>
      flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
      return
    }
  }
}

// -----// IR Dump After CSE (cse) //----- //
flow.executable private @main_dispatch_2 {
  flow.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xi8>>) {
      %cst = arith.constant 2.560000e+02 : f32
      %cst_0 = arith.constant -1.280000e+02 : f32
      %cst_1 = arith.constant 1.270000e+02 : f32
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
      %1 = tensor.empty() : tensor<2xi8>
      %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xi8>) {
      ^bb0(%in: f32, %out: i8):
        %3 = arith.mulf %in, %cst : f32
        %4 = arith.addf %3, %cst_0 : f32
        %5 = math.roundeven %4 : f32
        %6 = arith.minf %5, %cst_1 : f32
        %7 = arith.maxf %6, %cst_0 : f32
        %8 = arith.fptosi %7 : f32 to i8
        linalg.yield %8 : i8
      } -> tensor<2xi8>
      flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
      return
    }
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
  %2 = flow.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1) : (tensor<2xi8>) -> tensor<2xf32>
  %3 = flow.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2) : (tensor<2xf32>) -> tensor<2xf32>
  %4 = flow.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3) : (tensor<2xf32>) -> tensor<2xi8>
  %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
  %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
flow.executable private @main_dispatch_1 {
  flow.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_1_softmax_2xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
      %1 = tensor.empty() : tensor<2xf32>
      %2 = iree_linalg_ext.softmax dimension(0) ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
      flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
      return
    }
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
flow.executable private @main_dispatch_0 {
  flow.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xi8>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
      %cst = arith.constant -1.000000e+00 : f32
      %cst_0 = arith.constant 0.0125187514 : f32
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
      %1 = tensor.empty() : tensor<2xf32>
      %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%0 : tensor<2xi8>) outs(%1 : tensor<2xf32>) {
      ^bb0(%in: i8, %out: f32):
        %3 = arith.sitofp %in : i8 to f32
        %4 = arith.subf %3, %cst : f32
        %5 = arith.mulf %4, %cst_0 : f32
        linalg.yield %5 : f32
      } -> tensor<2xf32>
      flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
      return
    }
  }
}

// -----// IR Dump After CSE (cse) //----- //
flow.executable private @main_dispatch_0 {
  flow.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xi8>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
      %cst = arith.constant -1.000000e+00 : f32
      %cst_0 = arith.constant 0.0125187514 : f32
      %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
      %1 = tensor.empty() : tensor<2xf32>
      %2 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%0 : tensor<2xi8>) outs(%1 : tensor<2xf32>) {
      ^bb0(%in: i8, %out: f32):
        %3 = arith.sitofp %in : i8 to f32
        %4 = arith.subf %3, %cst : f32
        %5 = arith.mulf %4, %cst_0 : f32
        linalg.yield %5 : f32
      } -> tensor<2xf32>
      flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
      return
    }
  }
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
  %2 = flow.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1) : (tensor<2xi8>) -> tensor<2xf32>
  %3 = flow.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2) : (tensor<2xf32>) -> tensor<2xf32>
  %4 = flow.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3) : (tensor<2xf32>) -> tensor<2xi8>
  %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
  %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xi8>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %1 = tensor.empty() : tensor<2xf32>
        %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%0 : tensor<2xi8>) outs(%1 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %3 = arith.sitofp %in : i8 to f32
          %4 = arith.subf %3, %cst : f32
          %5 = arith.mulf %4, %cst_0 : f32
          linalg.yield %5 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  flow.executable private @main_dispatch_1 {
    flow.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %1 = tensor.empty() : tensor<2xf32>
        %2 = iree_linalg_ext.softmax dimension(0) ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  flow.executable private @main_dispatch_2 {
    flow.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xi8>>) {
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %1 = tensor.empty() : tensor<2xi8>
        %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %3 = arith.mulf %in, %cst : f32
          %4 = arith.addf %3, %cst_0 : f32
          %5 = math.roundeven %4 : f32
          %6 = arith.minf %5, %cst_1 : f32
          %7 = arith.maxf %6, %cst_0 : f32
          %8 = arith.fptosi %7 : f32 to i8
          linalg.yield %8 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
    %2 = flow.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1) : (tensor<2xi8>) -> tensor<2xf32>
    %3 = flow.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2) : (tensor<2xf32>) -> tensor<2xf32>
    %4 = flow.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3) : (tensor<2xf32>) -> tensor<2xi8>
    %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
    %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyInput (iree-stream-verify-input) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xi8>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %1 = tensor.empty() : tensor<2xf32>
        %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%0 : tensor<2xi8>) outs(%1 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %3 = arith.sitofp %in : i8 to f32
          %4 = arith.subf %3, %cst : f32
          %5 = arith.mulf %4, %cst_0 : f32
          linalg.yield %5 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  flow.executable private @main_dispatch_1 {
    flow.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %1 = tensor.empty() : tensor<2xf32>
        %2 = iree_linalg_ext.softmax dimension(0) ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  flow.executable private @main_dispatch_2 {
    flow.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xi8>>) {
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %1 = tensor.empty() : tensor<2xi8>
        %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %3 = arith.mulf %in, %cst : f32
          %4 = arith.addf %3, %cst_0 : f32
          %5 = math.roundeven %4 : f32
          %6 = arith.minf %5, %cst_1 : f32
          %7 = arith.maxf %6, %cst_0 : f32
          %8 = arith.fptosi %7 : f32 to i8
          linalg.yield %8 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
    %2 = flow.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1) : (tensor<2xi8>) -> tensor<2xf32>
    %3 = flow.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2) : (tensor<2xf32>) -> tensor<2xf32>
    %4 = flow.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3) : (tensor<2xf32>) -> tensor<2xi8>
    %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
    %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After OutlineConstants (iree-stream-outline-constants) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xi8>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %1 = tensor.empty() : tensor<2xf32>
        %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%0 : tensor<2xi8>) outs(%1 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %3 = arith.sitofp %in : i8 to f32
          %4 = arith.subf %3, %cst : f32
          %5 = arith.mulf %4, %cst_0 : f32
          linalg.yield %5 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  flow.executable private @main_dispatch_1 {
    flow.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %1 = tensor.empty() : tensor<2xf32>
        %2 = iree_linalg_ext.softmax dimension(0) ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  flow.executable private @main_dispatch_2 {
    flow.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xi8>>) {
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %1 = tensor.empty() : tensor<2xi8>
        %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %3 = arith.mulf %in, %cst : f32
          %4 = arith.addf %3, %cst_0 : f32
          %5 = math.roundeven %4 : f32
          %6 = arith.minf %5, %cst_1 : f32
          %7 = arith.maxf %6, %cst_0 : f32
          %8 = arith.fptosi %7 : f32 to i8
          linalg.yield %8 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
    %2 = flow.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1) : (tensor<2xi8>) -> tensor<2xf32>
    %3 = flow.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2) : (tensor<2xf32>) -> tensor<2xf32>
    %4 = flow.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3) : (tensor<2xf32>) -> tensor<2xi8>
    %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
    %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
  %2 = flow.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1) : (tensor<2xi8>) -> tensor<2xf32>
  %3 = flow.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2) : (tensor<2xf32>) -> tensor<2xf32>
  %4 = flow.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3) : (tensor<2xf32>) -> tensor<2xi8>
  %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
  %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
  %2 = flow.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1) : (tensor<2xi8>) -> tensor<2xf32>
  %3 = flow.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2) : (tensor<2xf32>) -> tensor<2xf32>
  %4 = flow.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3) : (tensor<2xf32>) -> tensor<2xi8>
  %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
  %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
  %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
  %2 = flow.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1) : (tensor<2xi8>) -> tensor<2xf32>
  %3 = flow.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2) : (tensor<2xf32>) -> tensor<2xf32>
  %4 = flow.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3) : (tensor<2xf32>) -> tensor<2xi8>
  %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
  %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xi8>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %1 = tensor.empty() : tensor<2xf32>
        %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%0 : tensor<2xi8>) outs(%1 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %3 = arith.sitofp %in : i8 to f32
          %4 = arith.subf %3, %cst : f32
          %5 = arith.mulf %4, %cst_0 : f32
          linalg.yield %5 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  flow.executable private @main_dispatch_1 {
    flow.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %1 = tensor.empty() : tensor<2xf32>
        %2 = iree_linalg_ext.softmax dimension(0) ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  flow.executable private @main_dispatch_2 {
    flow.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xi8>>) {
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %1 = tensor.empty() : tensor<2xi8>
        %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %3 = arith.mulf %in, %cst : f32
          %4 = arith.addf %3, %cst_0 : f32
          %5 = math.roundeven %4 : f32
          %6 = arith.minf %5, %cst_1 : f32
          %7 = arith.maxf %6, %cst_0 : f32
          %8 = arith.fptosi %7 : f32 to i8
          linalg.yield %8 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
    %2 = flow.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1) : (tensor<2xi8>) -> tensor<2xf32>
    %3 = flow.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2) : (tensor<2xf32>) -> tensor<2xf32>
    %4 = flow.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3) : (tensor<2xf32>) -> tensor<2xi8>
    %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
    %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xi8>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %1 = tensor.empty() : tensor<2xf32>
        %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%0 : tensor<2xi8>) outs(%1 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %3 = arith.sitofp %in : i8 to f32
          %4 = arith.subf %3, %cst : f32
          %5 = arith.mulf %4, %cst_0 : f32
          linalg.yield %5 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  flow.executable private @main_dispatch_1 {
    flow.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %1 = tensor.empty() : tensor<2xf32>
        %2 = iree_linalg_ext.softmax dimension(0) ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  flow.executable private @main_dispatch_2 {
    flow.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xi8>>) {
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %1 = tensor.empty() : tensor<2xi8>
        %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %3 = arith.mulf %in, %cst : f32
          %4 = arith.addf %3, %cst_0 : f32
          %5 = math.roundeven %4 : f32
          %6 = arith.minf %5, %cst_1 : f32
          %7 = arith.maxf %6, %cst_0 : f32
          %8 = arith.fptosi %7 : f32 to i8
          linalg.yield %8 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
    %2 = flow.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1) : (tensor<2xi8>) -> tensor<2xf32>
    %3 = flow.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2) : (tensor<2xf32>) -> tensor<2xf32>
    %4 = flow.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3) : (tensor<2xf32>) -> tensor<2xi8>
    %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
    %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xi8>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %1 = tensor.empty() : tensor<2xf32>
        %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%0 : tensor<2xi8>) outs(%1 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %3 = arith.sitofp %in : i8 to f32
          %4 = arith.subf %3, %cst : f32
          %5 = arith.mulf %4, %cst_0 : f32
          linalg.yield %5 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  flow.executable private @main_dispatch_1 {
    flow.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %1 = tensor.empty() : tensor<2xf32>
        %2 = iree_linalg_ext.softmax dimension(0) ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  flow.executable private @main_dispatch_2 {
    flow.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xi8>>) {
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %1 = tensor.empty() : tensor<2xi8>
        %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %3 = arith.mulf %in, %cst : f32
          %4 = arith.addf %3, %cst_0 : f32
          %5 = math.roundeven %4 : f32
          %6 = arith.minf %5, %cst_1 : f32
          %7 = arith.maxf %6, %cst_0 : f32
          %8 = arith.fptosi %7 : f32 to i8
          linalg.yield %8 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
    %2 = flow.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1) : (tensor<2xi8>) -> tensor<2xf32>
    %3 = flow.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2) : (tensor<2xf32>) -> tensor<2xf32>
    %4 = flow.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3) : (tensor<2xf32>) -> tensor<2xi8>
    %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
    %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  flow.executable private @main_dispatch_0 {
    flow.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xi8>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %1 = tensor.empty() : tensor<2xf32>
        %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%0 : tensor<2xi8>) outs(%1 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %3 = arith.sitofp %in : i8 to f32
          %4 = arith.subf %3, %cst : f32
          %5 = arith.mulf %4, %cst_0 : f32
          linalg.yield %5 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  flow.executable private @main_dispatch_1 {
    flow.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xf32>>) {
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %1 = tensor.empty() : tensor<2xf32>
        %2 = iree_linalg_ext.softmax dimension(0) ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  flow.executable private @main_dispatch_2 {
    flow.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !flow.dispatch.tensor<readonly:tensor<2xf32>>, %arg1: !flow.dispatch.tensor<writeonly:tensor<2xi8>>) {
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = flow.dispatch.tensor.load %arg0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %1 = tensor.empty() : tensor<2xi8>
        %2 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%0 : tensor<2xf32>) outs(%1 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %3 = arith.mulf %in, %cst : f32
          %4 = arith.addf %3, %cst_0 : f32
          %5 = math.roundeven %4 : f32
          %6 = arith.minf %5, %cst_1 : f32
          %7 = arith.maxf %6, %cst_0 : f32
          %8 = arith.fptosi %7 : f32 to i8
          linalg.yield %8 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %2, %arg1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %0 = hal.tensor.import %arg0 "input 0" : !hal.buffer_view -> tensor<1x2xi8>
    %1 = flow.tensor.reshape %0 : tensor<1x2xi8> -> tensor<2xi8>
    %2 = flow.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1) : (tensor<2xi8>) -> tensor<2xf32>
    %3 = flow.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2) : (tensor<2xf32>) -> tensor<2xf32>
    %4 = flow.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3) : (tensor<2xf32>) -> tensor<2xi8>
    %5 = flow.tensor.reshape %4 : tensor<2xi8> -> tensor<1x2xi8>
    %6 = hal.tensor.export %5 "output 0" : tensor<1x2xi8> -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After ConvertToStream (iree-stream-conversion) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst : f32
          %7 = arith.mulf %6, %cst_0 : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst_1 : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<1x2xi8> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %3 = stream.tensor.sizeof tensor<2xi8> : index
    %4 = stream.tensor.clone %2 : tensor<1x2xi8> in !stream.resource<*>{%0} -> tensor<2xi8> in !stream.resource<*>{%3}
    %c0 = arith.constant 0 : index
    %5 = stream.tensor.sizeof tensor<2xf32> : index
    %6 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%4[%c0 to %3 for %3]) : (!stream.resource<*>{%3}) -> !stream.resource<*>{%5}
    %c0_0 = arith.constant 0 : index
    %7 = stream.tensor.sizeof tensor<2xf32> : index
    %8 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%6[%c0_0 to %5 for %5]) : (!stream.resource<*>{%5}) -> !stream.resource<*>{%7}
    %c0_1 = arith.constant 0 : index
    %9 = stream.tensor.sizeof tensor<2xi8> : index
    %10 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%8[%c0_1 to %7 for %7]) : (!stream.resource<*>{%7}) -> !stream.resource<*>{%9}
    %11 = stream.tensor.sizeof tensor<1x2xi8> : index
    %12 = stream.tensor.clone %10 : tensor<2xi8> in !stream.resource<*>{%9} -> tensor<1x2xi8> in !stream.resource<*>{%11}
    %13 = stream.async.transfer %12 : !stream.resource<*>{%11} -> !stream.resource<external>{%11}
    %14 = stream.tensor.export %13 : tensor<1x2xi8> in !stream.resource<external>{%11} -> !hal.buffer_view
    return %14 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToTensors (iree-stream-verify-lowering-to-tensors) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst : f32
          %7 = arith.mulf %6, %cst_0 : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst_1 : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<1x2xi8> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %3 = stream.tensor.sizeof tensor<2xi8> : index
    %4 = stream.tensor.clone %2 : tensor<1x2xi8> in !stream.resource<*>{%0} -> tensor<2xi8> in !stream.resource<*>{%3}
    %c0 = arith.constant 0 : index
    %5 = stream.tensor.sizeof tensor<2xf32> : index
    %6 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%4[%c0 to %3 for %3]) : (!stream.resource<*>{%3}) -> !stream.resource<*>{%5}
    %c0_0 = arith.constant 0 : index
    %7 = stream.tensor.sizeof tensor<2xf32> : index
    %8 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%6[%c0_0 to %5 for %5]) : (!stream.resource<*>{%5}) -> !stream.resource<*>{%7}
    %c0_1 = arith.constant 0 : index
    %9 = stream.tensor.sizeof tensor<2xi8> : index
    %10 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%8[%c0_1 to %7 for %7]) : (!stream.resource<*>{%7}) -> !stream.resource<*>{%9}
    %11 = stream.tensor.sizeof tensor<1x2xi8> : index
    %12 = stream.tensor.clone %10 : tensor<2xi8> in !stream.resource<*>{%9} -> tensor<1x2xi8> in !stream.resource<*>{%11}
    %13 = stream.async.transfer %12 : !stream.resource<*>{%11} -> !stream.resource<external>{%11}
    %14 = stream.tensor.export %13 : tensor<1x2xi8> in !stream.resource<external>{%11} -> !hal.buffer_view
    return %14 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.sizeof tensor<1x2xi8> : index
  %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%0}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %3 = stream.tensor.sizeof tensor<2xi8> : index
  %4 = stream.tensor.sizeof tensor<2xf32> : index
  %5 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%2[%c0 to %3 for %3]) : (!stream.resource<*>{%3}) -> !stream.resource<*>{%4}
  %6 = stream.tensor.sizeof tensor<2xf32> : index
  %7 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%5[%c0 to %4 for %4]) : (!stream.resource<*>{%4}) -> !stream.resource<*>{%6}
  %8 = stream.tensor.sizeof tensor<2xi8> : index
  %9 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%7[%c0 to %6 for %6]) : (!stream.resource<*>{%6}) -> !stream.resource<*>{%8}
  %10 = stream.tensor.sizeof tensor<1x2xi8> : index
  %11 = stream.async.transfer %9 : !stream.resource<*>{%10} -> !stream.resource<external>{%10}
  %12 = stream.tensor.export %11 : tensor<1x2xi8> in !stream.resource<external>{%10} -> !hal.buffer_view
  return %12 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.sizeof tensor<1x2xi8> : index
  %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%0}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %3 = stream.tensor.sizeof tensor<2xi8> : index
  %4 = stream.tensor.sizeof tensor<2xf32> : index
  %5 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%2[%c0 to %3 for %3]) : (!stream.resource<*>{%3}) -> !stream.resource<*>{%4}
  %6 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%5[%c0 to %4 for %4]) : (!stream.resource<*>{%4}) -> !stream.resource<*>{%4}
  %7 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%6[%c0 to %4 for %4]) : (!stream.resource<*>{%4}) -> !stream.resource<*>{%3}
  %8 = stream.async.transfer %7 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
  %9 = stream.tensor.export %8 : tensor<1x2xi8> in !stream.resource<external>{%0} -> !hal.buffer_view
  return %9 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  %c2 = arith.constant 2 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.sizeof tensor<1x2xi8> : index
  %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%0}
  %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %3 = stream.tensor.sizeof tensor<2xi8> : index
  %4 = stream.tensor.sizeof tensor<2xf32> : index
  %5 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%2[%c0 to %3 for %3]) : (!stream.resource<*>{%3}) -> !stream.resource<*>{%4}
  %6 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%5[%c0 to %4 for %4]) : (!stream.resource<*>{%4}) -> !stream.resource<*>{%4}
  %7 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%6[%c0 to %4 for %4]) : (!stream.resource<*>{%4}) -> !stream.resource<*>{%3}
  %8 = stream.async.transfer %7 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
  %9 = stream.tensor.export %8 : tensor<1x2xi8> in !stream.resource<external>{%0} -> !hal.buffer_view
  return %9 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<1x2xi8> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %3 = stream.tensor.sizeof tensor<2xi8> : index
    %4 = stream.tensor.sizeof tensor<2xf32> : index
    %5 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%2[%c0 to %3 for %3]) : (!stream.resource<*>{%3}) -> !stream.resource<*>{%4}
    %6 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%5[%c0 to %4 for %4]) : (!stream.resource<*>{%4}) -> !stream.resource<*>{%4}
    %7 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%6[%c0 to %4 for %4]) : (!stream.resource<*>{%4}) -> !stream.resource<*>{%3}
    %8 = stream.async.transfer %7 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %9 = stream.tensor.export %8 : tensor<1x2xi8> in !stream.resource<external>{%0} -> !hal.buffer_view
    return %9 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<1x2xi8> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %3 = stream.tensor.sizeof tensor<2xi8> : index
    %4 = stream.tensor.sizeof tensor<2xf32> : index
    %5 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%2[%c0 to %3 for %3]) : (!stream.resource<*>{%3}) -> !stream.resource<*>{%4}
    %6 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%5[%c0 to %4 for %4]) : (!stream.resource<*>{%4}) -> !stream.resource<*>{%4}
    %7 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%6[%c0 to %4 for %4]) : (!stream.resource<*>{%4}) -> !stream.resource<*>{%3}
    %8 = stream.async.transfer %7 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %9 = stream.tensor.export %8 : tensor<1x2xi8> in !stream.resource<external>{%0} -> !hal.buffer_view
    return %9 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<1x2xi8> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %3 = stream.tensor.sizeof tensor<2xi8> : index
    %4 = stream.tensor.sizeof tensor<2xf32> : index
    %5 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%2[%c0 to %3 for %3]) : (!stream.resource<*>{%3}) -> !stream.resource<*>{%4}
    %6 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%5[%c0 to %4 for %4]) : (!stream.resource<*>{%4}) -> !stream.resource<*>{%4}
    %7 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%6[%c0 to %4 for %4]) : (!stream.resource<*>{%4}) -> !stream.resource<*>{%3}
    %8 = stream.async.transfer %7 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %9 = stream.tensor.export %8 : tensor<1x2xi8> in !stream.resource<external>{%0} -> !hal.buffer_view
    return %9 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<1x2xi8> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %3 = stream.tensor.sizeof tensor<2xi8> : index
    %4 = stream.tensor.sizeof tensor<2xf32> : index
    %5 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%2[%c0 to %3 for %3]) : (!stream.resource<*>{%3}) -> !stream.resource<*>{%4}
    %6 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%5[%c0 to %4 for %4]) : (!stream.resource<*>{%4}) -> !stream.resource<*>{%4}
    %7 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%6[%c0 to %4 for %4]) : (!stream.resource<*>{%4}) -> !stream.resource<*>{%3}
    %8 = stream.async.transfer %7 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %9 = stream.tensor.export %8 : tensor<1x2xi8> in !stream.resource<external>{%0} -> !hal.buffer_view
    return %9 : !hal.buffer_view
  }
}


// -----// IR Dump After CombineInitializers (iree-util-combine-initializers) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.sizeof tensor<1x2xi8> : index
    %1 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %3 = stream.tensor.sizeof tensor<2xi8> : index
    %4 = stream.tensor.sizeof tensor<2xf32> : index
    %5 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%2[%c0 to %3 for %3]) : (!stream.resource<*>{%3}) -> !stream.resource<*>{%4}
    %6 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%5[%c0 to %4 for %4]) : (!stream.resource<*>{%4}) -> !stream.resource<*>{%4}
    %7 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%6[%c0 to %4 for %4]) : (!stream.resource<*>{%4}) -> !stream.resource<*>{%3}
    %8 = stream.async.transfer %7 : !stream.resource<*>{%0} -> !stream.resource<external>{%0}
    %9 = stream.tensor.export %8 : tensor<1x2xi8> in !stream.resource<external>{%0} -> !hal.buffer_view
    return %9 : !hal.buffer_view
  }
}


// -----// IR Dump After EncodeDeviceTensors (iree-stream-encode-device-tensors) //----- //
stream.executable private @main_dispatch_0 {
  stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    stream.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
      %cst = arith.constant 0.0125187514 : f32
      %cst_0 = arith.constant -1.000000e+00 : f32
      %c0 = arith.constant 0 : index
      %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
      %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
      %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
      %3 = tensor.empty() : tensor<2xf32>
      %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
      ^bb0(%in: i8, %out: f32):
        %5 = arith.sitofp %in : i8 to f32
        %6 = arith.subf %5, %cst_0 : f32
        %7 = arith.mulf %6, %cst : f32
        linalg.yield %7 : f32
      } -> tensor<2xf32>
      flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
      return
    }
  }
}

// -----// IR Dump After EncodeDeviceTensors (iree-stream-encode-device-tensors) //----- //
stream.executable private @main_dispatch_2 {
  stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    stream.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
      %cst = arith.constant 1.270000e+02 : f32
      %cst_0 = arith.constant -1.280000e+02 : f32
      %cst_1 = arith.constant 2.560000e+02 : f32
      %c0 = arith.constant 0 : index
      %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
      %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
      %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
      %3 = tensor.empty() : tensor<2xi8>
      %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
      ^bb0(%in: f32, %out: i8):
        %5 = arith.mulf %in, %cst_1 : f32
        %6 = arith.addf %5, %cst_0 : f32
        %7 = math.roundeven %6 : f32
        %8 = arith.minf %7, %cst : f32
        %9 = arith.maxf %8, %cst_0 : f32
        %10 = arith.fptosi %9 : f32 to i8
        linalg.yield %10 : i8
      } -> tensor<2xi8>
      flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
      return
    }
  }
}

// -----// IR Dump After EncodeHostTensors (iree-stream-encode-host-tensors) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c2} -> !stream.resource<*>{%c2}
  %2 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1[%c0 to %c2 for %c2]) : (!stream.resource<*>{%c2}) -> !stream.resource<*>{%c8}
  %3 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c8}
  %4 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c2}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c2} -> !stream.resource<external>{%c2}
  %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After EncodeDeviceTensors (iree-stream-encode-device-tensors) //----- //
stream.executable private @main_dispatch_1 {
  stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
    %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
    stream.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
      %c0 = arith.constant 0 : index
      %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
      %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
      %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
      %3 = tensor.empty() : tensor<2xf32>
      %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
      flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
      return
    }
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c2} -> !stream.resource<*>{%c2}
  %2 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1[%c0 to %c2 for %c2]) : (!stream.resource<*>{%c2}) -> !stream.resource<*>{%c8}
  %3 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c8}
  %4 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c2}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c2} -> !stream.resource<external>{%c2}
  %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c2} -> !stream.resource<*>{%c2}
  %2 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1[%c0 to %c2 for %c2]) : (!stream.resource<*>{%c2}) -> !stream.resource<*>{%c8}
  %3 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c8}
  %4 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c2}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c2} -> !stream.resource<external>{%c2}
  %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c2} -> !stream.resource<*>{%c2}
  %2 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1[%c0 to %c2 for %c2]) : (!stream.resource<*>{%c2}) -> !stream.resource<*>{%c8}
  %3 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c8}
  %4 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c2}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c2} -> !stream.resource<external>{%c2}
  %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c2} -> !stream.resource<*>{%c2}
    %2 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1[%c0 to %c2 for %c2]) : (!stream.resource<*>{%c2}) -> !stream.resource<*>{%c8}
    %3 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c8}
    %4 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c2}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c2} -> !stream.resource<external>{%c2}
    %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c2} -> !stream.resource<*>{%c2}
    %2 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1[%c0 to %c2 for %c2]) : (!stream.resource<*>{%c2}) -> !stream.resource<*>{%c8}
    %3 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c8}
    %4 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c2}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c2} -> !stream.resource<external>{%c2}
    %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c2} -> !stream.resource<*>{%c2}
    %2 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1[%c0 to %c2 for %c2]) : (!stream.resource<*>{%c2}) -> !stream.resource<*>{%c8}
    %3 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c8}
    %4 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c2}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c2} -> !stream.resource<external>{%c2}
    %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c2} -> !stream.resource<*>{%c2}
    %2 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1[%c0 to %c2 for %c2]) : (!stream.resource<*>{%c2}) -> !stream.resource<*>{%c8}
    %3 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c8}
    %4 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c2}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c2} -> !stream.resource<external>{%c2}
    %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeCopyOnWrite (iree-stream-materialize-copy-on-write) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c2} -> !stream.resource<*>{%c2}
  %2 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1[%c0 to %c2 for %c2]) : (!stream.resource<*>{%c2}) -> !stream.resource<*>{%c8}
  %3 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c8}
  %4 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c2}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c2} -> !stream.resource<external>{%c2}
  %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After ElideAsyncCopies (iree-stream-elide-async-copies) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.async.transfer %0 : !stream.resource<external>{%c2} -> !stream.resource<*>{%c2}
    %2 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1[%c0 to %c2 for %c2]) : (!stream.resource<*>{%c2}) -> !stream.resource<*>{%c8}
    %3 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c8}
    %4 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c2}
    %5 = stream.async.transfer %4 : !stream.resource<*>{%c2} -> !stream.resource<external>{%c2}
    %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c2} -> !stream.resource<*>{%c2}
  %2 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1[%c0 to %c2 for %c2]) : (!stream.resource<*>{%c2}) -> !stream.resource<*>{%c8}
  %3 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c8}
  %4 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c2}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c2} -> !stream.resource<external>{%c2}
  %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After EmplaceAllocations (iree-stream-emplace-allocations) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.async.transfer %0 : !stream.resource<external>{%c2} -> !stream.resource<*>{%c2}
  %2 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%1[%c0 to %c2 for %c2]) : (!stream.resource<*>{%c2}) -> !stream.resource<*>{%c8}
  %3 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%2[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c8}
  %4 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%3[%c0 to %c8 for %c8]) : (!stream.resource<*>{%c8}) -> !stream.resource<*>{%c2}
  %5 = stream.async.transfer %4 : !stream.resource<*>{%c2} -> !stream.resource<external>{%c2}
  %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After RefineUsage (iree-stream-refine-usage) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%0[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
    %2 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%1[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
    %3 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%2[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
    %4 = stream.tensor.export %3 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%0[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
  %2 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%1[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
  %3 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%2[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
  %4 = stream.tensor.export %3 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%0[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
  %2 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%1[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
  %3 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%2[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
  %4 = stream.tensor.export %3 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%0[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
  %2 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%1[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
  %3 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%2[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
  %4 = stream.tensor.export %3 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%0[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
    %2 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%1[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
    %3 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%2[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
    %4 = stream.tensor.export %3 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%0[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
    %2 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%1[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
    %3 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%2[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
    %4 = stream.tensor.export %3 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%0[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
    %2 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%1[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
    %3 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%2[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
    %4 = stream.tensor.export %3 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%0[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
    %2 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%1[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
    %3 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%2[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
    %4 = stream.tensor.export %3 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyAsyncAccessRanges (iree-stream-verify-async-access-ranges) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%0[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
    %2 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%1[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
    %3 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%2[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
    %4 = stream.tensor.export %3 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After ScheduleExecution (iree-stream-schedule-execution) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg1: !stream.resource<external>{%c2}) -> !stream.resource<external>{%c2} {
    %3 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%arg1[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
    %4 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%3[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
    %5 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%4[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
    stream.yield %5 : !stream.resource<external>{%c2}
  } => !stream.timepoint
  %1 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2}
  %2 = stream.tensor.export %1 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %2 : !hal.buffer_view
}

// -----// IR Dump After ScheduleConcurrency (iree-stream-schedule-concurrency) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg1: !stream.resource<external>{%c2}) -> !stream.resource<external>{%c2} {
    %3 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%arg1[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
    %4 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%3[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
    %5 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%4[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
    stream.yield %5 : !stream.resource<external>{%c2}
  } => !stream.timepoint
  %1 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2}
  %2 = stream.tensor.export %1 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %2 : !hal.buffer_view
}

// -----// IR Dump After PropagateTimepoints (iree-stream-propagate-timepoints) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.timepoint.immediate => !stream.timepoint
    %2 = stream.timepoint.immediate => !stream.timepoint
    %results, %result_timepoint = stream.async.execute await(%2) => with(%0 as %arg1: !stream.resource<external>{%c2}) -> !stream.resource<external>{%c2} {
      %5 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%arg1[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
      %6 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%5[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
      %7 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%6[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
      stream.yield %7 : !stream.resource<external>{%c2}
    } => !stream.timepoint
    %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2}
    %4 = stream.tensor.export %3 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeBuiltins (iree-stream-materialize-builtins) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.timepoint.immediate => !stream.timepoint
    %2 = stream.timepoint.immediate => !stream.timepoint
    %results, %result_timepoint = stream.async.execute await(%2) => with(%0 as %arg1: !stream.resource<external>{%c2}) -> !stream.resource<external>{%c2} {
      %5 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%arg1[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
      %6 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%5[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
      %7 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%6[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
      stream.yield %7 : !stream.resource<external>{%c2}
    } => !stream.timepoint
    %3 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2}
    %4 = stream.tensor.export %3 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg1: !stream.resource<external>{%c2}) -> !stream.resource<external>{%c2} {
    %3 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%arg1[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
    %4 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%3[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
    %5 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%4[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
    stream.yield %5 : !stream.resource<external>{%c2}
  } => !stream.timepoint
  %1 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2}
  %2 = stream.tensor.export %1 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %2 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg1: !stream.resource<external>{%c2}) -> !stream.resource<external>{%c2} {
    %3 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%arg1[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
    %4 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%3[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
    %5 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%4[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
    stream.yield %5 : !stream.resource<external>{%c2}
  } => !stream.timepoint
  %1 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2}
  %2 = stream.tensor.export %1 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %2 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %results, %result_timepoint = stream.async.execute with(%0 as %arg1: !stream.resource<external>{%c2}) -> !stream.resource<external>{%c2} {
    %3 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%arg1[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
    %4 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%3[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
    %5 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%4[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
    stream.yield %5 : !stream.resource<external>{%c2}
  } => !stream.timepoint
  %1 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2}
  %2 = stream.tensor.export %1 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %2 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg1: !stream.resource<external>{%c2}) -> !stream.resource<external>{%c2} {
      %3 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%arg1[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
      %4 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%3[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
      %5 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%4[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
      stream.yield %5 : !stream.resource<external>{%c2}
    } => !stream.timepoint
    %1 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2}
    %2 = stream.tensor.export %1 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %2 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg1: !stream.resource<external>{%c2}) -> !stream.resource<external>{%c2} {
      %3 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%arg1[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
      %4 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%3[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
      %5 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%4[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
      stream.yield %5 : !stream.resource<external>{%c2}
    } => !stream.timepoint
    %1 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2}
    %2 = stream.tensor.export %1 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %2 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg1: !stream.resource<external>{%c2}) -> !stream.resource<external>{%c2} {
      %3 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%arg1[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
      %4 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%3[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
      %5 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%4[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
      stream.yield %5 : !stream.resource<external>{%c2}
    } => !stream.timepoint
    %1 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2}
    %2 = stream.tensor.export %1 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %2 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg1: !stream.resource<external>{%c2}) -> !stream.resource<external>{%c2} {
      %3 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%arg1[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
      %4 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%3[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
      %5 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%4[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
      stream.yield %5 : !stream.resource<external>{%c2}
    } => !stream.timepoint
    %1 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2}
    %2 = stream.tensor.export %1 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %2 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToAsync (iree-stream-verify-lowering-to-async) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %results, %result_timepoint = stream.async.execute with(%0 as %arg1: !stream.resource<external>{%c2}) -> !stream.resource<external>{%c2} {
      %3 = stream.async.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%arg1[%c0 to %c2 for %c2]) : (!stream.resource<external>{%c2}) -> !stream.resource<transient>{%c8}
      %4 = stream.async.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%3[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<transient>{%c8}
      %5 = stream.async.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%4[%c0 to %c8 for %c8]) : (!stream.resource<transient>{%c8}) -> !stream.resource<external>{%c2}
      stream.yield %5 : !stream.resource<external>{%c2}
    } => !stream.timepoint
    %1 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c2}
    %2 = stream.tensor.export %1 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %2 : !hal.buffer_view
  }
}


// -----// IR Dump After ScheduleAllocation (iree-stream-schedule-allocation) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %c0_0 = arith.constant 0 : index
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %2:3 = stream.resource.pack slices({
      [0, 1] = %c8,
      [1, 2] = %c8
    }) : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%2#0} => !stream.timepoint
    %3 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%2#0}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%2#1 for %c8] : !stream.resource<transient>{%2#0}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%2#1 for %c8] : !stream.resource<transient>{%2#0},
        wo %arg3[%2#2 for %c8] : !stream.resource<transient>{%2#0}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%2#2 for %c8] : !stream.resource<transient>{%2#0},
        wo %arg2[%c0_0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %4 = stream.resource.dealloca await(%3) => %result : !stream.resource<transient>{%2#0} => !stream.timepoint
    %5 = stream.timepoint.join max(%4, %3) => !stream.timepoint
    %6 = stream.timepoint.await %5 => %1 : !stream.resource<external>{%c2}
    %7 = stream.tensor.export %6 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %7 : !hal.buffer_view
  }
}


// -----// IR Dump After PackConstants (iree-stream-pack-constants) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %c0_0 = arith.constant 0 : index
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %2:3 = stream.resource.pack slices({
    [0, 1] = %c8,
    [1, 2] = %c8
  }) : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%2#0} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%2#0}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%2#1 for %c8] : !stream.resource<transient>{%2#0}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%2#1 for %c8] : !stream.resource<transient>{%2#0},
      wo %arg3[%2#2 for %c8] : !stream.resource<transient>{%2#0}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%2#2 for %c8] : !stream.resource<transient>{%2#0},
      wo %arg2[%c0_0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %4 = stream.resource.dealloca await(%3) => %result : !stream.resource<transient>{%2#0} => !stream.timepoint
  %5 = stream.timepoint.join max(%4, %3) => !stream.timepoint
  %6 = stream.timepoint.await %5 => %1 : !stream.resource<external>{%c2}
  %7 = stream.tensor.export %6 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %7 : !hal.buffer_view
}

// -----// IR Dump After PackAllocations (iree-stream-pack-allocations) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %c0_0 = arith.constant 0 : index
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %2:3 = stream.resource.pack slices({
    [0, 1] = %c8,
    [1, 2] = %c8
  }) : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%2#0} => !stream.timepoint
  %3 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%2#0}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%2#1 for %c8] : !stream.resource<transient>{%2#0}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%2#1 for %c8] : !stream.resource<transient>{%2#0},
      wo %arg3[%2#2 for %c8] : !stream.resource<transient>{%2#0}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%2#2 for %c8] : !stream.resource<transient>{%2#0},
      wo %arg2[%c0_0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %4 = stream.resource.dealloca await(%3) => %result : !stream.resource<transient>{%2#0} => !stream.timepoint
  %5 = stream.timepoint.join max(%4, %3) => !stream.timepoint
  %6 = stream.timepoint.await %5 => %1 : !stream.resource<external>{%c2}
  %7 = stream.tensor.export %6 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %7 : !hal.buffer_view
}

// -----// IR Dump After LayoutSlices (iree-stream-layout-slices) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %c0_0 = arith.constant 0 : index
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %c0_1 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %c64_2 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c128_3 = arith.constant 128 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128_3} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128_3}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0_1 for %c8] : !stream.resource<transient>{%c128_3}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%c0_1 for %c8] : !stream.resource<transient>{%c128_3},
      wo %arg3[%c64_2 for %c8] : !stream.resource<transient>{%c128_3}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%c64_2 for %c8] : !stream.resource<transient>{%c128_3},
      wo %arg2[%c0_0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128_3} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
  %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After PropagateSubranges (iree-util-propagate-subranges) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %c0_0 = arith.constant 0 : index
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %c0_1 = arith.constant 0 : index
    %c64 = arith.constant 64 : index
    %c64_2 = arith.constant 64 : index
    %c128 = arith.constant 128 : index
    %c128_3 = arith.constant 128 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128_3} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128_3}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0_1 for %c8] : !stream.resource<transient>{%c128_3}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0_1 for %c8] : !stream.resource<transient>{%c128_3},
        wo %arg3[%c64_2 for %c8] : !stream.resource<transient>{%c128_3}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64_2 for %c8] : !stream.resource<transient>{%c128_3},
        wo %arg2[%c0_0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128_3} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
    %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
      wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
  %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
      wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
  %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
      wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
  %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
    %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
    %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
    %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
    %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToCmd (iree-stream-verify-lowering-to-cmd) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
    %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
      wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
  %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
      wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
  %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
      wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
  %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
    %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
    %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
    %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
    %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
      wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
  %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
      wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
  %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
      wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
  %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
      wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
  %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %6 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 0 : index} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
    %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 0 : index} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
    %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 0 : index} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
    %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 0 : index} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %1 : !stream.resource<external>{%c2}
    %6 = stream.tensor.export %5 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After ElideTimepoints (iree-stream-elide-timepoints) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 0 : index, iree.fixedpoint.modified} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.immediate => !stream.timepoint
    %5 = stream.timepoint.join max(%3, %4) => !stream.timepoint
    %6 = stream.timepoint.await %5 => %1 : !stream.resource<external>{%c2}
    %7 = stream.tensor.export %6 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %7 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
      wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
  %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
      wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
  %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c128 = arith.constant 128 : index
  %c64 = arith.constant 64 : index
  %c2 = arith.constant 2 : index
  %c8 = arith.constant 8 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
      wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
  %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 1 : index} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 1 : index} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 1 : index} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 1 : index} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After ElideTimepoints (iree-stream-elide-timepoints) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 1 : index} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c8] : !stream.resource<transient>{%c128},
        wo %arg3[%c64 for %c8] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c64 for %c8] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseDispatchBindings (iree-stream-fuse-dispatch-bindings) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: index, %arg3: index) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg2] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%arg3] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: index, %arg3: index) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg2] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%arg3] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: index, %arg3: index) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg2] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%arg3] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %c0_0 = arith.constant 0 : index
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%c0, %c0 : index, index) {
        ro %arg1[%c0_0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0_0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%c0, %c64 : index, index) {
        ro %arg3[%c0_0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0_0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%c64, %c0 : index, index) {
        ro %arg3[%c0_0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0_0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After AnnotateDispatchArguments (iree-stream-annotate-dispatch-arguments) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: index {stream.values = [0 : index]}, %arg3: index {stream.values = [0 : index]}) {
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg2] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%arg3] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst_0 : f32
          %7 = arith.mulf %6, %cst : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: index {stream.values = [0 : index]}, %arg3: index {stream.alignment = 64 : index, stream.values = [64 : index]}) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg2] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%arg3] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: index {stream.alignment = 64 : index, stream.values = [64 : index]}, %arg3: index {stream.values = [0 : index]}) {
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg2] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%arg3] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst_1 : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %c0_0 = arith.constant 0 : index
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%c0, %c0 : index, index) {
        ro %arg1[%c0_0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0_0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%c0, %c64 : index, index) {
        ro %arg3[%c0_0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0_0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%c64, %c0 : index, index) {
        ro %arg3[%c0_0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0_0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After PackDispatchOperands (iree-stream-pack-dispatch-operands) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: i32, %arg3: i32) {
        %0 = arith.index_castui %arg2 {stream.values = [0 : index]} : i32 to index
        %1 = arith.index_castui %arg3 {stream.values = [0 : index]} : i32 to index
        %cst = arith.constant 0.0125187514 : f32
        %cst_0 = arith.constant -1.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %2 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %3 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %4 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %5 = tensor.empty() : tensor<2xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%4 : tensor<2xi8>) outs(%5 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %7 = arith.sitofp %in : i8 to f32
          %8 = arith.subf %7, %cst_0 : f32
          %9 = arith.mulf %8, %cst : f32
          linalg.yield %9 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %6, %3, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: i32, %arg3: i32) {
        %0 = arith.index_castui %arg2 {stream.values = [0 : index]} : i32 to index
        %1 = arith.index_castui %arg3 {stream.alignment = 64 : index, stream.values = [64 : index]} : i32 to index
        %c0 = arith.constant 0 : index
        %2 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %3 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %4 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %5 = tensor.empty() : tensor<2xf32>
        %6 = iree_linalg_ext.softmax dimension(0) ins(%4 : tensor<2xf32>) outs(%5 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %6, %3, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: i32, %arg3: i32) {
        %0 = arith.index_castui %arg2 {stream.alignment = 64 : index, stream.values = [64 : index]} : i32 to index
        %1 = arith.index_castui %arg3 {stream.values = [0 : index]} : i32 to index
        %cst = arith.constant 1.270000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 2.560000e+02 : f32
        %c0 = arith.constant 0 : index
        %2 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %3 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %4 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %5 = tensor.empty() : tensor<2xi8>
        %6 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%4 : tensor<2xf32>) outs(%5 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %7 = arith.mulf %in, %cst_1 : f32
          %8 = arith.addf %7, %cst_0 : f32
          %9 = math.roundeven %8 : f32
          %10 = arith.minf %9, %cst : f32
          %11 = arith.maxf %10, %cst_0 : f32
          %12 = arith.fptosi %11 : f32 to i8
          linalg.yield %12 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %6, %3, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c64 = arith.constant 64 : index
    %c2 = arith.constant 2 : index
    %c8 = arith.constant 8 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %c0_0 = arith.constant 0 : index
    %c0_i32 = arith.constant 0 : i32
    %c0_i32_1 = arith.constant 0 : i32
    %c0_i32_2 = arith.constant 0 : i32
    %c64_i32 = arith.constant 64 : i32
    %c64_i32_3 = arith.constant 64 : i32
    %c0_i32_4 = arith.constant 0 : i32
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%c0_i32, %c0_i32_1 : i32, i32) {
        ro %arg1[%c0_0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0_0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%c0_i32_2, %c64_i32 : i32, i32) {
        ro %arg3[%c0_0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0_0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%c64_i32_3, %c0_i32_4 : i32, i32) {
        ro %arg3[%c0_0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0_0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c64_i32 = arith.constant 64 : i32
  %c0_i32 = arith.constant 0 : i32
  %c128 = arith.constant 128 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%c0_i32, %c0_i32 : i32, i32) {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%c0_i32, %c64_i32 : i32, i32) {
      ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
      wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%c64_i32, %c0_i32 : i32, i32) {
      ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
  %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c64_i32 = arith.constant 64 : i32
  %c0_i32 = arith.constant 0 : i32
  %c128 = arith.constant 128 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%c0_i32, %c0_i32 : i32, i32) {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%c0_i32, %c64_i32 : i32, i32) {
      ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
      wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%c64_i32, %c0_i32 : i32, i32) {
      ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
  %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c64_i32 = arith.constant 64 : i32
  %c0_i32 = arith.constant 0 : i32
  %c128 = arith.constant 128 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%c0_i32, %c0_i32 : i32, i32) {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%c0_i32, %c64_i32 : i32, i32) {
      ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
      wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%c64_i32, %c0_i32 : i32, i32) {
      ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
  %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: i32, %arg3: i32) {
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = arith.index_castui %arg2 {stream.values = [0 : index]} : i32 to index
        %1 = arith.index_castui %arg3 {stream.values = [0 : index]} : i32 to index
        %2 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %3 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %4 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %5 = tensor.empty() : tensor<2xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%4 : tensor<2xi8>) outs(%5 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %7 = arith.sitofp %in : i8 to f32
          %8 = arith.subf %7, %cst : f32
          %9 = arith.mulf %8, %cst_0 : f32
          linalg.yield %9 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %6, %3, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: i32, %arg3: i32) {
        %0 = arith.index_castui %arg2 {stream.values = [0 : index]} : i32 to index
        %1 = arith.index_castui %arg3 {stream.alignment = 64 : index, stream.values = [64 : index]} : i32 to index
        %2 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %3 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %4 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %5 = tensor.empty() : tensor<2xf32>
        %6 = iree_linalg_ext.softmax dimension(0) ins(%4 : tensor<2xf32>) outs(%5 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %6, %3, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: i32, %arg3: i32) {
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = arith.index_castui %arg2 {stream.alignment = 64 : index, stream.values = [64 : index]} : i32 to index
        %1 = arith.index_castui %arg3 {stream.values = [0 : index]} : i32 to index
        %2 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %3 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %4 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %5 = tensor.empty() : tensor<2xi8>
        %6 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%4 : tensor<2xf32>) outs(%5 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %7 = arith.mulf %in, %cst : f32
          %8 = arith.addf %7, %cst_0 : f32
          %9 = math.roundeven %8 : f32
          %10 = arith.minf %9, %cst_1 : f32
          %11 = arith.maxf %10, %cst_0 : f32
          %12 = arith.fptosi %11 : f32 to i8
          linalg.yield %12 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %6, %3, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c64_i32 = arith.constant 64 : i32
    %c0_i32 = arith.constant 0 : i32
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%c0_i32, %c0_i32 : i32, i32) {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%c0_i32, %c64_i32 : i32, i32) {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%c64_i32, %c0_i32 : i32, i32) {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: i32, %arg3: i32) {
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = arith.index_castui %arg2 {stream.values = [0 : index]} : i32 to index
        %1 = arith.index_castui %arg3 {stream.values = [0 : index]} : i32 to index
        %2 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %3 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %4 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %5 = tensor.empty() : tensor<2xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%4 : tensor<2xi8>) outs(%5 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %7 = arith.sitofp %in : i8 to f32
          %8 = arith.subf %7, %cst : f32
          %9 = arith.mulf %8, %cst_0 : f32
          linalg.yield %9 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %6, %3, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: i32, %arg3: i32) {
        %0 = arith.index_castui %arg2 {stream.values = [0 : index]} : i32 to index
        %1 = arith.index_castui %arg3 {stream.alignment = 64 : index, stream.values = [64 : index]} : i32 to index
        %2 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %3 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %4 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %5 = tensor.empty() : tensor<2xf32>
        %6 = iree_linalg_ext.softmax dimension(0) ins(%4 : tensor<2xf32>) outs(%5 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %6, %3, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: i32, %arg3: i32) {
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = arith.index_castui %arg2 {stream.alignment = 64 : index, stream.values = [64 : index]} : i32 to index
        %1 = arith.index_castui %arg3 {stream.values = [0 : index]} : i32 to index
        %2 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %3 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %4 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %5 = tensor.empty() : tensor<2xi8>
        %6 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%4 : tensor<2xf32>) outs(%5 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %7 = arith.mulf %in, %cst : f32
          %8 = arith.addf %7, %cst_0 : f32
          %9 = math.roundeven %8 : f32
          %10 = arith.minf %9, %cst_1 : f32
          %11 = arith.maxf %10, %cst_0 : f32
          %12 = arith.fptosi %11 : f32 to i8
          linalg.yield %12 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %6, %3, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c64_i32 = arith.constant 64 : i32
    %c0_i32 = arith.constant 0 : i32
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%c0_i32, %c0_i32 : i32, i32) {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%c0_i32, %c64_i32 : i32, i32) {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%c64_i32, %c0_i32 : i32, i32) {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: i32, %arg3: i32) {
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = arith.index_castui %arg2 {stream.values = [0 : index]} : i32 to index
        %1 = arith.index_castui %arg3 {stream.values = [0 : index]} : i32 to index
        %2 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %3 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %4 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %5 = tensor.empty() : tensor<2xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%4 : tensor<2xi8>) outs(%5 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %7 = arith.sitofp %in : i8 to f32
          %8 = arith.subf %7, %cst : f32
          %9 = arith.mulf %8, %cst_0 : f32
          linalg.yield %9 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %6, %3, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: i32, %arg3: i32) {
        %0 = arith.index_castui %arg2 {stream.values = [0 : index]} : i32 to index
        %1 = arith.index_castui %arg3 {stream.alignment = 64 : index, stream.values = [64 : index]} : i32 to index
        %2 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %3 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %4 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %5 = tensor.empty() : tensor<2xf32>
        %6 = iree_linalg_ext.softmax dimension(0) ins(%4 : tensor<2xf32>) outs(%5 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %6, %3, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: i32, %arg3: i32) {
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = arith.index_castui %arg2 {stream.alignment = 64 : index, stream.values = [64 : index]} : i32 to index
        %1 = arith.index_castui %arg3 {stream.values = [0 : index]} : i32 to index
        %2 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %3 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %4 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %5 = tensor.empty() : tensor<2xi8>
        %6 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%4 : tensor<2xf32>) outs(%5 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %7 = arith.mulf %in, %cst : f32
          %8 = arith.addf %7, %cst_0 : f32
          %9 = math.roundeven %8 : f32
          %10 = arith.minf %9, %cst_1 : f32
          %11 = arith.maxf %10, %cst_0 : f32
          %12 = arith.fptosi %11 : f32 to i8
          linalg.yield %12 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %6, %3, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c64_i32 = arith.constant 64 : i32
    %c0_i32 = arith.constant 0 : i32
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%c0_i32, %c0_i32 : i32, i32) {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%c0_i32, %c64_i32 : i32, i32) {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%c64_i32, %c0_i32 : i32, i32) {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: i32, %arg3: i32) {
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = arith.index_castui %arg2 {stream.values = [0 : index]} : i32 to index
        %1 = arith.index_castui %arg3 {stream.values = [0 : index]} : i32 to index
        %2 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %3 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %4 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %5 = tensor.empty() : tensor<2xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%4 : tensor<2xi8>) outs(%5 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %7 = arith.sitofp %in : i8 to f32
          %8 = arith.subf %7, %cst : f32
          %9 = arith.mulf %8, %cst_0 : f32
          linalg.yield %9 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %6, %3, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: i32, %arg3: i32) {
        %0 = arith.index_castui %arg2 {stream.values = [0 : index]} : i32 to index
        %1 = arith.index_castui %arg3 {stream.alignment = 64 : index, stream.values = [64 : index]} : i32 to index
        %2 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %3 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %4 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %5 = tensor.empty() : tensor<2xf32>
        %6 = iree_linalg_ext.softmax dimension(0) ins(%4 : tensor<2xf32>) outs(%5 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %6, %3, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: i32, %arg3: i32) {
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = arith.index_castui %arg2 {stream.alignment = 64 : index, stream.values = [64 : index]} : i32 to index
        %1 = arith.index_castui %arg3 {stream.values = [0 : index]} : i32 to index
        %2 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %3 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %4 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %5 = tensor.empty() : tensor<2xi8>
        %6 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%4 : tensor<2xf32>) outs(%5 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %7 = arith.mulf %in, %cst : f32
          %8 = arith.addf %7, %cst_0 : f32
          %9 = math.roundeven %8 : f32
          %10 = arith.minf %9, %cst_1 : f32
          %11 = arith.maxf %10, %cst_0 : f32
          %12 = arith.fptosi %11 : f32 to i8
          linalg.yield %12 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %6, %3, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c64_i32 = arith.constant 64 : i32
    %c0_i32 = arith.constant 0 : i32
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32(%c0_i32, %c0_i32 : i32, i32) {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32(%c0_i32, %c64_i32 : i32, i32) {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8(%c64_i32, %c0_i32 : i32, i32) {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldUniformOperands (iree-stream-fold-uniform-operands) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = arith.index_castui %c0_i32 {stream.values = [0 : index]} : i32 to index
        %1 = arith.index_castui %c0_i32 {stream.values = [0 : index]} : i32 to index
        %2 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %3 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %4 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %5 = tensor.empty() : tensor<2xf32>
        %6 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%4 : tensor<2xi8>) outs(%5 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %7 = arith.sitofp %in : i8 to f32
          %8 = arith.subf %7, %cst : f32
          %9 = arith.mulf %8, %cst_0 : f32
          linalg.yield %9 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %6, %3, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %c64_i32 = arith.constant 64 : i32
        %0 = arith.index_castui %c0_i32 {stream.values = [0 : index]} : i32 to index
        %1 = arith.index_castui %c64_i32 {stream.alignment = 64 : index, stream.values = [64 : index]} : i32 to index
        %2 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %3 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %4 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %5 = tensor.empty() : tensor<2xf32>
        %6 = iree_linalg_ext.softmax dimension(0) ins(%4 : tensor<2xf32>) outs(%5 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %6, %3, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c64_i32 = arith.constant 64 : i32
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = arith.index_castui %c64_i32 {stream.alignment = 64 : index, stream.values = [64 : index]} : i32 to index
        %1 = arith.index_castui %c0_i32 {stream.values = [0 : index]} : i32 to index
        %2 = stream.binding.subspan %arg0[%0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %3 = stream.binding.subspan %arg1[%1] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %4 = flow.dispatch.tensor.load %2, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %5 = tensor.empty() : tensor<2xi8>
        %6 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%4 : tensor<2xf32>) outs(%5 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %7 = arith.mulf %in, %cst : f32
          %8 = arith.addf %7, %cst_0 : f32
          %9 = math.roundeven %8 : f32
          %10 = arith.minf %9, %cst_1 : f32
          %11 = arith.maxf %10, %cst_0 : f32
          %12 = arith.fptosi %11 : f32 to i8
          linalg.yield %12 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %6, %3, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c64_i32 = arith.constant 64 : i32
    %c0_i32 = arith.constant 0 : i32
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c128 = arith.constant 128 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
      wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
  %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c128 = arith.constant 128 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
      wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
  %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c128 = arith.constant 128 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
      wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
  %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst : f32
          %7 = arith.mulf %6, %cst_0 : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %c64 = arith.constant 64 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c64] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c64 = arith.constant 64 : index
        %c0 = arith.constant 0 : index
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = stream.binding.subspan %arg0[%c64] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst_1 : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst : f32
          %7 = arith.mulf %6, %cst_0 : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %c64 = arith.constant 64 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c64] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c64 = arith.constant 64 : index
        %c0 = arith.constant 0 : index
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = stream.binding.subspan %arg0[%c64] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst_1 : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst : f32
          %7 = arith.mulf %6, %cst_0 : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %c64 = arith.constant 64 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c64] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c64 = arith.constant 64 : index
        %c0 = arith.constant 0 : index
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = stream.binding.subspan %arg0[%c64] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst_1 : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst : f32
          %7 = arith.mulf %6, %cst_0 : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %c64 = arith.constant 64 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c64] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c64 = arith.constant 64 : index
        %c0 = arith.constant 0 : index
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = stream.binding.subspan %arg0[%c64] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst_1 : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst : f32
          %7 = arith.mulf %6, %cst_0 : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %c64 = arith.constant 64 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c64] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c64 = arith.constant 64 : index
        %c0 = arith.constant 0 : index
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = stream.binding.subspan %arg0[%c64] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst_1 : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst : f32
          %7 = arith.mulf %6, %cst_0 : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %c64 = arith.constant 64 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c64] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c64 = arith.constant 64 : index
        %c0 = arith.constant 0 : index
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = stream.binding.subspan %arg0[%c64] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst_1 : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst : f32
          %7 = arith.mulf %6, %cst_0 : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %c64 = arith.constant 64 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c64] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c64 = arith.constant 64 : index
        %c0 = arith.constant 0 : index
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = stream.binding.subspan %arg0[%c64] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst_1 : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst : f32
          %7 = arith.mulf %6, %cst_0 : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %c64 = arith.constant 64 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c64] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c64 = arith.constant 64 : index
        %c0 = arith.constant 0 : index
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = stream.binding.subspan %arg0[%c64] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst_1 : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c128 = arith.constant 128 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
  %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
  %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
  %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
    stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
      ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
      wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
      ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
      wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
    }
    stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
      ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
      wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
  %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
  return %5 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst : f32
          %7 = arith.mulf %6, %cst_0 : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %c64 = arith.constant 64 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c64] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c64 = arith.constant 64 : index
        %c0 = arith.constant 0 : index
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = stream.binding.subspan %arg0[%c64] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst_1 : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst : f32
          %7 = arith.mulf %6, %cst_0 : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %c64 = arith.constant 64 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c64] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c64 = arith.constant 64 : index
        %c0 = arith.constant 0 : index
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = stream.binding.subspan %arg0[%c64] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst_1 : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst : f32
          %7 = arith.mulf %6, %cst_0 : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %c64 = arith.constant 64 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c64] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c64 = arith.constant 64 : index
        %c0 = arith.constant 0 : index
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = stream.binding.subspan %arg0[%c64] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst_1 : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::AssignTargetDevicesPass (iree-hal-assign-target-devices) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst : f32
          %7 = arith.mulf %6, %cst_0 : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %c64 = arith.constant 64 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c64] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c64 = arith.constant 64 : index
        %c0 = arith.constant 0 : index
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = stream.binding.subspan %arg0[%c64] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst_1 : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::VerifyTargetEnvironmentPass (iree-hal-verify-target-environment) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  stream.executable private @main_dispatch_0 {
    stream.executable.export public @main_dispatch_0_generic_2_i8xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_0_generic_2_i8xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant -1.000000e+00 : f32
        %cst_0 = arith.constant 0.0125187514 : f32
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xi8>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
        ^bb0(%in: i8, %out: f32):
          %5 = arith.sitofp %in : i8 to f32
          %6 = arith.subf %5, %cst : f32
          %7 = arith.mulf %6, %cst_0 : f32
          linalg.yield %7 : f32
        } -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_1 {
    stream.executable.export public @main_dispatch_1_softmax_2xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_1_softmax_2xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c0 = arith.constant 0 : index
        %c64 = arith.constant 64 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c64] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xf32>
        %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
        return
      }
    }
  }
  stream.executable private @main_dispatch_2 {
    stream.executable.export public @main_dispatch_2_generic_2_f32xi8 workgroups() -> (index, index, index) {
      %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @main_dispatch_2_generic_2_f32xi8(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}) {
        %c64 = arith.constant 64 : index
        %c0 = arith.constant 0 : index
        %cst = arith.constant 2.560000e+02 : f32
        %cst_0 = arith.constant -1.280000e+02 : f32
        %cst_1 = arith.constant 1.270000e+02 : f32
        %0 = stream.binding.subspan %arg0[%c64] : !stream.binding -> !flow.dispatch.tensor<readonly:tensor<2xf32>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
        %3 = tensor.empty() : tensor<2xi8>
        %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
        ^bb0(%in: f32, %out: i8):
          %5 = arith.mulf %in, %cst : f32
          %6 = arith.addf %5, %cst_0 : f32
          %7 = math.roundeven %6 : f32
          %8 = arith.minf %7, %cst_1 : f32
          %9 = arith.maxf %8, %cst_0 : f32
          %10 = arith.fptosi %9 : f32 to i8
          linalg.yield %10 : i8
        } -> tensor<2xi8>
        flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
        return
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_1::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      }
      stream.cmd.dispatch @main_dispatch_2::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::{anonymous}::MaterializeInterfacesPass (iree-hal-materialize-interfaces) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#map = affine_map<(d0) -> (d0)>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  hal.executable private @main_dispatch_0 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @main_dispatch_0_generic_2_i8xf32() {
          %c0 = arith.constant 0 : index
          %cst = arith.constant -1.000000e+00 : f32
          %cst_0 = arith.constant 0.0125187514 : f32
          %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
          %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
          %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
          %3 = tensor.empty() : tensor<2xf32>
          %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
          ^bb0(%in: i8, %out: f32):
            %5 = arith.sitofp %in : i8 to f32
            %6 = arith.subf %5, %cst : f32
            %7 = arith.mulf %6, %cst_0 : f32
            linalg.yield %7 : f32
          } -> tensor<2xf32>
          flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
          return
        }
      }
    }
  }
  hal.executable private @main_dispatch_1 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @main_dispatch_1_softmax_2xf32() {
          %c0 = arith.constant 0 : index
          %c64 = arith.constant 64 : index
          %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
          %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %3 = tensor.empty() : tensor<2xf32>
          %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
          flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
          return
        }
      }
    }
  }
  hal.executable private @main_dispatch_2 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(0) layout(#pipeline_layout) {
      ^bb0(%arg0: !hal.device):
        %x, %y, %z = flow.dispatch.workgroup_count_from_slice 
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @main_dispatch_2_generic_2_f32xi8() {
          %c64 = arith.constant 64 : index
          %c0 = arith.constant 0 : index
          %cst = arith.constant 2.560000e+02 : f32
          %cst_0 = arith.constant -1.280000e+02 : f32
          %cst_1 = arith.constant 1.270000e+02 : f32
          %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
          %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
          %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
          %3 = tensor.empty() : tensor<2xi8>
          %4 = linalg.generic {indexing_maps = [#map, #map], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
          ^bb0(%in: f32, %out: i8):
            %5 = arith.mulf %in, %cst : f32
            %6 = arith.addf %5, %cst_0 : f32
            %7 = math.roundeven %6 : f32
            %8 = arith.minf %7, %cst_1 : f32
            %9 = arith.maxf %8, %cst_0 : f32
            %10 = arith.fptosi %9 : f32 to i8
            linalg.yield %10 : i8
          } -> tensor<2xi8>
          flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
          return
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %0 = stream.tensor.import %arg0 : !hal.buffer_view -> tensor<1x2xi8> in !stream.resource<external>{%c2}
    %1 = stream.resource.alloc uninitialized : !stream.resource<external>{%c2}
    %result, %result_timepoint = stream.resource.alloca uninitialized : !stream.resource<transient>{%c128} => !stream.timepoint
    %2 = stream.cmd.execute await(%result_timepoint) => with(%0 as %arg1: !stream.resource<external>{%c2}, %1 as %arg2: !stream.resource<external>{%c2}, %result as %arg3: !stream.resource<transient>{%c128}) {
      stream.cmd.dispatch @main_dispatch_0::@static::@main_dispatch_0_generic_2_i8xf32 {
        ro %arg1[%c0 for %c2] : !stream.resource<external>{%c2},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      } attributes {hal.interface.bindings = [#hal.interface.binding<0, 0>, #hal.interface.binding<0, 1>]}
      stream.cmd.dispatch @main_dispatch_1::@static::@main_dispatch_1_softmax_2xf32 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg3[%c0 for %c128] : !stream.resource<transient>{%c128}
      } attributes {hal.interface.bindings = [#hal.interface.binding<0, 0>, #hal.interface.binding<0, 1>]}
      stream.cmd.dispatch @main_dispatch_2::@static::@main_dispatch_2_generic_2_f32xi8 {
        ro %arg3[%c0 for %c128] : !stream.resource<transient>{%c128},
        wo %arg2[%c0 for %c2] : !stream.resource<external>{%c2}
      } attributes {hal.interface.bindings = [#hal.interface.binding<0, 0>, #hal.interface.binding<0, 1>]}
    } => !stream.timepoint
    %3 = stream.resource.dealloca await(%2) => %result : !stream.resource<transient>{%c128} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %1 : !stream.resource<external>{%c2}
    %5 = stream.tensor.export %4 : tensor<1x2xi8> in !stream.resource<external>{%c2} -> !hal.buffer_view
    return %5 : !hal.buffer_view
  }
}


// -----// IR Dump After TypePropagation (iree-codegen-type-propagation) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
  %3 = tensor.empty() : tensor<2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %5 = arith.sitofp %in : i8 to f32
    %6 = arith.subf %5, %cst : f32
    %7 = arith.mulf %6, %cst_0 : f32
    linalg.yield %7 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After BubbleUpOrdinalOps (iree-codegen-bubble-up-ordinal-ops) //----- //
module {
  func.func @main_dispatch_0_generic_2_i8xf32() {
    %c0 = arith.constant 0 : index
    %cst = arith.constant -1.000000e+00 : f32
    %cst_0 = arith.constant 0.0125187514 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %3 = tensor.empty() : tensor<2xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %5 = arith.sitofp %in : i8 to f32
      %6 = arith.subf %5, %cst : f32
      %7 = arith.mulf %6, %cst_0 : f32
      linalg.yield %7 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    return
  }
}

// -----// IR Dump After BufferizeCopyOnlyDispatches (iree-codegen-bufferize-copy-only-dispatches) //----- //
module {
  func.func @main_dispatch_0_generic_2_i8xf32() {
    %c0 = arith.constant 0 : index
    %cst = arith.constant -1.000000e+00 : f32
    %cst_0 = arith.constant 0.0125187514 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %3 = tensor.empty() : tensor<2xf32>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
    ^bb0(%in: i8, %out: f32):
      %5 = arith.sitofp %in : i8 to f32
      %6 = arith.subf %5, %cst : f32
      %7 = arith.mulf %6, %cst_0 : f32
      linalg.yield %7 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    return
  }
}

// -----// IR Dump After TypePropagation (iree-codegen-type-propagation) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = tensor.empty() : tensor<2xf32>
  %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
  flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After TypePropagation (iree-codegen-type-propagation) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = tensor.empty() : tensor<2xi8>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %5 = arith.mulf %in, %cst : f32
    %6 = arith.addf %5, %cst_0 : f32
    %7 = math.roundeven %6 : f32
    %8 = arith.minf %7, %cst_1 : f32
    %9 = arith.maxf %8, %cst_0 : f32
    %10 = arith.fptosi %9 : f32 to i8
    linalg.yield %10 : i8
  } -> tensor<2xi8>
  flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  return
}

// -----// IR Dump After BubbleUpOrdinalOps (iree-codegen-bubble-up-ordinal-ops) //----- //
module {
  func.func @main_dispatch_1_softmax_2xf32() {
    %c0 = arith.constant 0 : index
    %c64 = arith.constant 64 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %3 = tensor.empty() : tensor<2xf32>
    %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
    flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    return
  }
}

// -----// IR Dump After DecomposeSoftmax (iree-linalg-ext-decompose-softmax) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
  %3 = tensor.empty() : tensor<2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %5 = arith.sitofp %in : i8 to f32
    %6 = arith.subf %5, %cst : f32
    %7 = arith.mulf %6, %cst_0 : f32
    linalg.yield %7 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After BubbleUpOrdinalOps (iree-codegen-bubble-up-ordinal-ops) //----- //
module {
  func.func @main_dispatch_2_generic_2_f32xi8() {
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 2.560000e+02 : f32
    %cst_0 = arith.constant -1.280000e+02 : f32
    %cst_1 = arith.constant 1.270000e+02 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
    %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %3 = tensor.empty() : tensor<2xi8>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %5 = arith.mulf %in, %cst : f32
      %6 = arith.addf %5, %cst_0 : f32
      %7 = math.roundeven %6 : f32
      %8 = arith.minf %7, %cst_1 : f32
      %9 = arith.maxf %8, %cst_0 : f32
      %10 = arith.fptosi %9 : f32 to i8
      linalg.yield %10 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
    return
  }
}

// -----// IR Dump After BufferizeCopyOnlyDispatches (iree-codegen-bufferize-copy-only-dispatches) //----- //
module {
  func.func @main_dispatch_1_softmax_2xf32() {
    %c0 = arith.constant 0 : index
    %c64 = arith.constant 64 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %3 = tensor.empty() : tensor<2xf32>
    %4 = iree_linalg_ext.softmax dimension(0) ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xf32>) -> tensor<2xf32>
    flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    return
  }
}

// -----// IR Dump After RematerializeParallelOps (iree-codegen-rematerialize-parallel-ops) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
  %3 = tensor.empty() : tensor<2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %5 = arith.sitofp %in : i8 to f32
    %6 = arith.subf %5, %cst : f32
    %7 = arith.mulf %6, %cst_0 : f32
    linalg.yield %7 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After BufferizeCopyOnlyDispatches (iree-codegen-bufferize-copy-only-dispatches) //----- //
module {
  func.func @main_dispatch_2_generic_2_f32xi8() {
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 2.560000e+02 : f32
    %cst_0 = arith.constant -1.280000e+02 : f32
    %cst_1 = arith.constant 1.270000e+02 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
    %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %3 = tensor.empty() : tensor<2xi8>
    %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
    ^bb0(%in: f32, %out: i8):
      %5 = arith.mulf %in, %cst : f32
      %6 = arith.addf %5, %cst_0 : f32
      %7 = math.roundeven %6 : f32
      %8 = arith.minf %7, %cst_1 : f32
      %9 = arith.maxf %8, %cst_0 : f32
      %10 = arith.fptosi %9 : f32 to i8
      linalg.yield %10 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
    return
  }
}

// -----// IR Dump After LLVMCPUMaterializeEncoding (iree-llvmcpu-materialize-encoding) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
  %3 = tensor.empty() : tensor<2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %5 = arith.sitofp %in : i8 to f32
    %6 = arith.subf %5, %cst : f32
    %7 = arith.mulf %6, %cst_0 : f32
    linalg.yield %7 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After EraseHALDescriptorTypeFromMemRef (iree-codegen-erase-hal-descriptor-type-from-memref) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
  %3 = tensor.empty() : tensor<2xf32>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2 : tensor<2xi8>) outs(%3 : tensor<2xf32>) {
  ^bb0(%in: i8, %out: f32):
    %5 = arith.sitofp %in : i8 to f32
    %6 = arith.subf %5, %cst : f32
    %7 = arith.mulf %6, %cst_0 : f32
    linalg.yield %7 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After DecomposeSoftmax (iree-linalg-ext-decompose-softmax) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = tensor.empty() : tensor<2xf32>
  %4 = tensor.empty() : tensor<2xf32>
  %5 = tensor.empty() : tensor<f32>
  %cst = arith.constant -1.000000e+30 : f32
  %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<f32>) -> tensor<f32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%2 : tensor<2xf32>) outs(%6 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %12 = arith.maxf %in, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2, %7 : tensor<2xf32>, tensor<f32>) outs(%4 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %12 = arith.subf %in, %in_1 : f32
    %13 = math.exp %12 : f32
    linalg.yield %13 : f32
  } -> tensor<2xf32>
  %cst_0 = arith.constant 0.000000e+00 : f32
  %9 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<f32>) -> tensor<f32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%8 : tensor<2xf32>) outs(%9 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %12 = arith.addf %in, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %11 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%8, %10 : tensor<2xf32>, tensor<f32>) outs(%4 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %12 = arith.divf %in, %in_1 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %11, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After DecomposeSoftmax (iree-linalg-ext-decompose-softmax) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = tensor.empty() : tensor<2xi8>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %5 = arith.mulf %in, %cst : f32
    %6 = arith.addf %5, %cst_0 : f32
    %7 = math.roundeven %6 : f32
    %8 = arith.minf %7, %cst_1 : f32
    %9 = arith.maxf %8, %cst_0 : f32
    %10 = arith.fptosi %9 : f32 to i8
    linalg.yield %10 : i8
  } -> tensor<2xi8>
  flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  return
}

// -----// IR Dump After RematerializeParallelOps (iree-codegen-rematerialize-parallel-ops) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = tensor.empty() : tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%2 : tensor<2xf32>) outs(%5 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.maxf %in, %out : f32
    linalg.yield %10 : f32
  } -> tensor<f32>
  %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%2, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.addf %11, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%3 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.divf %11, %in_2 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After RematerializeParallelOps (iree-codegen-rematerialize-parallel-ops) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = tensor.empty() : tensor<2xi8>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %5 = arith.mulf %in, %cst : f32
    %6 = arith.addf %5, %cst_0 : f32
    %7 = math.roundeven %6 : f32
    %8 = arith.minf %7, %cst_1 : f32
    %9 = arith.maxf %8, %cst_0 : f32
    %10 = arith.fptosi %9 : f32 to i8
    linalg.yield %10 : i8
  } -> tensor<2xi8>
  flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  return
}

// -----// IR Dump After LLVMCPUMaterializeEncoding (iree-llvmcpu-materialize-encoding) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = tensor.empty() : tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%2 : tensor<2xf32>) outs(%5 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.maxf %in, %out : f32
    linalg.yield %10 : f32
  } -> tensor<f32>
  %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%2, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.addf %11, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%3 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.divf %11, %in_2 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After LLVMCPUMaterializeEncoding (iree-llvmcpu-materialize-encoding) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = tensor.empty() : tensor<2xi8>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %5 = arith.mulf %in, %cst : f32
    %6 = arith.addf %5, %cst_0 : f32
    %7 = math.roundeven %6 : f32
    %8 = arith.minf %7, %cst_1 : f32
    %9 = arith.maxf %8, %cst_0 : f32
    %10 = arith.fptosi %9 : f32 to i8
    linalg.yield %10 : i8
  } -> tensor<2xi8>
  flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  return
}

// -----// IR Dump After EraseHALDescriptorTypeFromMemRef (iree-codegen-erase-hal-descriptor-type-from-memref) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = tensor.empty() : tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%2 : tensor<2xf32>) outs(%5 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.maxf %in, %out : f32
    linalg.yield %10 : f32
  } -> tensor<f32>
  %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%2, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.addf %11, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%3 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.divf %11, %in_2 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After TileAndDistributeToWorkgroups (iree-codegen-tile-and-distribute-to-workgroups) //----- //
hal.executable.variant public @static, target = <"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}> {
  hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert>} {
  ^bb0(%arg0: !hal.device):
    %c1 = arith.constant 1 : index
    hal.return %c1, %c1, %c1 : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_0_generic_2_i8xf32() {
      %c2 = arith.constant 2 : index
      %c0 = arith.constant 0 : index
      %cst = arith.constant -1.000000e+00 : f32
      %cst_0 = arith.constant 0.0125187514 : f32
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
      %workgroup_id_x = hal.interface.workgroup.id[0] : index
      %workgroup_count_x = hal.interface.workgroup.count[0] : index
      %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
      %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
      scf.for %arg0 = %2 to %c2 step %3 {
        %4 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [%c2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<?xi8>
        %5 = tensor.empty() : tensor<2xf32>
        %cast = tensor.cast %4 : tensor<?xi8> to tensor<2xi8>
        %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cast : tensor<2xi8>) outs(%5 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
        ^bb0(%in: i8, %out: f32):
          %7 = arith.sitofp %in : i8 to f32
          %8 = arith.subf %7, %cst : f32
          %9 = arith.mulf %8, %cst_0 : f32
          linalg.yield %9 : f32
        } -> tensor<2xf32>
        %cast_1 = tensor.cast %6 : tensor<2xf32> to tensor<?xf32>
        flow.dispatch.tensor.store %cast_1, %1, offsets = [%arg0], sizes = [%c2], strides = [1] : tensor<?xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
      }
      return
    }
  }
}

// -----// IR Dump After EraseHALDescriptorTypeFromMemRef (iree-codegen-erase-hal-descriptor-type-from-memref) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = tensor.empty() : tensor<2xi8>
  %4 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2 : tensor<2xf32>) outs(%3 : tensor<2xi8>) {
  ^bb0(%in: f32, %out: i8):
    %5 = arith.mulf %in, %cst : f32
    %6 = arith.addf %5, %cst_0 : f32
    %7 = math.roundeven %6 : f32
    %8 = arith.minf %7, %cst_1 : f32
    %9 = arith.maxf %8, %cst_0 : f32
    %10 = arith.fptosi %9 : f32 to i8
    linalg.yield %10 : i8
  } -> tensor<2xi8>
  flow.dispatch.tensor.store %4, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  return
}

// -----// IR Dump After TileAndDistributeToWorkgroups (iree-codegen-tile-and-distribute-to-workgroups) //----- //
hal.executable.variant public @static, target = <"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}> {
  hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert>} {
  ^bb0(%arg0: !hal.device):
    %c1 = arith.constant 1 : index
    hal.return %c1, %c1, %c1 : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_1_softmax_2xf32() {
      %cst = arith.constant 0.000000e+00 : f32
      %cst_0 = arith.constant -1.000000e+30 : f32
      %c0 = arith.constant 0 : index
      %c64 = arith.constant 64 : index
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
      %2 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
      %3 = tensor.empty() : tensor<2xf32>
      %4 = tensor.empty() : tensor<f32>
      %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
      %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%2 : tensor<2xf32>) outs(%5 : tensor<f32>) {
      ^bb0(%in: f32, %out: f32):
        %10 = arith.maxf %in, %out : f32
        linalg.yield %10 : f32
      } -> tensor<f32>
      %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
      %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%2, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
      ^bb0(%in: f32, %in_1: f32, %out: f32):
        %10 = arith.subf %in, %in_1 : f32
        %11 = math.exp %10 : f32
        %12 = arith.addf %11, %out : f32
        linalg.yield %12 : f32
      } -> tensor<f32>
      %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%2, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%3 : tensor<2xf32>) {
      ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
        %10 = arith.subf %in, %in_1 : f32
        %11 = math.exp %10 : f32
        %12 = arith.divf %11, %in_2 : f32
        linalg.yield %12 : f32
      } -> tensor<2xf32>
      flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
      return
    }
  }
}

// -----// IR Dump After ConvertToDestinationPassingStyle (iree-codegen-convert-to-destination-passing-style) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [%c2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<?xf32>
    %cast = tensor.cast %4 : tensor<?xf32> to tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [%c2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<?xi8>
    %cast_1 = tensor.cast %5 : tensor<?xi8> to tensor<2xi8>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cast_1 : tensor<2xi8>) outs(%cast : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: i8, %out: f32):
      %7 = arith.sitofp %in : i8 to f32
      %8 = arith.subf %7, %cst : f32
      %9 = arith.mulf %8, %cst_0 : f32
      linalg.yield %9 : f32
    } -> tensor<2xf32>
    %cast_2 = tensor.cast %6 : tensor<2xf32> to tensor<?xf32>
    flow.dispatch.tensor.store %cast_2, %1, offsets = [%arg0], sizes = [%c2], strides = [1] : tensor<?xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After TileAndDistributeToWorkgroups (iree-codegen-tile-and-distribute-to-workgroups) //----- //
hal.executable.variant public @static, target = <"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}> {
  hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert>} {
  ^bb0(%arg0: !hal.device):
    %c1 = arith.constant 1 : index
    hal.return %c1, %c1, %c1 : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_2_generic_2_f32xi8() {
      %c2 = arith.constant 2 : index
      %c64 = arith.constant 64 : index
      %c0 = arith.constant 0 : index
      %cst = arith.constant 2.560000e+02 : f32
      %cst_0 = arith.constant -1.280000e+02 : f32
      %cst_1 = arith.constant 1.270000e+02 : f32
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
      %workgroup_id_x = hal.interface.workgroup.id[0] : index
      %workgroup_count_x = hal.interface.workgroup.count[0] : index
      %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
      %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
      scf.for %arg0 = %2 to %c2 step %3 {
        %4 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [%c2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<?xf32>
        %5 = tensor.empty() : tensor<2xi8>
        %cast = tensor.cast %4 : tensor<?xf32> to tensor<2xf32>
        %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cast : tensor<2xf32>) outs(%5 : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
        ^bb0(%in: f32, %out: i8):
          %7 = arith.mulf %in, %cst : f32
          %8 = arith.addf %7, %cst_0 : f32
          %9 = math.roundeven %8 : f32
          %10 = arith.minf %9, %cst_1 : f32
          %11 = arith.maxf %10, %cst_0 : f32
          %12 = arith.fptosi %11 : f32 to i8
          linalg.yield %12 : i8
        } -> tensor<2xi8>
        %cast_2 = tensor.cast %6 : tensor<2xi8> to tensor<?xi8>
        flow.dispatch.tensor.store %cast_2, %1, offsets = [%arg0], sizes = [%c2], strides = [1] : tensor<?xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
      }
      return
    }
  }
}

// -----// IR Dump After FoldAffineMinInDistributedLoops (iree-codegen-fold-affinemin-in-distributed-loops) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [%c2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<?xf32>
    %cast = tensor.cast %4 : tensor<?xf32> to tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [%c2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<?xi8>
    %cast_1 = tensor.cast %5 : tensor<?xi8> to tensor<2xi8>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cast_1 : tensor<2xi8>) outs(%cast : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: i8, %out: f32):
      %7 = arith.sitofp %in : i8 to f32
      %8 = arith.subf %7, %cst : f32
      %9 = arith.mulf %8, %cst_0 : f32
      linalg.yield %9 : f32
    } -> tensor<2xf32>
    %cast_2 = tensor.cast %6 : tensor<2xf32> to tensor<?xf32>
    flow.dispatch.tensor.store %cast_2, %1, offsets = [%arg0], sizes = [%c2], strides = [1] : tensor<?xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @main_dispatch_0_generic_2_i8xf32() {
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant -1.000000e+00 : f32
    %cst_0 = arith.constant 0.0125187514 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
    %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
    scf.for %arg0 = %2 to %c2 step %3 {
      %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
      %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
      %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xi8>) outs(%4 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
      ^bb0(%in: i8, %out: f32):
        %7 = arith.sitofp %in : i8 to f32
        %8 = arith.subf %7, %cst : f32
        %9 = arith.mulf %8, %cst_0 : f32
        linalg.yield %9 : f32
      } -> tensor<2xf32>
      flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @main_dispatch_0_generic_2_i8xf32() {
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant -1.000000e+00 : f32
    %cst_0 = arith.constant 0.0125187514 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
    %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
    scf.for %arg0 = %2 to %c2 step %3 {
      %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
      %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
      %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xi8>) outs(%4 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
      ^bb0(%in: i8, %out: f32):
        %7 = arith.sitofp %in : i8 to f32
        %8 = arith.subf %7, %cst : f32
        %9 = arith.mulf %8, %cst_0 : f32
        linalg.yield %9 : f32
      } -> tensor<2xf32>
      flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    }
    return
  }
}

// -----// IR Dump After ConvertToDestinationPassingStyle (iree-codegen-convert-to-destination-passing-style) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = tensor.empty() : tensor<f32>
  %6 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%6 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %11 = arith.maxf %in, %out : f32
    linalg.yield %11 : f32
  } -> tensor<f32>
  %8 = linalg.fill ins(%cst : f32) outs(%5 : tensor<f32>) -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %7 : tensor<2xf32>, tensor<f32>) outs(%8 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %11 = arith.subf %in, %in_1 : f32
    %12 = math.exp %11 : f32
    %13 = arith.addf %12, %out : f32
    linalg.yield %13 : f32
  } -> tensor<f32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %7, %9 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %11 = arith.subf %in, %in_1 : f32
    %12 = math.exp %11 : f32
    %13 = arith.divf %12, %in_2 : f32
    linalg.yield %13 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %10, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After FoldAffineMinInDistributedLoops (iree-codegen-fold-affinemin-in-distributed-loops) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = tensor.empty() : tensor<f32>
  %6 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%6 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %11 = arith.maxf %in, %out : f32
    linalg.yield %11 : f32
  } -> tensor<f32>
  %8 = linalg.fill ins(%cst : f32) outs(%5 : tensor<f32>) -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %7 : tensor<2xf32>, tensor<f32>) outs(%8 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %11 = arith.subf %in, %in_1 : f32
    %12 = math.exp %11 : f32
    %13 = arith.addf %12, %out : f32
    linalg.yield %13 : f32
  } -> tensor<f32>
  %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %7, %9 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %11 = arith.subf %in, %in_1 : f32
    %12 = math.exp %11 : f32
    %13 = arith.divf %12, %in_2 : f32
    linalg.yield %13 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %10, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After FuseTensorPadWithConsumer (iree-codegen-fuse-tensor-pad-with-consumer) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xi8>) outs(%4 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: i8, %out: f32):
      %7 = arith.sitofp %in : i8 to f32
      %8 = arith.subf %7, %cst : f32
      %9 = arith.mulf %8, %cst_0 : f32
      linalg.yield %9 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After ConvertToDestinationPassingStyle (iree-codegen-convert-to-destination-passing-style) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [%c2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<?xi8>
    %cast = tensor.cast %4 : tensor<?xi8> to tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [%c2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<?xf32>
    %cast_2 = tensor.cast %5 : tensor<?xf32> to tensor<2xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cast_2 : tensor<2xf32>) outs(%cast : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: f32, %out: i8):
      %7 = arith.mulf %in, %cst : f32
      %8 = arith.addf %7, %cst_0 : f32
      %9 = math.roundeven %8 : f32
      %10 = arith.minf %9, %cst_1 : f32
      %11 = arith.maxf %10, %cst_0 : f32
      %12 = arith.fptosi %11 : f32 to i8
      linalg.yield %12 : i8
    } -> tensor<2xi8>
    %cast_3 = tensor.cast %6 : tensor<2xi8> to tensor<?xi8>
    flow.dispatch.tensor.store %cast_3, %1, offsets = [%arg0], sizes = [%c2], strides = [1] : tensor<?xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @main_dispatch_1_softmax_2xf32() {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -1.000000e+30 : f32
    %c0 = arith.constant 0 : index
    %c64 = arith.constant 64 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %4 = tensor.empty() : tensor<f32>
    %5 = tensor.empty() : tensor<f32>
    %6 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%6 : tensor<f32>) {
    ^bb0(%in: f32, %out: f32):
      %11 = arith.maxf %in, %out : f32
      linalg.yield %11 : f32
    } -> tensor<f32>
    %8 = linalg.fill ins(%cst : f32) outs(%5 : tensor<f32>) -> tensor<f32>
    %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %7 : tensor<2xf32>, tensor<f32>) outs(%8 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %11 = arith.subf %in, %in_1 : f32
      %12 = math.exp %11 : f32
      %13 = arith.addf %12, %out : f32
      linalg.yield %13 : f32
    } -> tensor<f32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %7, %9 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
    ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
      %11 = arith.subf %in, %in_1 : f32
      %12 = math.exp %11 : f32
      %13 = arith.divf %12, %in_2 : f32
      linalg.yield %13 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %10, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    return
  }
}

// -----// IR Dump After ConcretizePadResultShape (iree-codegen-concretize-pad-result-shape) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xi8>) outs(%4 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: i8, %out: f32):
      %7 = arith.sitofp %in : i8 to f32
      %8 = arith.subf %7, %cst : f32
      %9 = arith.mulf %8, %cst_0 : f32
      linalg.yield %9 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After TileAndDecomposeAttention (iree-linalg-ext-tile-and-decompose-attention) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xi8>) outs(%4 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: i8, %out: f32):
      %7 = arith.sitofp %in : i8 to f32
      %8 = arith.subf %7, %cst : f32
      %9 = arith.mulf %8, %cst_0 : f32
      linalg.yield %9 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @main_dispatch_1_softmax_2xf32() {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant -1.000000e+30 : f32
    %c0 = arith.constant 0 : index
    %c64 = arith.constant 64 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %4 = tensor.empty() : tensor<f32>
    %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%5 : tensor<f32>) {
    ^bb0(%in: f32, %out: f32):
      %10 = arith.maxf %in, %out : f32
      linalg.yield %10 : f32
    } -> tensor<f32>
    %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
    %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
    ^bb0(%in: f32, %in_1: f32, %out: f32):
      %10 = arith.subf %in, %in_1 : f32
      %11 = math.exp %10 : f32
      %12 = arith.addf %11, %out : f32
      linalg.yield %12 : f32
    } -> tensor<f32>
    %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
    ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
      %10 = arith.subf %in, %in_1 : f32
      %11 = math.exp %10 : f32
      %12 = arith.divf %11, %in_2 : f32
      linalg.yield %12 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    return
  }
}

// -----// IR Dump After FoldAffineMinInDistributedLoops (iree-codegen-fold-affinemin-in-distributed-loops) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [%c2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<?xi8>
    %cast = tensor.cast %4 : tensor<?xi8> to tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [%c2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<?xf32>
    %cast_2 = tensor.cast %5 : tensor<?xf32> to tensor<2xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%cast_2 : tensor<2xf32>) outs(%cast : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: f32, %out: i8):
      %7 = arith.mulf %in, %cst : f32
      %8 = arith.addf %7, %cst_0 : f32
      %9 = math.roundeven %8 : f32
      %10 = arith.minf %9, %cst_1 : f32
      %11 = arith.maxf %10, %cst_0 : f32
      %12 = arith.fptosi %11 : f32 to i8
      linalg.yield %12 : i8
    } -> tensor<2xi8>
    %cast_3 = tensor.cast %6 : tensor<2xi8> to tensor<?xi8>
    flow.dispatch.tensor.store %cast_3, %1, offsets = [%arg0], sizes = [%c2], strides = [1] : tensor<?xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After TileAndDecomposeWinogradTransform (iree-linalg-ext-tile-and-decompose-winograd) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xi8>) outs(%4 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: i8, %out: f32):
      %7 = arith.sitofp %in : i8 to f32
      %8 = arith.subf %7, %cst : f32
      %9 = arith.mulf %8, %cst_0 : f32
      linalg.yield %9 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @main_dispatch_2_generic_2_f32xi8() {
    %c2 = arith.constant 2 : index
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 2.560000e+02 : f32
    %cst_0 = arith.constant -1.280000e+02 : f32
    %cst_1 = arith.constant 1.270000e+02 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
    %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
    scf.for %arg0 = %2 to %c2 step %3 {
      %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
      %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
      %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xf32>) outs(%4 : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
      ^bb0(%in: f32, %out: i8):
        %7 = arith.mulf %in, %cst : f32
        %8 = arith.addf %7, %cst_0 : f32
        %9 = math.roundeven %8 : f32
        %10 = arith.minf %9, %cst_1 : f32
        %11 = arith.maxf %10, %cst_0 : f32
        %12 = arith.fptosi %11 : f32 to i8
        linalg.yield %12 : i8
      } -> tensor<2xi8>
      flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
    }
    return
  }
}

// -----// IR Dump After RematerializeParallelOps (iree-codegen-rematerialize-parallel-ops) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xi8>) outs(%4 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: i8, %out: f32):
      %7 = arith.sitofp %in : i8 to f32
      %8 = arith.subf %7, %cst : f32
      %9 = arith.mulf %8, %cst_0 : f32
      linalg.yield %9 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After FuseTensorPadWithConsumer (iree-codegen-fuse-tensor-pad-with-consumer) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%5 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.maxf %in, %out : f32
    linalg.yield %10 : f32
  } -> tensor<f32>
  %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.addf %11, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.divf %11, %in_2 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @main_dispatch_2_generic_2_f32xi8() {
    %c2 = arith.constant 2 : index
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 2.560000e+02 : f32
    %cst_0 = arith.constant -1.280000e+02 : f32
    %cst_1 = arith.constant 1.270000e+02 : f32
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
    %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
    scf.for %arg0 = %2 to %c2 step %3 {
      %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
      %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
      %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xf32>) outs(%4 : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
      ^bb0(%in: f32, %out: i8):
        %7 = arith.mulf %in, %cst : f32
        %8 = arith.addf %7, %cst_0 : f32
        %9 = math.roundeven %8 : f32
        %10 = arith.minf %9, %cst_1 : f32
        %11 = arith.maxf %10, %cst_0 : f32
        %12 = arith.fptosi %11 : f32 to i8
        linalg.yield %12 : i8
      } -> tensor<2xi8>
      flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
    }
    return
  }
}

// -----// IR Dump After LLVMCPUTileAndFuse (iree-llvmcpu-tile-and-fuse) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xi8>) outs(%4 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: i8, %out: f32):
      %7 = arith.sitofp %in : i8 to f32
      %8 = arith.subf %7, %cst : f32
      %9 = arith.mulf %8, %cst_0 : f32
      linalg.yield %9 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After ConcretizePadResultShape (iree-codegen-concretize-pad-result-shape) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%5 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.maxf %in, %out : f32
    linalg.yield %10 : f32
  } -> tensor<f32>
  %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.addf %11, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.divf %11, %in_2 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After FuseTensorPadWithConsumer (iree-codegen-fuse-tensor-pad-with-consumer) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xi8>) outs(%4 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: i8, %out: f32):
      %7 = arith.sitofp %in : i8 to f32
      %8 = arith.subf %7, %cst : f32
      %9 = arith.mulf %8, %cst_0 : f32
      linalg.yield %9 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After TileAndDecomposeAttention (iree-linalg-ext-tile-and-decompose-attention) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%5 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.maxf %in, %out : f32
    linalg.yield %10 : f32
  } -> tensor<f32>
  %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.addf %11, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.divf %11, %in_2 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After FuseTensorPadWithConsumer (iree-codegen-fuse-tensor-pad-with-consumer) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xf32>) outs(%4 : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: f32, %out: i8):
      %7 = arith.mulf %in, %cst : f32
      %8 = arith.addf %7, %cst_0 : f32
      %9 = math.roundeven %8 : f32
      %10 = arith.minf %9, %cst_1 : f32
      %11 = arith.maxf %10, %cst_0 : f32
      %12 = arith.fptosi %11 : f32 to i8
      linalg.yield %12 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After ConcretizePadResultShape (iree-codegen-concretize-pad-result-shape) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xi8>) outs(%4 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: i8, %out: f32):
      %7 = arith.sitofp %in : i8 to f32
      %8 = arith.subf %7, %cst : f32
      %9 = arith.mulf %8, %cst_0 : f32
      linalg.yield %9 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After TileAndDecomposeWinogradTransform (iree-linalg-ext-tile-and-decompose-winograd) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%5 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.maxf %in, %out : f32
    linalg.yield %10 : f32
  } -> tensor<f32>
  %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.addf %11, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.divf %11, %in_2 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After ConcretizePadResultShape (iree-codegen-concretize-pad-result-shape) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xf32>) outs(%4 : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: f32, %out: i8):
      %7 = arith.mulf %in, %cst : f32
      %8 = arith.addf %7, %cst_0 : f32
      %9 = math.roundeven %8 : f32
      %10 = arith.minf %9, %cst_1 : f32
      %11 = arith.maxf %10, %cst_0 : f32
      %12 = arith.fptosi %11 : f32 to i8
      linalg.yield %12 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After LLVMCPUSplitReduction (iree-llvmcpu-split-reduction) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xi8>) outs(%4 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: i8, %out: f32):
      %7 = arith.sitofp %in : i8 to f32
      %8 = arith.subf %7, %cst : f32
      %9 = arith.mulf %8, %cst_0 : f32
      linalg.yield %9 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After RematerializeParallelOps (iree-codegen-rematerialize-parallel-ops) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%5 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.maxf %in, %out : f32
    linalg.yield %10 : f32
  } -> tensor<f32>
  %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.addf %11, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.divf %11, %in_2 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After TileAndDecomposeAttention (iree-linalg-ext-tile-and-decompose-attention) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xf32>) outs(%4 : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: f32, %out: i8):
      %7 = arith.mulf %in, %cst : f32
      %8 = arith.addf %7, %cst_0 : f32
      %9 = math.roundeven %8 : f32
      %10 = arith.minf %9, %cst_1 : f32
      %11 = arith.maxf %10, %cst_0 : f32
      %12 = arith.fptosi %11 : f32 to i8
      linalg.yield %12 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After LLVMCPUTile (iree-llvmcpu-tile) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xi8>) outs(%4 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: i8, %out: f32):
      %7 = arith.sitofp %in : i8 to f32
      %8 = arith.subf %7, %cst : f32
      %9 = arith.mulf %8, %cst_0 : f32
      linalg.yield %9 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After LLVMCPUTileAndFuse (iree-llvmcpu-tile-and-fuse) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%5 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.maxf %in, %out : f32
    linalg.yield %10 : f32
  } -> tensor<f32>
  %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.addf %11, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.divf %11, %in_2 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After TileAndDecomposeWinogradTransform (iree-linalg-ext-tile-and-decompose-winograd) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xf32>) outs(%4 : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: f32, %out: i8):
      %7 = arith.mulf %in, %cst : f32
      %8 = arith.addf %7, %cst_0 : f32
      %9 = math.roundeven %8 : f32
      %10 = arith.minf %9, %cst_1 : f32
      %11 = arith.maxf %10, %cst_0 : f32
      %12 = arith.fptosi %11 : f32 to i8
      linalg.yield %12 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After FuseTensorPadWithConsumer (iree-codegen-fuse-tensor-pad-with-consumer) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xi8>) outs(%4 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: i8, %out: f32):
      %7 = arith.sitofp %in : i8 to f32
      %8 = arith.subf %7, %cst : f32
      %9 = arith.mulf %8, %cst_0 : f32
      linalg.yield %9 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After FuseTensorPadWithConsumer (iree-codegen-fuse-tensor-pad-with-consumer) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%5 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.maxf %in, %out : f32
    linalg.yield %10 : f32
  } -> tensor<f32>
  %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.addf %11, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.divf %11, %in_2 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After RematerializeParallelOps (iree-codegen-rematerialize-parallel-ops) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xf32>) outs(%4 : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: f32, %out: i8):
      %7 = arith.mulf %in, %cst : f32
      %8 = arith.addf %7, %cst_0 : f32
      %9 = math.roundeven %8 : f32
      %10 = arith.minf %9, %cst_1 : f32
      %11 = arith.maxf %10, %cst_0 : f32
      %12 = arith.fptosi %11 : f32 to i8
      linalg.yield %12 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After ConcretizePadResultShape (iree-codegen-concretize-pad-result-shape) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xi8>) outs(%4 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: i8, %out: f32):
      %7 = arith.sitofp %in : i8 to f32
      %8 = arith.subf %7, %cst : f32
      %9 = arith.mulf %8, %cst_0 : f32
      linalg.yield %9 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After ConcretizePadResultShape (iree-codegen-concretize-pad-result-shape) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%5 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.maxf %in, %out : f32
    linalg.yield %10 : f32
  } -> tensor<f32>
  %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.addf %11, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.divf %11, %in_2 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After TensorToVectorVectorizePad (iree-codegen-vectorize-tensor-pad) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xi8>) outs(%4 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: i8, %out: f32):
      %7 = arith.sitofp %in : i8 to f32
      %8 = arith.subf %7, %cst : f32
      %9 = arith.mulf %8, %cst_0 : f32
      linalg.yield %9 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After LLVMCPUTileAndFuse (iree-llvmcpu-tile-and-fuse) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xf32>) outs(%4 : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: f32, %out: i8):
      %7 = arith.mulf %in, %cst : f32
      %8 = arith.addf %7, %cst_0 : f32
      %9 = math.roundeven %8 : f32
      %10 = arith.minf %9, %cst_1 : f32
      %11 = arith.maxf %10, %cst_0 : f32
      %12 = arith.fptosi %11 : f32 to i8
      linalg.yield %12 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After LLVMCPUSplitReduction (iree-llvmcpu-split-reduction) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%5 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.maxf %in, %out : f32
    linalg.yield %10 : f32
  } -> tensor<f32>
  %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.addf %11, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.divf %11, %in_2 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After DecomposePackUnPackOps (iree-codegen-decompose-pack-unpack-ops) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xi8>) outs(%4 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: i8, %out: f32):
      %7 = arith.sitofp %in : i8 to f32
      %8 = arith.subf %7, %cst : f32
      %9 = arith.mulf %8, %cst_0 : f32
      linalg.yield %9 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After FuseTensorPadWithConsumer (iree-codegen-fuse-tensor-pad-with-consumer) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xf32>) outs(%4 : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: f32, %out: i8):
      %7 = arith.mulf %in, %cst : f32
      %8 = arith.addf %7, %cst_0 : f32
      %9 = math.roundeven %8 : f32
      %10 = arith.minf %9, %cst_1 : f32
      %11 = arith.maxf %10, %cst_0 : f32
      %12 = arith.fptosi %11 : f32 to i8
      linalg.yield %12 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xi8>) outs(%4 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: i8, %out: f32):
      %7 = arith.sitofp %in : i8 to f32
      %8 = arith.subf %7, %cst : f32
      %9 = arith.mulf %8, %cst_0 : f32
      linalg.yield %9 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant -1.000000e+00 : f32
  %cst_0 = arith.constant 0.0125187514 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xi8>) outs(%4 : tensor<2xf32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: i8, %out: f32):
      %7 = arith.sitofp %in : i8 to f32
      %8 = arith.subf %7, %cst : f32
      %9 = arith.mulf %8, %cst_0 : f32
      linalg.yield %9 : f32
    } -> tensor<2xf32>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After ConcretizePadResultShape (iree-codegen-concretize-pad-result-shape) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xf32>) outs(%4 : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: f32, %out: i8):
      %7 = arith.mulf %in, %cst : f32
      %8 = arith.addf %7, %cst_0 : f32
      %9 = math.roundeven %8 : f32
      %10 = arith.minf %9, %cst_1 : f32
      %11 = arith.maxf %10, %cst_0 : f32
      %12 = arith.fptosi %11 : f32 to i8
      linalg.yield %12 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After LLVMCPUTile (iree-llvmcpu-tile) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%5 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.maxf %in, %out : f32
    linalg.yield %10 : f32
  } -> tensor<f32>
  %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.addf %11, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.divf %11, %in_2 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After LLVMCPUSplitReduction (iree-llvmcpu-split-reduction) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xf32>) outs(%4 : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: f32, %out: i8):
      %7 = arith.mulf %in, %cst : f32
      %8 = arith.addf %7, %cst_0 : f32
      %9 = math.roundeven %8 : f32
      %10 = arith.minf %9, %cst_1 : f32
      %11 = arith.maxf %10, %cst_0 : f32
      %12 = arith.fptosi %11 : f32 to i8
      linalg.yield %12 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After LLVMCPUVectorization (iree-llvmcpu-vectorization) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0_i8 = arith.constant 0 : i8
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %6 = vector.transfer_read %5[%c0], %c0_i8 {in_bounds = [true]} : tensor<2xi8>, vector<2xi8>
    %7 = arith.sitofp %6 : vector<2xi8> to vector<2xf32>
    %8 = arith.subf %7, %cst_0 : vector<2xf32>
    %9 = arith.mulf %8, %cst : vector<2xf32>
    %10 = vector.transfer_write %9, %4[%c0] {in_bounds = [true]} : vector<2xf32>, tensor<2xf32>
    flow.dispatch.tensor.store %10, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After FuseTensorPadWithConsumer (iree-codegen-fuse-tensor-pad-with-consumer) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%5 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.maxf %in, %out : f32
    linalg.yield %10 : f32
  } -> tensor<f32>
  %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.addf %11, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.divf %11, %in_2 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After LLVMCPUTile (iree-llvmcpu-tile) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xf32>) outs(%4 : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: f32, %out: i8):
      %7 = arith.mulf %in, %cst : f32
      %8 = arith.addf %7, %cst_0 : f32
      %9 = math.roundeven %8 : f32
      %10 = arith.minf %9, %cst_1 : f32
      %11 = arith.maxf %10, %cst_0 : f32
      %12 = arith.fptosi %11 : f32 to i8
      linalg.yield %12 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0_i8 = arith.constant 0 : i8
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %6 = vector.transfer_read %5[%c0], %c0_i8 {in_bounds = [true]} : tensor<2xi8>, vector<2xi8>
    %7 = arith.sitofp %6 : vector<2xi8> to vector<2xf32>
    %8 = arith.subf %7, %cst_0 : vector<2xf32>
    %9 = arith.mulf %8, %cst : vector<2xf32>
    %10 = vector.transfer_write %9, %4[%c0] {in_bounds = [true]} : vector<2xf32>, tensor<2xf32>
    flow.dispatch.tensor.store %10, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0_i8 = arith.constant 0 : i8
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
    %6 = vector.transfer_read %5[%c0], %c0_i8 {in_bounds = [true]} : tensor<2xi8>, vector<2xi8>
    %7 = arith.sitofp %6 : vector<2xi8> to vector<2xf32>
    %8 = arith.subf %7, %cst_0 : vector<2xf32>
    %9 = arith.mulf %8, %cst : vector<2xf32>
    %10 = vector.transfer_write %9, %4[%c0] {in_bounds = [true]} : vector<2xf32>, tensor<2xf32>
    flow.dispatch.tensor.store %10, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  }
  return
}

// -----// IR Dump After FuseTensorPadWithConsumer (iree-codegen-fuse-tensor-pad-with-consumer) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xf32>) outs(%4 : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: f32, %out: i8):
      %7 = arith.mulf %in, %cst : f32
      %8 = arith.addf %7, %cst_0 : f32
      %9 = math.roundeven %8 : f32
      %10 = arith.minf %9, %cst_1 : f32
      %11 = arith.maxf %10, %cst_0 : f32
      %12 = arith.fptosi %11 : f32 to i8
      linalg.yield %12 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After ConcretizePadResultShape (iree-codegen-concretize-pad-result-shape) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%5 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.maxf %in, %out : f32
    linalg.yield %10 : f32
  } -> tensor<f32>
  %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.addf %11, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.divf %11, %in_2 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After EliminateEmptyTensors (iree-eliminate-empty-tensors) //----- //
module {
  func.func @main_dispatch_0_generic_2_i8xf32() {
    %cst = arith.constant dense<0.0125187514> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
    %c0_i8 = arith.constant 0 : i8
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
    %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
    scf.for %arg0 = %2 to %c2 step %3 {
      %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
      %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
      %6 = vector.transfer_read %5[%c0], %c0_i8 {in_bounds = [true]} : tensor<2xi8>, vector<2xi8>
      %7 = arith.sitofp %6 : vector<2xi8> to vector<2xf32>
      %8 = arith.subf %7, %cst_0 : vector<2xf32>
      %9 = arith.mulf %8, %cst : vector<2xf32>
      %10 = vector.transfer_write %9, %4[%c0] {in_bounds = [true]} : vector<2xf32>, tensor<2xf32>
      flow.dispatch.tensor.store %10, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    }
    return
  }
}

// -----// IR Dump After ConcretizePadResultShape (iree-codegen-concretize-pad-result-shape) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xf32>) outs(%4 : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: f32, %out: i8):
      %7 = arith.mulf %in, %cst : f32
      %8 = arith.addf %7, %cst_0 : f32
      %9 = math.roundeven %8 : f32
      %10 = arith.minf %9, %cst_1 : f32
      %11 = arith.maxf %10, %cst_0 : f32
      %12 = arith.fptosi %11 : f32 to i8
      linalg.yield %12 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After TensorToVectorVectorizePad (iree-codegen-vectorize-tensor-pad) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%5 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.maxf %in, %out : f32
    linalg.yield %10 : f32
  } -> tensor<f32>
  %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.addf %11, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.divf %11, %in_2 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After EmptyTensorToAllocTensor (empty-tensor-to-alloc-tensor) //----- //
module {
  func.func @main_dispatch_0_generic_2_i8xf32() {
    %cst = arith.constant dense<0.0125187514> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
    %c0_i8 = arith.constant 0 : i8
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xi8>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
    %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
    scf.for %arg0 = %2 to %c2 step %3 {
      %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
      %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xi8>> -> tensor<2xi8>
      %6 = vector.transfer_read %5[%c0], %c0_i8 {in_bounds = [true]} : tensor<2xi8>, vector<2xi8>
      %7 = arith.sitofp %6 : vector<2xi8> to vector<2xf32>
      %8 = arith.subf %7, %cst_0 : vector<2xf32>
      %9 = arith.mulf %8, %cst : vector<2xf32>
      %10 = vector.transfer_write %9, %4[%c0] {in_bounds = [true]} : vector<2xf32>, tensor<2xf32>
      flow.dispatch.tensor.store %10, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    }
    return
  }
}

// -----// IR Dump After TensorToVectorVectorizePad (iree-codegen-vectorize-tensor-pad) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xf32>) outs(%4 : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: f32, %out: i8):
      %7 = arith.mulf %in, %cst : f32
      %8 = arith.addf %7, %cst_0 : f32
      %9 = math.roundeven %8 : f32
      %10 = arith.minf %9, %cst_1 : f32
      %11 = arith.maxf %10, %cst_0 : f32
      %12 = arith.fptosi %11 : f32 to i8
      linalg.yield %12 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After DecomposePackUnPackOps (iree-codegen-decompose-pack-unpack-ops) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%5 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.maxf %in, %out : f32
    linalg.yield %10 : f32
  } -> tensor<f32>
  %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.addf %11, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.divf %11, %in_2 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After IREEComprehensiveBufferize (iree-codegen-iree-comprehensive-bufferize) //----- //
module {
  func.func @main_dispatch_0_generic_2_i8xf32() {
    %cst = arith.constant dense<0.0125187514> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
    %c0_i8 = arith.constant 0 : i8
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<2xi8, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<2xf32, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
    %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
    scf.for %arg0 = %2 to %c2 step %3 {
      %4 = vector.transfer_read %0[%c0], %c0_i8 {in_bounds = [true]} : memref<2xi8, #hal.descriptor_type<storage_buffer>>, vector<2xi8>
      %5 = arith.sitofp %4 : vector<2xi8> to vector<2xf32>
      %6 = arith.subf %5, %cst_0 : vector<2xf32>
      %7 = arith.mulf %6, %cst : vector<2xf32>
      vector.transfer_write %7, %1[%c0] {in_bounds = [true]} : vector<2xf32>, memref<2xf32, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%1 : memref<2xf32, #hal.descriptor_type<storage_buffer>>) outs(%1 : memref<2xf32, #hal.descriptor_type<storage_buffer>>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
    }
    return
  }
}

// -----// IR Dump After DecomposePackUnPackOps (iree-codegen-decompose-pack-unpack-ops) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xf32>) outs(%4 : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: f32, %out: i8):
      %7 = arith.mulf %in, %cst : f32
      %8 = arith.addf %7, %cst_0 : f32
      %9 = math.roundeven %8 : f32
      %10 = arith.minf %9, %cst_1 : f32
      %11 = arith.maxf %10, %cst_0 : f32
      %12 = arith.fptosi %11 : f32 to i8
      linalg.yield %12 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%5 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.maxf %in, %out : f32
    linalg.yield %10 : f32
  } -> tensor<f32>
  %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.addf %11, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.divf %11, %in_2 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
module {
  func.func @main_dispatch_0_generic_2_i8xf32() {
    %cst = arith.constant dense<0.0125187514> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
    %c0_i8 = arith.constant 0 : i8
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<2xi8, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<2xf32, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
    %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
    scf.for %arg0 = %2 to %c2 step %3 {
      %4 = vector.transfer_read %0[%c0], %c0_i8 {in_bounds = [true]} : memref<2xi8, #hal.descriptor_type<storage_buffer>>, vector<2xi8>
      %5 = arith.sitofp %4 : vector<2xi8> to vector<2xf32>
      %6 = arith.subf %5, %cst_0 : vector<2xf32>
      %7 = arith.mulf %6, %cst : vector<2xf32>
      vector.transfer_write %7, %1[%c0] {in_bounds = [true]} : vector<2xf32>, memref<2xf32, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%1 : memref<2xf32, #hal.descriptor_type<storage_buffer>>) outs(%1 : memref<2xf32, #hal.descriptor_type<storage_buffer>>) {
      ^bb0(%in: f32, %out: f32):
        linalg.yield %in : f32
      }
    }
    return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xf32>) outs(%4 : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: f32, %out: i8):
      %7 = arith.mulf %in, %cst : f32
      %8 = arith.addf %7, %cst_0 : f32
      %9 = math.roundeven %8 : f32
      %10 = arith.minf %9, %cst_1 : f32
      %11 = arith.maxf %10, %cst_0 : f32
      %12 = arith.fptosi %11 : f32 to i8
      linalg.yield %12 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant -1.000000e+30 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = linalg.fill ins(%cst_0 : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3 : tensor<2xf32>) outs(%5 : tensor<f32>) {
  ^bb0(%in: f32, %out: f32):
    %10 = arith.maxf %in, %out : f32
    linalg.yield %10 : f32
  } -> tensor<f32>
  %7 = linalg.fill ins(%cst : f32) outs(%4 : tensor<f32>) -> tensor<f32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>], iterator_types = ["reduction"]} ins(%3, %6 : tensor<2xf32>, tensor<f32>) outs(%7 : tensor<f32>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[0], [0], [4]]>} {
  ^bb0(%in: f32, %in_1: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.addf %11, %out : f32
    linalg.yield %12 : f32
  } -> tensor<f32>
  %9 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> ()>, affine_map<(d0) -> ()>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%3, %6, %8 : tensor<2xf32>, tensor<f32>, tensor<f32>) outs(%2 : tensor<2xf32>) {
  ^bb0(%in: f32, %in_1: f32, %in_2: f32, %out: f32):
    %10 = arith.subf %in, %in_1 : f32
    %11 = math.exp %10 : f32
    %12 = arith.divf %11, %in_2 : f32
    linalg.yield %12 : f32
  } -> tensor<2xf32>
  flow.dispatch.tensor.store %9, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 2.560000e+02 : f32
  %cst_0 = arith.constant -1.280000e+02 : f32
  %cst_1 = arith.constant 1.270000e+02 : f32
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %6 = linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%5 : tensor<2xf32>) outs(%4 : tensor<2xi8>) attrs =  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[2], [4], [0]]>} {
    ^bb0(%in: f32, %out: i8):
      %7 = arith.mulf %in, %cst : f32
      %8 = arith.addf %7, %cst_0 : f32
      %9 = math.roundeven %8 : f32
      %10 = arith.minf %9, %cst_1 : f32
      %11 = arith.maxf %10, %cst_0 : f32
      %12 = arith.fptosi %11 : f32 to i8
      linalg.yield %12 : i8
    } -> tensor<2xi8>
    flow.dispatch.tensor.store %6, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0_i8 = arith.constant 0 : i8
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<2xi8, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<2xf32, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = vector.transfer_read %0[%c0], %c0_i8 {in_bounds = [true]} : memref<2xi8, #hal.descriptor_type<storage_buffer>>, vector<2xi8>
    %5 = arith.sitofp %4 : vector<2xi8> to vector<2xf32>
    %6 = arith.subf %5, %cst_0 : vector<2xf32>
    %7 = arith.mulf %6, %cst : vector<2xf32>
    vector.transfer_write %7, %1[%c0] {in_bounds = [true]} : vector<2xf32>, memref<2xf32, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0_i8 = arith.constant 0 : i8
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<2xi8, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<2xf32, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = vector.transfer_read %0[%c0], %c0_i8 {in_bounds = [true]} : memref<2xi8, #hal.descriptor_type<storage_buffer>>, vector<2xi8>
    %5 = arith.sitofp %4 : vector<2xi8> to vector<2xf32>
    %6 = arith.subf %5, %cst_0 : vector<2xf32>
    %7 = arith.mulf %6, %cst : vector<2xf32>
    vector.transfer_write %7, %1[%c0] {in_bounds = [true]} : vector<2xf32>, memref<2xf32, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After LLVMCPUVectorization (iree-llvmcpu-vectorization) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant dense<-1.000000e+30> : vector<f32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_1 = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = vector.transfer_read %3[%c0], %cst_1 {in_bounds = [true]} : tensor<2xf32>, vector<2xf32>
  %6 = vector.extractelement %cst[] : vector<f32>
  %7 = vector.multi_reduction <maxf>, %5, %6 [0] : vector<2xf32> to f32
  %8 = vector.broadcast %7 : f32 to vector<f32>
  %9 = vector.transfer_write %8, %4[] : vector<f32>, tensor<f32>
  %10 = vector.transfer_read %3[%c0], %cst_1 {in_bounds = [true]} : tensor<2xf32>, vector<2xf32>
  %extracted = tensor.extract %9[] : tensor<f32>
  %11 = vector.broadcast %extracted : f32 to vector<2xf32>
  %12 = vector.extractelement %cst_0[] : vector<f32>
  %13 = arith.subf %10, %11 : vector<2xf32>
  %14 = math.exp %13 : vector<2xf32>
  %15 = vector.multi_reduction <add>, %14, %12 [0] : vector<2xf32> to f32
  %16 = vector.broadcast %15 : f32 to vector<f32>
  %17 = vector.transfer_write %16, %4[] : vector<f32>, tensor<f32>
  %18 = vector.transfer_read %3[%c0], %cst_1 {in_bounds = [true]} : tensor<2xf32>, vector<2xf32>
  %extracted_2 = tensor.extract %9[] : tensor<f32>
  %19 = vector.broadcast %extracted_2 : f32 to vector<2xf32>
  %extracted_3 = tensor.extract %17[] : tensor<f32>
  %20 = vector.broadcast %extracted_3 : f32 to vector<2xf32>
  %21 = arith.subf %18, %19 : vector<2xf32>
  %22 = math.exp %21 : vector<2xf32>
  %23 = arith.divf %22, %20 : vector<2xf32>
  %24 = vector.transfer_write %23, %2[%c0] {in_bounds = [true]} : vector<2xf32>, tensor<2xf32>
  flow.dispatch.tensor.store %24, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After LLVMCPUVectorization (iree-llvmcpu-vectorization) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %cst_2 = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %6 = vector.transfer_read %5[%c0], %cst_2 {in_bounds = [true]} : tensor<2xf32>, vector<2xf32>
    %7 = arith.mulf %6, %cst_1 : vector<2xf32>
    %8 = arith.addf %7, %cst_0 : vector<2xf32>
    %9 = math.roundeven %8 : vector<2xf32>
    %10 = arith.minf %9, %cst : vector<2xf32>
    %11 = arith.maxf %10, %cst_0 : vector<2xf32>
    %12 = arith.fptosi %11 : vector<2xf32> to vector<2xi8>
    %13 = vector.transfer_write %12, %4[%c0] {in_bounds = [true]} : vector<2xi8>, tensor<2xi8>
    flow.dispatch.tensor.store %13, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0_i8 = arith.constant 0 : i8
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<2xi8, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<2xf32, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = vector.transfer_read %0[%c0], %c0_i8 {in_bounds = [true]} : memref<2xi8, #hal.descriptor_type<storage_buffer>>, vector<2xi8>
    %5 = arith.sitofp %4 : vector<2xi8> to vector<2xf32>
    %6 = arith.subf %5, %cst_0 : vector<2xf32>
    %7 = arith.mulf %6, %cst : vector<2xf32>
    vector.transfer_write %7, %1[%c0] {in_bounds = [true]} : vector<2xf32>, memref<2xf32, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant dense<-1.000000e+30> : vector<f32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_1 = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = vector.transfer_read %3[%c0], %cst_1 {in_bounds = [true]} : tensor<2xf32>, vector<2xf32>
  %6 = vector.extractelement %cst[] : vector<f32>
  %7 = vector.multi_reduction <maxf>, %5, %6 [0] : vector<2xf32> to f32
  %8 = vector.broadcast %7 : f32 to vector<f32>
  %9 = vector.transfer_write %8, %4[] : vector<f32>, tensor<f32>
  %10 = vector.transfer_read %3[%c0], %cst_1 {in_bounds = [true]} : tensor<2xf32>, vector<2xf32>
  %extracted = tensor.extract %9[] : tensor<f32>
  %11 = vector.broadcast %extracted : f32 to vector<2xf32>
  %12 = vector.extractelement %cst_0[] : vector<f32>
  %13 = arith.subf %10, %11 : vector<2xf32>
  %14 = math.exp %13 : vector<2xf32>
  %15 = vector.multi_reduction <add>, %14, %12 [0] : vector<2xf32> to f32
  %16 = vector.broadcast %15 : f32 to vector<f32>
  %17 = vector.transfer_write %16, %4[] : vector<f32>, tensor<f32>
  %18 = vector.transfer_read %3[%c0], %cst_1 {in_bounds = [true]} : tensor<2xf32>, vector<2xf32>
  %extracted_2 = tensor.extract %9[] : tensor<f32>
  %19 = vector.broadcast %extracted_2 : f32 to vector<2xf32>
  %extracted_3 = tensor.extract %17[] : tensor<f32>
  %20 = vector.broadcast %extracted_3 : f32 to vector<2xf32>
  %21 = arith.subf %18, %19 : vector<2xf32>
  %22 = math.exp %21 : vector<2xf32>
  %23 = arith.divf %22, %20 : vector<2xf32>
  %24 = vector.transfer_write %23, %2[%c0] {in_bounds = [true]} : vector<2xf32>, tensor<2xf32>
  flow.dispatch.tensor.store %24, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %cst_2 = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %6 = vector.transfer_read %5[%c0], %cst_2 {in_bounds = [true]} : tensor<2xf32>, vector<2xf32>
    %7 = arith.mulf %6, %cst_1 : vector<2xf32>
    %8 = arith.addf %7, %cst_0 : vector<2xf32>
    %9 = math.roundeven %8 : vector<2xf32>
    %10 = arith.minf %9, %cst : vector<2xf32>
    %11 = arith.maxf %10, %cst_0 : vector<2xf32>
    %12 = arith.fptosi %11 : vector<2xf32> to vector<2xi8>
    %13 = vector.transfer_write %12, %4[%c0] {in_bounds = [true]} : vector<2xi8>, tensor<2xi8>
    flow.dispatch.tensor.store %13, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After CleanupBufferAllocView (iree-codegen-cleanup-buffer-alloc-view) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0_i8 = arith.constant 0 : i8
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<2xi8, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<2xf32, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = vector.transfer_read %0[%c0], %c0_i8 {in_bounds = [true]} : memref<2xi8, #hal.descriptor_type<storage_buffer>>, vector<2xi8>
    %5 = arith.sitofp %4 : vector<2xi8> to vector<2xf32>
    %6 = arith.subf %5, %cst_0 : vector<2xf32>
    %7 = arith.mulf %6, %cst : vector<2xf32>
    vector.transfer_write %7, %1[%c0] {in_bounds = [true]} : vector<2xf32>, memref<2xf32, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant dense<-1.000000e+30> : vector<f32>
  %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_1 = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
  %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
  %4 = tensor.empty() : tensor<f32>
  %5 = vector.transfer_read %3[%c0], %cst_1 {in_bounds = [true]} : tensor<2xf32>, vector<2xf32>
  %6 = vector.extractelement %cst[] : vector<f32>
  %7 = vector.multi_reduction <maxf>, %5, %6 [0] : vector<2xf32> to f32
  %8 = vector.broadcast %7 : f32 to vector<f32>
  %9 = vector.transfer_write %8, %4[] : vector<f32>, tensor<f32>
  %extracted = tensor.extract %9[] : tensor<f32>
  %10 = vector.broadcast %extracted : f32 to vector<2xf32>
  %11 = vector.extractelement %cst_0[] : vector<f32>
  %12 = arith.subf %5, %10 : vector<2xf32>
  %13 = math.exp %12 : vector<2xf32>
  %14 = vector.multi_reduction <add>, %13, %11 [0] : vector<2xf32> to f32
  %15 = vector.broadcast %14 : f32 to vector<f32>
  %16 = vector.transfer_write %15, %4[] : vector<f32>, tensor<f32>
  %extracted_2 = tensor.extract %16[] : tensor<f32>
  %17 = vector.broadcast %extracted_2 : f32 to vector<2xf32>
  %18 = arith.divf %13, %17 : vector<2xf32>
  %19 = vector.transfer_write %18, %2[%c0] {in_bounds = [true]} : vector<2xf32>, tensor<2xf32>
  flow.dispatch.tensor.store %19, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %cst_2 = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
    %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %6 = vector.transfer_read %5[%c0], %cst_2 {in_bounds = [true]} : tensor<2xf32>, vector<2xf32>
    %7 = arith.mulf %6, %cst_1 : vector<2xf32>
    %8 = arith.addf %7, %cst_0 : vector<2xf32>
    %9 = math.roundeven %8 : vector<2xf32>
    %10 = arith.minf %9, %cst : vector<2xf32>
    %11 = arith.maxf %10, %cst_0 : vector<2xf32>
    %12 = arith.fptosi %11 : vector<2xf32> to vector<2xi8>
    %13 = vector.transfer_write %12, %4[%c0] {in_bounds = [true]} : vector<2xi8>, tensor<2xi8>
    flow.dispatch.tensor.store %13, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
  }
  return
}

// -----// IR Dump After EraseHALDescriptorTypeFromMemRef (iree-codegen-erase-hal-descriptor-type-from-memref) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0_i8 = arith.constant 0 : i8
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
  memref.assume_alignment %0, 64 : memref<2xi8>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
  memref.assume_alignment %1, 64 : memref<2xf32>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = vector.transfer_read %0[%c0], %c0_i8 {in_bounds = [true]} : memref<2xi8>, vector<2xi8>
    %5 = arith.sitofp %4 : vector<2xi8> to vector<2xf32>
    %6 = arith.subf %5, %cst_0 : vector<2xf32>
    %7 = arith.mulf %6, %cst : vector<2xf32>
    vector.transfer_write %7, %1[%c0] {in_bounds = [true]} : vector<2xf32>, memref<2xf32>
  }
  return
}

// -----// IR Dump After EliminateEmptyTensors (iree-eliminate-empty-tensors) //----- //
module {
  func.func @main_dispatch_1_softmax_2xf32() {
    %cst = arith.constant dense<-1.000000e+30> : vector<f32>
    %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
    %cst_1 = arith.constant 0.000000e+00 : f32
    %c0 = arith.constant 0 : index
    %c64 = arith.constant 64 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %4 = tensor.empty() : tensor<f32>
    %5 = vector.transfer_read %3[%c0], %cst_1 {in_bounds = [true]} : tensor<2xf32>, vector<2xf32>
    %6 = vector.extractelement %cst[] : vector<f32>
    %7 = vector.multi_reduction <maxf>, %5, %6 [0] : vector<2xf32> to f32
    %8 = vector.broadcast %7 : f32 to vector<f32>
    %9 = vector.transfer_write %8, %4[] : vector<f32>, tensor<f32>
    %extracted = tensor.extract %9[] : tensor<f32>
    %10 = vector.broadcast %extracted : f32 to vector<2xf32>
    %11 = vector.extractelement %cst_0[] : vector<f32>
    %12 = arith.subf %5, %10 : vector<2xf32>
    %13 = math.exp %12 : vector<2xf32>
    %14 = vector.multi_reduction <add>, %13, %11 [0] : vector<2xf32> to f32
    %15 = vector.broadcast %14 : f32 to vector<f32>
    %16 = vector.transfer_write %15, %4[] : vector<f32>, tensor<f32>
    %extracted_2 = tensor.extract %16[] : tensor<f32>
    %17 = vector.broadcast %extracted_2 : f32 to vector<2xf32>
    %18 = arith.divf %13, %17 : vector<2xf32>
    %19 = vector.transfer_write %18, %2[%c0] {in_bounds = [true]} : vector<2xf32>, tensor<2xf32>
    flow.dispatch.tensor.store %19, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    return
  }
}

// -----// IR Dump After EliminateEmptyTensors (iree-eliminate-empty-tensors) //----- //
module {
  func.func @main_dispatch_2_generic_2_f32xi8() {
    %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
    %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
    %cst_2 = arith.constant 0.000000e+00 : f32
    %c2 = arith.constant 2 : index
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
    %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
    scf.for %arg0 = %2 to %c2 step %3 {
      %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
      %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
      %6 = vector.transfer_read %5[%c0], %cst_2 {in_bounds = [true]} : tensor<2xf32>, vector<2xf32>
      %7 = arith.mulf %6, %cst_1 : vector<2xf32>
      %8 = arith.addf %7, %cst_0 : vector<2xf32>
      %9 = math.roundeven %8 : vector<2xf32>
      %10 = arith.minf %9, %cst : vector<2xf32>
      %11 = arith.maxf %10, %cst_0 : vector<2xf32>
      %12 = arith.fptosi %11 : vector<2xf32> to vector<2xi8>
      %13 = vector.transfer_write %12, %4[%c0] {in_bounds = [true]} : vector<2xi8>, tensor<2xi8>
      flow.dispatch.tensor.store %13, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
    }
    return
  }
}

// -----// IR Dump After RemoveSingleIterationLoop (iree-codegen-remove-single-iteration-loop) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0_i8 = arith.constant 0 : i8
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
  memref.assume_alignment %0, 64 : memref<2xi8>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
  memref.assume_alignment %1, 64 : memref<2xf32>
  %2 = vector.transfer_read %0[%c0], %c0_i8 {in_bounds = [true]} : memref<2xi8>, vector<2xi8>
  %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
  %4 = arith.subf %3, %cst_0 : vector<2xf32>
  %5 = arith.mulf %4, %cst : vector<2xf32>
  vector.transfer_write %5, %1[%c0] {in_bounds = [true]} : vector<2xf32>, memref<2xf32>
  return
}

// -----// IR Dump After EmptyTensorToAllocTensor (empty-tensor-to-alloc-tensor) //----- //
module {
  func.func @main_dispatch_1_softmax_2xf32() {
    %cst = arith.constant dense<-1.000000e+30> : vector<f32>
    %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
    %cst_1 = arith.constant 0.000000e+00 : f32
    %c0 = arith.constant 0 : index
    %c64 = arith.constant 64 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    %2 = flow.dispatch.tensor.load %1, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xf32>> -> tensor<2xf32>
    %3 = flow.dispatch.tensor.load %0, offsets = [0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
    %4 = bufferization.alloc_tensor() : tensor<f32>
    %5 = vector.transfer_read %3[%c0], %cst_1 {in_bounds = [true]} : tensor<2xf32>, vector<2xf32>
    %6 = vector.extractelement %cst[] : vector<f32>
    %7 = vector.multi_reduction <maxf>, %5, %6 [0] : vector<2xf32> to f32
    %8 = vector.broadcast %7 : f32 to vector<f32>
    %9 = vector.transfer_write %8, %4[] : vector<f32>, tensor<f32>
    %extracted = tensor.extract %9[] : tensor<f32>
    %10 = vector.broadcast %extracted : f32 to vector<2xf32>
    %11 = vector.extractelement %cst_0[] : vector<f32>
    %12 = arith.subf %5, %10 : vector<2xf32>
    %13 = math.exp %12 : vector<2xf32>
    %14 = vector.multi_reduction <add>, %13, %11 [0] : vector<2xf32> to f32
    %15 = vector.broadcast %14 : f32 to vector<f32>
    %16 = vector.transfer_write %15, %4[] : vector<f32>, tensor<f32>
    %extracted_2 = tensor.extract %16[] : tensor<f32>
    %17 = vector.broadcast %extracted_2 : f32 to vector<2xf32>
    %18 = arith.divf %13, %17 : vector<2xf32>
    %19 = vector.transfer_write %18, %2[%c0] {in_bounds = [true]} : vector<2xf32>, tensor<2xf32>
    flow.dispatch.tensor.store %19, %1, offsets = [0], sizes = [2], strides = [1] : tensor<2xf32> -> !flow.dispatch.tensor<writeonly:tensor<2xf32>>
    return
  }
}

// -----// IR Dump After EmptyTensorToAllocTensor (empty-tensor-to-alloc-tensor) //----- //
module {
  func.func @main_dispatch_2_generic_2_f32xi8() {
    %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
    %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
    %cst_2 = arith.constant 0.000000e+00 : f32
    %c2 = arith.constant 2 : index
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : !flow.dispatch.tensor<readonly:tensor<2xf32>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : !flow.dispatch.tensor<writeonly:tensor<2xi8>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
    %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
    scf.for %arg0 = %2 to %c2 step %3 {
      %4 = flow.dispatch.tensor.load %1, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<writeonly:tensor<2xi8>> -> tensor<2xi8>
      %5 = flow.dispatch.tensor.load %0, offsets = [%arg0], sizes = [2], strides = [1] : !flow.dispatch.tensor<readonly:tensor<2xf32>> -> tensor<2xf32>
      %6 = vector.transfer_read %5[%c0], %cst_2 {in_bounds = [true]} : tensor<2xf32>, vector<2xf32>
      %7 = arith.mulf %6, %cst_1 : vector<2xf32>
      %8 = arith.addf %7, %cst_0 : vector<2xf32>
      %9 = math.roundeven %8 : vector<2xf32>
      %10 = arith.minf %9, %cst : vector<2xf32>
      %11 = arith.maxf %10, %cst_0 : vector<2xf32>
      %12 = arith.fptosi %11 : vector<2xf32> to vector<2xi8>
      %13 = vector.transfer_write %12, %4[%c0] {in_bounds = [true]} : vector<2xi8>, tensor<2xi8>
      flow.dispatch.tensor.store %13, %1, offsets = [%arg0], sizes = [2], strides = [1] : tensor<2xi8> -> !flow.dispatch.tensor<writeonly:tensor<2xi8>>
    }
    return
  }
}

// -----// IR Dump After LLVMCPUVectorLowering (iree-llvmcpu-vector-lowering) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
  memref.assume_alignment %0, 64 : memref<2xi8>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
  memref.assume_alignment %1, 64 : memref<2xf32>
  %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
  %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
  %4 = arith.subf %3, %cst_0 : vector<2xf32>
  %5 = arith.mulf %4, %cst : vector<2xf32>
  vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
  return
}

// -----// IR Dump After IREEComprehensiveBufferize (iree-codegen-iree-comprehensive-bufferize) //----- //
module {
  func.func @main_dispatch_1_softmax_2xf32() {
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
    %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
    %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<2xf32, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
    %2 = vector.transfer_read %0[%c0], %cst {in_bounds = [true]} : memref<2xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
    %3 = vector.extractelement %cst_1[] : vector<f32>
    %4 = vector.multi_reduction <maxf>, %2, %3 [0] : vector<2xf32> to f32
    %5 = vector.broadcast %4 : f32 to vector<f32>
    vector.transfer_write %5, %alloca[] : vector<f32>, memref<f32>
    %6 = memref.load %alloca[] : memref<f32>
    %7 = vector.broadcast %6 : f32 to vector<2xf32>
    %8 = vector.extractelement %cst_0[] : vector<f32>
    %9 = arith.subf %2, %7 : vector<2xf32>
    %10 = math.exp %9 : vector<2xf32>
    %11 = vector.multi_reduction <add>, %10, %8 [0] : vector<2xf32> to f32
    %12 = vector.broadcast %11 : f32 to vector<f32>
    vector.transfer_write %12, %alloca[] : vector<f32>, memref<f32>
    %13 = memref.load %alloca[] : memref<f32>
    %14 = vector.broadcast %13 : f32 to vector<2xf32>
    %15 = arith.divf %10, %14 : vector<2xf32>
    vector.transfer_write %15, %1[%c0] {in_bounds = [true]} : vector<2xf32>, memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
    linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%1 : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>) outs(%1 : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    return
  }
}

// -----// IR Dump After IREEComprehensiveBufferize (iree-codegen-iree-comprehensive-bufferize) //----- //
module {
  func.func @main_dispatch_2_generic_2_f32xi8() {
    %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
    %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
    %cst_2 = arith.constant 0.000000e+00 : f32
    %c2 = arith.constant 2 : index
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<2xi8, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
    %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
    scf.for %arg0 = %2 to %c2 step %3 {
      %4 = vector.transfer_read %0[%c0], %cst_2 {in_bounds = [true]} : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
      %5 = arith.mulf %4, %cst_1 : vector<2xf32>
      %6 = arith.addf %5, %cst_0 : vector<2xf32>
      %7 = math.roundeven %6 : vector<2xf32>
      %8 = arith.minf %7, %cst : vector<2xf32>
      %9 = arith.maxf %8, %cst_0 : vector<2xf32>
      %10 = arith.fptosi %9 : vector<2xf32> to vector<2xi8>
      vector.transfer_write %10, %1[%c0] {in_bounds = [true]} : vector<2xi8>, memref<2xi8, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%1 : memref<2xi8, #hal.descriptor_type<storage_buffer>>) outs(%1 : memref<2xi8, #hal.descriptor_type<storage_buffer>>) {
      ^bb0(%in: i8, %out: i8):
        linalg.yield %in : i8
      }
    }
    return
  }
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
module {
  func.func @main_dispatch_1_softmax_2xf32() {
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
    %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
    %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<2xf32, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
    %2 = vector.transfer_read %0[%c0], %cst {in_bounds = [true]} : memref<2xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
    %3 = vector.extractelement %cst_1[] : vector<f32>
    %4 = vector.multi_reduction <maxf>, %2, %3 [0] : vector<2xf32> to f32
    %5 = vector.broadcast %4 : f32 to vector<f32>
    vector.transfer_write %5, %alloca[] : vector<f32>, memref<f32>
    %6 = memref.load %alloca[] : memref<f32>
    %7 = vector.broadcast %6 : f32 to vector<2xf32>
    %8 = vector.extractelement %cst_0[] : vector<f32>
    %9 = arith.subf %2, %7 : vector<2xf32>
    %10 = math.exp %9 : vector<2xf32>
    %11 = vector.multi_reduction <add>, %10, %8 [0] : vector<2xf32> to f32
    %12 = vector.broadcast %11 : f32 to vector<f32>
    vector.transfer_write %12, %alloca[] : vector<f32>, memref<f32>
    %13 = memref.load %alloca[] : memref<f32>
    %14 = vector.broadcast %13 : f32 to vector<2xf32>
    %15 = arith.divf %10, %14 : vector<2xf32>
    vector.transfer_write %15, %1[%c0] {in_bounds = [true]} : vector<2xf32>, memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
    linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%1 : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>) outs(%1 : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    }
    return
  }
}

// -----// IR Dump After LLVMCPULowerExecutableTarget (iree-llvmcpu-lower-executable-target) //----- //
hal.executable.variant public @static, target = <"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}> {
  hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert>} {
  ^bb0(%arg0: !hal.device):
    %c1 = arith.constant 1 : index
    hal.return %c1, %c1, %c1 : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_0_generic_2_i8xf32() {
      %cst = arith.constant dense<0.0125187514> : vector<2xf32>
      %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
      %c0 = arith.constant 0 : index
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
      memref.assume_alignment %0, 64 : memref<2xi8>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
      memref.assume_alignment %1, 64 : memref<2xf32>
      %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
      %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
      %4 = arith.subf %3, %cst_0 : vector<2xf32>
      %5 = arith.mulf %4, %cst : vector<2xf32>
      vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
      return
    }
  }
}

// -----// IR Dump After ResolveShapedTypeResultDims (resolve-shaped-type-result-dims) //----- //
module {
  func.func @main_dispatch_2_generic_2_f32xi8() {
    %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
    %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
    %cst_2 = arith.constant 0.000000e+00 : f32
    %c2 = arith.constant 2 : index
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8, #hal.descriptor_type<storage_buffer>>
    memref.assume_alignment %1, 64 : memref<2xi8, #hal.descriptor_type<storage_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
    %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
    scf.for %arg0 = %2 to %c2 step %3 {
      %4 = vector.transfer_read %0[%c0], %cst_2 {in_bounds = [true]} : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
      %5 = arith.mulf %4, %cst_1 : vector<2xf32>
      %6 = arith.addf %5, %cst_0 : vector<2xf32>
      %7 = math.roundeven %6 : vector<2xf32>
      %8 = arith.minf %7, %cst : vector<2xf32>
      %9 = arith.maxf %8, %cst_0 : vector<2xf32>
      %10 = arith.fptosi %9 : vector<2xf32> to vector<2xi8>
      vector.transfer_write %10, %1[%c0] {in_bounds = [true]} : vector<2xi8>, memref<2xi8, #hal.descriptor_type<storage_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0) -> (d0)>, affine_map<(d0) -> (d0)>], iterator_types = ["parallel"]} ins(%1 : memref<2xi8, #hal.descriptor_type<storage_buffer>>) outs(%1 : memref<2xi8, #hal.descriptor_type<storage_buffer>>) {
      ^bb0(%in: i8, %out: i8):
        linalg.yield %in : i8
      }
    }
    return
  }
}

// -----// IR Dump After LowerUKernelOpsToCalls (iree-codegen-lower-ukernel-ops-to-calls) //----- //
module {
  func.func @main_dispatch_0_generic_2_i8xf32() {
    %cst = arith.constant dense<0.0125187514> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
    memref.assume_alignment %0, 64 : memref<2xi8>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
    memref.assume_alignment %1, 64 : memref<2xf32>
    %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
    %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
    %4 = arith.subf %3, %cst_0 : vector<2xf32>
    %5 = arith.mulf %4, %cst : vector<2xf32>
    vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
    return
  }
}

// -----// IR Dump After LinalgExtToLoops (iree-linalg-ext-to-loops) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
  memref.assume_alignment %0, 64 : memref<2xi8>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
  memref.assume_alignment %1, 64 : memref<2xf32>
  %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
  %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
  %4 = arith.subf %3, %cst_0 : vector<2xf32>
  %5 = arith.mulf %4, %cst : vector<2xf32>
  vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<2xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  %2 = vector.transfer_read %0[%c0], %cst {in_bounds = [true]} : memref<2xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  %3 = vector.extractelement %cst_1[] : vector<f32>
  %4 = vector.multi_reduction <maxf>, %2, %3 [0] : vector<2xf32> to f32
  %5 = vector.broadcast %4 : f32 to vector<f32>
  vector.transfer_write %5, %alloca[] : vector<f32>, memref<f32>
  %6 = memref.load %alloca[] : memref<f32>
  %7 = vector.broadcast %6 : f32 to vector<2xf32>
  %8 = vector.extractelement %cst_0[] : vector<f32>
  %9 = arith.subf %2, %7 : vector<2xf32>
  %10 = math.exp %9 : vector<2xf32>
  %11 = vector.multi_reduction <add>, %10, %8 [0] : vector<2xf32> to f32
  %12 = vector.broadcast %11 : f32 to vector<f32>
  vector.transfer_write %12, %alloca[] : vector<f32>, memref<f32>
  %13 = memref.load %alloca[] : memref<f32>
  %14 = vector.broadcast %13 : f32 to vector<2xf32>
  %15 = arith.divf %10, %14 : vector<2xf32>
  vector.transfer_write %15, %1[%c0] {in_bounds = [true]} : vector<2xf32>, memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<2xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  %2 = vector.transfer_read %0[%c0], %cst {in_bounds = [true]} : memref<2xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  %3 = vector.extractelement %cst_1[] : vector<f32>
  %4 = vector.multi_reduction <maxf>, %2, %3 [0] : vector<2xf32> to f32
  %5 = vector.broadcast %4 : f32 to vector<f32>
  vector.transfer_write %5, %alloca[] : vector<f32>, memref<f32>
  %6 = memref.load %alloca[] : memref<f32>
  %7 = vector.broadcast %6 : f32 to vector<2xf32>
  %8 = vector.extractelement %cst_0[] : vector<f32>
  %9 = arith.subf %2, %7 : vector<2xf32>
  %10 = math.exp %9 : vector<2xf32>
  %11 = vector.multi_reduction <add>, %10, %8 [0] : vector<2xf32> to f32
  %12 = vector.broadcast %11 : f32 to vector<f32>
  vector.transfer_write %12, %alloca[] : vector<f32>, memref<f32>
  %13 = memref.load %alloca[] : memref<f32>
  %14 = vector.broadcast %13 : f32 to vector<2xf32>
  %15 = arith.divf %10, %14 : vector<2xf32>
  vector.transfer_write %15, %1[%c0] {in_bounds = [true]} : vector<2xf32>, memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After MemrefCopyToLinalgPass (iree-codegen-memrefcopy-to-linalg) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
  memref.assume_alignment %0, 64 : memref<2xi8>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
  memref.assume_alignment %1, 64 : memref<2xf32>
  %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
  %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
  %4 = arith.subf %3, %cst_0 : vector<2xf32>
  %5 = arith.mulf %4, %cst : vector<2xf32>
  vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %cst_2 = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<2xi8, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = vector.transfer_read %0[%c0], %cst_2 {in_bounds = [true]} : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
    %5 = arith.mulf %4, %cst_1 : vector<2xf32>
    %6 = arith.addf %5, %cst_0 : vector<2xf32>
    %7 = math.roundeven %6 : vector<2xf32>
    %8 = arith.minf %7, %cst : vector<2xf32>
    %9 = arith.maxf %8, %cst_0 : vector<2xf32>
    %10 = arith.fptosi %9 : vector<2xf32> to vector<2xi8>
    vector.transfer_write %10, %1[%c0] {in_bounds = [true]} : vector<2xi8>, memref<2xi8, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %cst_2 = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<2xi8, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = vector.transfer_read %0[%c0], %cst_2 {in_bounds = [true]} : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
    %5 = arith.mulf %4, %cst_1 : vector<2xf32>
    %6 = arith.addf %5, %cst_0 : vector<2xf32>
    %7 = math.roundeven %6 : vector<2xf32>
    %8 = arith.minf %7, %cst : vector<2xf32>
    %9 = arith.maxf %8, %cst_0 : vector<2xf32>
    %10 = arith.fptosi %9 : vector<2xf32> to vector<2xi8>
    vector.transfer_write %10, %1[%c0] {in_bounds = [true]} : vector<2xi8>, memref<2xi8, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<2xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  %2 = vector.transfer_read %0[%c0], %cst {in_bounds = [true]} : memref<2xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  %3 = vector.extractelement %cst_1[] : vector<f32>
  %4 = vector.multi_reduction <maxf>, %2, %3 [0] : vector<2xf32> to f32
  %5 = vector.broadcast %4 : f32 to vector<f32>
  vector.transfer_write %5, %alloca[] : vector<f32>, memref<f32>
  %6 = memref.load %alloca[] : memref<f32>
  %7 = vector.broadcast %6 : f32 to vector<2xf32>
  %8 = vector.extractelement %cst_0[] : vector<f32>
  %9 = arith.subf %2, %7 : vector<2xf32>
  %10 = math.exp %9 : vector<2xf32>
  %11 = vector.multi_reduction <add>, %10, %8 [0] : vector<2xf32> to f32
  %12 = vector.broadcast %11 : f32 to vector<f32>
  vector.transfer_write %12, %alloca[] : vector<f32>, memref<f32>
  %13 = memref.load %alloca[] : memref<f32>
  %14 = vector.broadcast %13 : f32 to vector<2xf32>
  %15 = arith.divf %10, %14 : vector<2xf32>
  vector.transfer_write %15, %1[%c0] {in_bounds = [true]} : vector<2xf32>, memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
  memref.assume_alignment %0, 64 : memref<2xi8>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
  memref.assume_alignment %1, 64 : memref<2xf32>
  %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
  %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
  %4 = arith.subf %3, %cst_0 : vector<2xf32>
  %5 = arith.mulf %4, %cst : vector<2xf32>
  vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %cst_2 = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<2xi8, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = vector.transfer_read %0[%c0], %cst_2 {in_bounds = [true]} : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
    %5 = arith.mulf %4, %cst_1 : vector<2xf32>
    %6 = arith.addf %5, %cst_0 : vector<2xf32>
    %7 = math.roundeven %6 : vector<2xf32>
    %8 = arith.minf %7, %cst : vector<2xf32>
    %9 = arith.maxf %8, %cst_0 : vector<2xf32>
    %10 = arith.fptosi %9 : vector<2xf32> to vector<2xi8>
    vector.transfer_write %10, %1[%c0] {in_bounds = [true]} : vector<2xi8>, memref<2xi8, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After CleanupBufferAllocView (iree-codegen-cleanup-buffer-alloc-view) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<2xf32, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  %2 = vector.transfer_read %0[%c0], %cst {in_bounds = [true]} : memref<2xf32, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
  %3 = vector.extractelement %cst_1[] : vector<f32>
  %4 = vector.multi_reduction <maxf>, %2, %3 [0] : vector<2xf32> to f32
  %5 = vector.broadcast %4 : f32 to vector<f32>
  vector.transfer_write %5, %alloca[] : vector<f32>, memref<f32>
  %6 = memref.load %alloca[] : memref<f32>
  %7 = vector.broadcast %6 : f32 to vector<2xf32>
  %8 = vector.extractelement %cst_0[] : vector<f32>
  %9 = arith.subf %2, %7 : vector<2xf32>
  %10 = math.exp %9 : vector<2xf32>
  %11 = vector.multi_reduction <add>, %10, %8 [0] : vector<2xf32> to f32
  %12 = vector.broadcast %11 : f32 to vector<f32>
  vector.transfer_write %12, %alloca[] : vector<f32>, memref<f32>
  %13 = memref.load %alloca[] : memref<f32>
  %14 = vector.broadcast %13 : f32 to vector<2xf32>
  %15 = arith.divf %10, %14 : vector<2xf32>
  vector.transfer_write %15, %1[%c0] {in_bounds = [true]} : vector<2xf32>, memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  return
}

// -----// IR Dump After ConvertBf16ArithToF32 (iree-convert-bf16-arith-to-f32) //----- //
module {
  func.func @main_dispatch_0_generic_2_i8xf32() {
    %cst = arith.constant dense<0.0125187514> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
    memref.assume_alignment %0, 64 : memref<2xi8>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
    memref.assume_alignment %1, 64 : memref<2xf32>
    %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
    %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
    %4 = arith.subf %3, %cst_0 : vector<2xf32>
    %5 = arith.mulf %4, %cst : vector<2xf32>
    vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
    return
  }
}

// -----// IR Dump After CleanupBufferAllocView (iree-codegen-cleanup-buffer-alloc-view) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %cst_2 = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8, #hal.descriptor_type<storage_buffer>>
  memref.assume_alignment %1, 64 : memref<2xi8, #hal.descriptor_type<storage_buffer>>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = vector.transfer_read %0[%c0], %cst_2 {in_bounds = [true]} : memref<2xf32, strided<[1], offset: 16>, #hal.descriptor_type<storage_buffer>>, vector<2xf32>
    %5 = arith.mulf %4, %cst_1 : vector<2xf32>
    %6 = arith.addf %5, %cst_0 : vector<2xf32>
    %7 = math.roundeven %6 : vector<2xf32>
    %8 = arith.minf %7, %cst : vector<2xf32>
    %9 = arith.maxf %8, %cst_0 : vector<2xf32>
    %10 = arith.fptosi %9 : vector<2xf32> to vector<2xi8>
    vector.transfer_write %10, %1[%c0] {in_bounds = [true]} : vector<2xi8>, memref<2xi8, #hal.descriptor_type<storage_buffer>>
  }
  return
}

// -----// IR Dump After EraseHALDescriptorTypeFromMemRef (iree-codegen-erase-hal-descriptor-type-from-memref) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
  memref.assume_alignment %0, 64 : memref<2xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
  %2 = vector.transfer_read %0[%c0], %cst {in_bounds = [true]} : memref<2xf32>, vector<2xf32>
  %3 = vector.extractelement %cst_1[] : vector<f32>
  %4 = vector.multi_reduction <maxf>, %2, %3 [0] : vector<2xf32> to f32
  %5 = vector.broadcast %4 : f32 to vector<f32>
  vector.transfer_write %5, %alloca[] : vector<f32>, memref<f32>
  %6 = memref.load %alloca[] : memref<f32>
  %7 = vector.broadcast %6 : f32 to vector<2xf32>
  %8 = vector.extractelement %cst_0[] : vector<f32>
  %9 = arith.subf %2, %7 : vector<2xf32>
  %10 = math.exp %9 : vector<2xf32>
  %11 = vector.multi_reduction <add>, %10, %8 [0] : vector<2xf32> to f32
  %12 = vector.broadcast %11 : f32 to vector<f32>
  vector.transfer_write %12, %alloca[] : vector<f32>, memref<f32>
  %13 = memref.load %alloca[] : memref<f32>
  %14 = vector.broadcast %13 : f32 to vector<2xf32>
  %15 = arith.divf %10, %14 : vector<2xf32>
  vector.transfer_write %15, %1[%c0] {in_bounds = [true]} : vector<2xf32>, memref<2xf32, strided<[1], offset: 16>>
  return
}

// -----// IR Dump After ConvertBf16ToUInt16Buffers (iree-convert-bf16-to-uint16-buffers) //----- //
module {
  func.func @main_dispatch_0_generic_2_i8xf32() {
    %cst = arith.constant dense<0.0125187514> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
    memref.assume_alignment %0, 64 : memref<2xi8>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
    memref.assume_alignment %1, 64 : memref<2xf32>
    %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
    %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
    %4 = arith.subf %3, %cst_0 : vector<2xf32>
    %5 = arith.mulf %4, %cst : vector<2xf32>
    vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
    return
  }
}

// -----// IR Dump After EraseHALDescriptorTypeFromMemRef (iree-codegen-erase-hal-descriptor-type-from-memref) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %cst_2 = arith.constant 0.000000e+00 : f32
  %c2 = arith.constant 2 : index
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
  memref.assume_alignment %1, 64 : memref<2xi8>
  %workgroup_id_x = hal.interface.workgroup.id[0] : index
  %workgroup_count_x = hal.interface.workgroup.count[0] : index
  %2 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_id_x]
  %3 = affine.apply affine_map<()[s0] -> (s0 * 2)>()[%workgroup_count_x]
  scf.for %arg0 = %2 to %c2 step %3 {
    %4 = vector.transfer_read %0[%c0], %cst_2 {in_bounds = [true]} : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
    %5 = arith.mulf %4, %cst_1 : vector<2xf32>
    %6 = arith.addf %5, %cst_0 : vector<2xf32>
    %7 = math.roundeven %6 : vector<2xf32>
    %8 = arith.minf %7, %cst : vector<2xf32>
    %9 = arith.maxf %8, %cst_0 : vector<2xf32>
    %10 = arith.fptosi %9 : vector<2xf32> to vector<2xi8>
    vector.transfer_write %10, %1[%c0] {in_bounds = [true]} : vector<2xi8>, memref<2xi8>
  }
  return
}

// -----// IR Dump After RemoveSingleIterationLoop (iree-codegen-remove-single-iteration-loop) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
  memref.assume_alignment %0, 64 : memref<2xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
  %2 = vector.transfer_read %0[%c0], %cst {in_bounds = [true]} : memref<2xf32>, vector<2xf32>
  %3 = vector.extractelement %cst_1[] : vector<f32>
  %4 = vector.multi_reduction <maxf>, %2, %3 [0] : vector<2xf32> to f32
  %5 = vector.broadcast %4 : f32 to vector<f32>
  vector.transfer_write %5, %alloca[] : vector<f32>, memref<f32>
  %6 = memref.load %alloca[] : memref<f32>
  %7 = vector.broadcast %6 : f32 to vector<2xf32>
  %8 = vector.extractelement %cst_0[] : vector<f32>
  %9 = arith.subf %2, %7 : vector<2xf32>
  %10 = math.exp %9 : vector<2xf32>
  %11 = vector.multi_reduction <add>, %10, %8 [0] : vector<2xf32> to f32
  %12 = vector.broadcast %11 : f32 to vector<f32>
  vector.transfer_write %12, %alloca[] : vector<f32>, memref<f32>
  %13 = memref.load %alloca[] : memref<f32>
  %14 = vector.broadcast %13 : f32 to vector<2xf32>
  %15 = arith.divf %10, %14 : vector<2xf32>
  vector.transfer_write %15, %1[%c0] {in_bounds = [true]} : vector<2xf32>, memref<2xf32, strided<[1], offset: 16>>
  return
}

// -----// IR Dump After RemoveSingleIterationLoop (iree-codegen-remove-single-iteration-loop) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %cst_2 = arith.constant 0.000000e+00 : f32
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
  memref.assume_alignment %1, 64 : memref<2xi8>
  %2 = vector.transfer_read %0[%c0], %cst_2 {in_bounds = [true]} : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  %3 = arith.mulf %2, %cst_1 : vector<2xf32>
  %4 = arith.addf %3, %cst_0 : vector<2xf32>
  %5 = math.roundeven %4 : vector<2xf32>
  %6 = arith.minf %5, %cst : vector<2xf32>
  %7 = arith.maxf %6, %cst_0 : vector<2xf32>
  %8 = arith.fptosi %7 : vector<2xf32> to vector<2xi8>
  vector.transfer_write %8, %1[%c0] {in_bounds = [true]} : vector<2xi8>, memref<2xi8>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
  memref.assume_alignment %0, 64 : memref<2xi8>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
  memref.assume_alignment %1, 64 : memref<2xf32>
  %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
  %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
  %4 = arith.subf %3, %cst_0 : vector<2xf32>
  %5 = arith.mulf %4, %cst : vector<2xf32>
  vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
  memref.assume_alignment %0, 64 : memref<2xi8>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
  memref.assume_alignment %1, 64 : memref<2xf32>
  %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
  %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
  %4 = arith.subf %3, %cst_0 : vector<2xf32>
  %5 = arith.mulf %4, %cst : vector<2xf32>
  vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
  return
}

// -----// IR Dump After ArithBufferize (arith-bufferize) //----- //
module {
  func.func @main_dispatch_0_generic_2_i8xf32() {
    %cst = arith.constant dense<0.0125187514> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
    memref.assume_alignment %0, 64 : memref<2xi8>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
    memref.assume_alignment %1, 64 : memref<2xf32>
    %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
    %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
    %4 = arith.subf %3, %cst_0 : vector<2xf32>
    %5 = arith.mulf %4, %cst : vector<2xf32>
    vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
    return
  }
}

// -----// IR Dump After LLVMCPUVectorLowering (iree-llvmcpu-vector-lowering) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant dense<0.000000e+00> : vector<1xf32>
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
  memref.assume_alignment %0, 64 : memref<2xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
  %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
  %3 = vector.extractelement %cst_1[] : vector<f32>
  %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
  %5 = vector.insertelement %4, %cst[%c0 : index] : vector<1xf32>
  %6 = vector.extract %5[0] : vector<1xf32>
  %7 = vector.broadcast %6 : f32 to vector<f32>
  %8 = vector.extractelement %7[] : vector<f32>
  memref.store %8, %alloca[] : memref<f32>
  %9 = memref.load %alloca[] : memref<f32>
  %10 = vector.broadcast %9 : f32 to vector<2xf32>
  %11 = vector.extractelement %cst_0[] : vector<f32>
  %12 = arith.subf %2, %10 : vector<2xf32>
  %13 = math.exp %12 : vector<2xf32>
  %14 = vector.reduction <add>, %13, %11 : vector<2xf32> into f32
  %15 = vector.insertelement %14, %cst[%c0 : index] : vector<1xf32>
  %16 = vector.extract %15[0] : vector<1xf32>
  %17 = vector.broadcast %16 : f32 to vector<f32>
  %18 = vector.extractelement %17[] : vector<f32>
  memref.store %18, %alloca[] : memref<f32>
  %19 = memref.load %alloca[] : memref<f32>
  %20 = vector.broadcast %19 : f32 to vector<2xf32>
  %21 = arith.divf %13, %20 : vector<2xf32>
  vector.store %21, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  return
}

// -----// IR Dump After LLVMCPUVectorLowering (iree-llvmcpu-vector-lowering) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
  memref.assume_alignment %1, 64 : memref<2xi8>
  %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  %3 = arith.mulf %2, %cst_1 : vector<2xf32>
  %4 = arith.addf %3, %cst_0 : vector<2xf32>
  %5 = math.roundeven %4 : vector<2xf32>
  %6 = arith.minf %5, %cst : vector<2xf32>
  %7 = arith.maxf %6, %cst_0 : vector<2xf32>
  %8 = arith.fptosi %7 : vector<2xf32> to vector<2xi8>
  vector.store %8, %1[%c0] : memref<2xi8>, vector<2xi8>
  return
}

// -----// IR Dump After FoldTensorExtractOp (iree-codegen-fold-tensor-extract-op) //----- //
module {
  func.func @main_dispatch_0_generic_2_i8xf32() {
    %cst = arith.constant dense<0.0125187514> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
    memref.assume_alignment %0, 64 : memref<2xi8>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
    memref.assume_alignment %1, 64 : memref<2xf32>
    %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
    %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
    %4 = arith.subf %3, %cst_0 : vector<2xf32>
    %5 = arith.mulf %4, %cst : vector<2xf32>
    vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
    return
  }
}

// -----// IR Dump After ConvertComplexToStandard (convert-complex-to-standard) //----- //
module {
  func.func @main_dispatch_0_generic_2_i8xf32() {
    %cst = arith.constant dense<0.0125187514> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
    memref.assume_alignment %0, 64 : memref<2xi8>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
    memref.assume_alignment %1, 64 : memref<2xf32>
    %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
    %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
    %4 = arith.subf %3, %cst_0 : vector<2xf32>
    %5 = arith.mulf %4, %cst : vector<2xf32>
    vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
    return
  }
}

// -----// IR Dump After PolynomialApproximationPass (iree-codegen-polynomial-approximation) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
  memref.assume_alignment %0, 64 : memref<2xi8>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
  memref.assume_alignment %1, 64 : memref<2xf32>
  %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
  %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
  %4 = arith.subf %3, %cst_0 : vector<2xf32>
  %5 = arith.mulf %4, %cst : vector<2xf32>
  vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
  return
}

// -----// IR Dump After HoistStaticallyBoundAllocations (iree-hoist-statically-bound-allocations) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
  memref.assume_alignment %0, 64 : memref<2xi8>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
  memref.assume_alignment %1, 64 : memref<2xf32>
  %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
  %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
  %4 = arith.subf %3, %cst_0 : vector<2xf32>
  %5 = arith.mulf %4, %cst : vector<2xf32>
  vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
  return
}

// -----// IR Dump After LLVMCPULowerExecutableTarget (iree-llvmcpu-lower-executable-target) //----- //
hal.executable.variant public @static, target = <"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}> {
  hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert>} {
  ^bb0(%arg0: !hal.device):
    %c1 = arith.constant 1 : index
    hal.return %c1, %c1, %c1 : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_1_softmax_2xf32() {
      %cst = arith.constant dense<0.000000e+00> : vector<1xf32>
      %c0 = arith.constant 0 : index
      %c64 = arith.constant 64 : index
      %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
      %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
      %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
      memref.assume_alignment %0, 64 : memref<2xf32>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
      memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
      %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
      %3 = vector.extractelement %cst_1[] : vector<f32>
      %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
      %5 = vector.insertelement %4, %cst[%c0 : index] : vector<1xf32>
      %6 = vector.extract %5[0] : vector<1xf32>
      %7 = vector.broadcast %6 : f32 to vector<f32>
      %8 = vector.extractelement %7[] : vector<f32>
      memref.store %8, %alloca[] : memref<f32>
      %9 = memref.load %alloca[] : memref<f32>
      %10 = vector.broadcast %9 : f32 to vector<2xf32>
      %11 = vector.extractelement %cst_0[] : vector<f32>
      %12 = arith.subf %2, %10 : vector<2xf32>
      %13 = math.exp %12 : vector<2xf32>
      %14 = vector.reduction <add>, %13, %11 : vector<2xf32> into f32
      %15 = vector.insertelement %14, %cst[%c0 : index] : vector<1xf32>
      %16 = vector.extract %15[0] : vector<1xf32>
      %17 = vector.broadcast %16 : f32 to vector<f32>
      %18 = vector.extractelement %17[] : vector<f32>
      memref.store %18, %alloca[] : memref<f32>
      %19 = memref.load %alloca[] : memref<f32>
      %20 = vector.broadcast %19 : f32 to vector<2xf32>
      %21 = arith.divf %13, %20 : vector<2xf32>
      vector.store %21, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
      return
    }
  }
}

// -----// IR Dump After LLVMCPULowerExecutableTarget (iree-llvmcpu-lower-executable-target) //----- //
hal.executable.variant public @static, target = <"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}> {
  hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert>} {
  ^bb0(%arg0: !hal.device):
    %c1 = arith.constant 1 : index
    hal.return %c1, %c1, %c1 : index, index, index
  }
  builtin.module {
    func.func @main_dispatch_2_generic_2_f32xi8() {
      %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
      %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
      %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
      %c64 = arith.constant 64 : index
      %c0 = arith.constant 0 : index
      %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
      memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
      %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
      memref.assume_alignment %1, 64 : memref<2xi8>
      %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
      %3 = arith.mulf %2, %cst_1 : vector<2xf32>
      %4 = arith.addf %3, %cst_0 : vector<2xf32>
      %5 = math.roundeven %4 : vector<2xf32>
      %6 = arith.minf %5, %cst : vector<2xf32>
      %7 = arith.maxf %6, %cst_0 : vector<2xf32>
      %8 = arith.fptosi %7 : vector<2xf32> to vector<2xi8>
      vector.store %8, %1[%c0] : memref<2xi8>, vector<2xi8>
      return
    }
  }
}

// -----// IR Dump After IREEExpandStridedMetadata (iree-codegen-expand-strided-metadata) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
  memref.assume_alignment %0, 64 : memref<2xi8>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
  memref.assume_alignment %1, 64 : memref<2xf32>
  %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
  %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
  %4 = arith.subf %3, %cst_0 : vector<2xf32>
  %5 = arith.mulf %4, %cst : vector<2xf32>
  vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
  return
}

// -----// IR Dump After CleanupBufferAllocView (iree-codegen-cleanup-buffer-alloc-view) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
  memref.assume_alignment %0, 64 : memref<2xi8>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
  memref.assume_alignment %1, 64 : memref<2xf32>
  %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
  %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
  %4 = arith.subf %3, %cst_0 : vector<2xf32>
  %5 = arith.mulf %4, %cst : vector<2xf32>
  vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
  return
}

// -----// IR Dump After LowerUKernelOpsToCalls (iree-codegen-lower-ukernel-ops-to-calls) //----- //
module {
  func.func @main_dispatch_1_softmax_2xf32() {
    %cst = arith.constant dense<0.000000e+00> : vector<1xf32>
    %c0 = arith.constant 0 : index
    %c64 = arith.constant 64 : index
    %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
    %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
    %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
    memref.assume_alignment %0, 64 : memref<2xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
    memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
    %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
    %3 = vector.extractelement %cst_1[] : vector<f32>
    %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
    %5 = vector.insertelement %4, %cst[%c0 : index] : vector<1xf32>
    %6 = vector.extract %5[0] : vector<1xf32>
    %7 = vector.broadcast %6 : f32 to vector<f32>
    %8 = vector.extractelement %7[] : vector<f32>
    memref.store %8, %alloca[] : memref<f32>
    %9 = memref.load %alloca[] : memref<f32>
    %10 = vector.broadcast %9 : f32 to vector<2xf32>
    %11 = vector.extractelement %cst_0[] : vector<f32>
    %12 = arith.subf %2, %10 : vector<2xf32>
    %13 = math.exp %12 : vector<2xf32>
    %14 = vector.reduction <add>, %13, %11 : vector<2xf32> into f32
    %15 = vector.insertelement %14, %cst[%c0 : index] : vector<1xf32>
    %16 = vector.extract %15[0] : vector<1xf32>
    %17 = vector.broadcast %16 : f32 to vector<f32>
    %18 = vector.extractelement %17[] : vector<f32>
    memref.store %18, %alloca[] : memref<f32>
    %19 = memref.load %alloca[] : memref<f32>
    %20 = vector.broadcast %19 : f32 to vector<2xf32>
    %21 = arith.divf %13, %20 : vector<2xf32>
    vector.store %21, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
    return
  }
}

// -----// IR Dump After LLVMCPUCheckIRBeforeLLVMConversion (iree-llvmcpu-check-ir-before-llvm-conversion) //----- //
module {
  func.func @main_dispatch_0_generic_2_i8xf32() {
    %cst = arith.constant dense<0.0125187514> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
    memref.assume_alignment %0, 64 : memref<2xi8>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
    memref.assume_alignment %1, 64 : memref<2xf32>
    %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
    %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
    %4 = arith.subf %3, %cst_0 : vector<2xf32>
    %5 = arith.mulf %4, %cst : vector<2xf32>
    vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
    return
  }
}

// -----// IR Dump After LowerUKernelOpsToCalls (iree-codegen-lower-ukernel-ops-to-calls) //----- //
module {
  func.func @main_dispatch_2_generic_2_f32xi8() {
    %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
    %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
    memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
    memref.assume_alignment %1, 64 : memref<2xi8>
    %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
    %3 = arith.mulf %2, %cst_1 : vector<2xf32>
    %4 = arith.addf %3, %cst_0 : vector<2xf32>
    %5 = math.roundeven %4 : vector<2xf32>
    %6 = arith.minf %5, %cst : vector<2xf32>
    %7 = arith.maxf %6, %cst_0 : vector<2xf32>
    %8 = arith.fptosi %7 : vector<2xf32> to vector<2xi8>
    vector.store %8, %1[%c0] : memref<2xi8>, vector<2xi8>
    return
  }
}

// -----// IR Dump After LinalgExtToLoops (iree-linalg-ext-to-loops) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant dense<0.000000e+00> : vector<1xf32>
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
  memref.assume_alignment %0, 64 : memref<2xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
  %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
  %3 = vector.extractelement %cst_1[] : vector<f32>
  %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
  %5 = vector.insertelement %4, %cst[%c0 : index] : vector<1xf32>
  %6 = vector.extract %5[0] : vector<1xf32>
  %7 = vector.broadcast %6 : f32 to vector<f32>
  %8 = vector.extractelement %7[] : vector<f32>
  memref.store %8, %alloca[] : memref<f32>
  %9 = memref.load %alloca[] : memref<f32>
  %10 = vector.broadcast %9 : f32 to vector<2xf32>
  %11 = vector.extractelement %cst_0[] : vector<f32>
  %12 = arith.subf %2, %10 : vector<2xf32>
  %13 = math.exp %12 : vector<2xf32>
  %14 = vector.reduction <add>, %13, %11 : vector<2xf32> into f32
  %15 = vector.insertelement %14, %cst[%c0 : index] : vector<1xf32>
  %16 = vector.extract %15[0] : vector<1xf32>
  %17 = vector.broadcast %16 : f32 to vector<f32>
  %18 = vector.extractelement %17[] : vector<f32>
  memref.store %18, %alloca[] : memref<f32>
  %19 = memref.load %alloca[] : memref<f32>
  %20 = vector.broadcast %19 : f32 to vector<2xf32>
  %21 = arith.divf %13, %20 : vector<2xf32>
  vector.store %21, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  return
}

// -----// IR Dump After LinalgExtToLoops (iree-linalg-ext-to-loops) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
  memref.assume_alignment %1, 64 : memref<2xi8>
  %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  %3 = arith.mulf %2, %cst_1 : vector<2xf32>
  %4 = arith.addf %3, %cst_0 : vector<2xf32>
  %5 = math.roundeven %4 : vector<2xf32>
  %6 = arith.minf %5, %cst : vector<2xf32>
  %7 = arith.maxf %6, %cst_0 : vector<2xf32>
  %8 = arith.fptosi %7 : vector<2xf32> to vector<2xi8>
  vector.store %8, %1[%c0] : memref<2xi8>, vector<2xi8>
  return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
  memref.assume_alignment %0, 64 : memref<2xi8>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
  memref.assume_alignment %1, 64 : memref<2xf32>
  %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
  %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
  %4 = arith.subf %3, %cst_0 : vector<2xf32>
  %5 = arith.mulf %4, %cst : vector<2xf32>
  vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
  return
}

// -----// IR Dump After MemrefCopyToLinalgPass (iree-codegen-memrefcopy-to-linalg) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant dense<0.000000e+00> : vector<1xf32>
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
  memref.assume_alignment %0, 64 : memref<2xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
  %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
  %3 = vector.extractelement %cst_1[] : vector<f32>
  %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
  %5 = vector.insertelement %4, %cst[%c0 : index] : vector<1xf32>
  %6 = vector.extract %5[0] : vector<1xf32>
  %7 = vector.broadcast %6 : f32 to vector<f32>
  %8 = vector.extractelement %7[] : vector<f32>
  memref.store %8, %alloca[] : memref<f32>
  %9 = memref.load %alloca[] : memref<f32>
  %10 = vector.broadcast %9 : f32 to vector<2xf32>
  %11 = vector.extractelement %cst_0[] : vector<f32>
  %12 = arith.subf %2, %10 : vector<2xf32>
  %13 = math.exp %12 : vector<2xf32>
  %14 = vector.reduction <add>, %13, %11 : vector<2xf32> into f32
  %15 = vector.insertelement %14, %cst[%c0 : index] : vector<1xf32>
  %16 = vector.extract %15[0] : vector<1xf32>
  %17 = vector.broadcast %16 : f32 to vector<f32>
  %18 = vector.extractelement %17[] : vector<f32>
  memref.store %18, %alloca[] : memref<f32>
  %19 = memref.load %alloca[] : memref<f32>
  %20 = vector.broadcast %19 : f32 to vector<2xf32>
  %21 = arith.divf %13, %20 : vector<2xf32>
  vector.store %21, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  return
}

// -----// IR Dump After MemrefCopyToLinalgPass (iree-codegen-memrefcopy-to-linalg) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
  memref.assume_alignment %1, 64 : memref<2xi8>
  %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  %3 = arith.mulf %2, %cst_1 : vector<2xf32>
  %4 = arith.addf %3, %cst_0 : vector<2xf32>
  %5 = math.roundeven %4 : vector<2xf32>
  %6 = arith.minf %5, %cst : vector<2xf32>
  %7 = arith.maxf %6, %cst_0 : vector<2xf32>
  %8 = arith.fptosi %7 : vector<2xf32> to vector<2xi8>
  vector.store %8, %1[%c0] : memref<2xi8>, vector<2xi8>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
  memref.assume_alignment %0, 64 : memref<2xi8>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
  memref.assume_alignment %1, 64 : memref<2xf32>
  %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
  %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
  %4 = arith.subf %3, %cst_0 : vector<2xf32>
  %5 = arith.mulf %4, %cst : vector<2xf32>
  vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
  memref.assume_alignment %0, 64 : memref<2xi8>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
  memref.assume_alignment %1, 64 : memref<2xf32>
  %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
  %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
  %4 = arith.subf %3, %cst_0 : vector<2xf32>
  %5 = arith.mulf %4, %cst : vector<2xf32>
  vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
  return
}

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
  memref.assume_alignment %1, 64 : memref<2xi8>
  %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  %3 = arith.mulf %2, %cst_1 : vector<2xf32>
  %4 = arith.addf %3, %cst_0 : vector<2xf32>
  %5 = math.roundeven %4 : vector<2xf32>
  %6 = arith.minf %5, %cst : vector<2xf32>
  %7 = arith.maxf %6, %cst_0 : vector<2xf32>
  %8 = arith.fptosi %7 : vector<2xf32> to vector<2xi8>
  vector.store %8, %1[%c0] : memref<2xi8>, vector<2xi8>
  return
}

// -----// IR Dump After LinalgLowerToLoops (convert-linalg-to-loops) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant dense<0.000000e+00> : vector<1xf32>
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
  memref.assume_alignment %0, 64 : memref<2xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
  %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
  %3 = vector.extractelement %cst_1[] : vector<f32>
  %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
  %5 = vector.insertelement %4, %cst[%c0 : index] : vector<1xf32>
  %6 = vector.extract %5[0] : vector<1xf32>
  %7 = vector.broadcast %6 : f32 to vector<f32>
  %8 = vector.extractelement %7[] : vector<f32>
  memref.store %8, %alloca[] : memref<f32>
  %9 = memref.load %alloca[] : memref<f32>
  %10 = vector.broadcast %9 : f32 to vector<2xf32>
  %11 = vector.extractelement %cst_0[] : vector<f32>
  %12 = arith.subf %2, %10 : vector<2xf32>
  %13 = math.exp %12 : vector<2xf32>
  %14 = vector.reduction <add>, %13, %11 : vector<2xf32> into f32
  %15 = vector.insertelement %14, %cst[%c0 : index] : vector<1xf32>
  %16 = vector.extract %15[0] : vector<1xf32>
  %17 = vector.broadcast %16 : f32 to vector<f32>
  %18 = vector.extractelement %17[] : vector<f32>
  memref.store %18, %alloca[] : memref<f32>
  %19 = memref.load %alloca[] : memref<f32>
  %20 = vector.broadcast %19 : f32 to vector<2xf32>
  %21 = arith.divf %13, %20 : vector<2xf32>
  vector.store %21, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  return
}

// -----// IR Dump After ArithExpandOps (arith-expand) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
  memref.assume_alignment %0, 64 : memref<2xi8>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
  memref.assume_alignment %1, 64 : memref<2xf32>
  %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
  %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
  %4 = arith.subf %3, %cst_0 : vector<2xf32>
  %5 = arith.mulf %4, %cst : vector<2xf32>
  vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
  return
}

// -----// IR Dump After ConvertBf16ArithToF32 (iree-convert-bf16-arith-to-f32) //----- //
module {
  func.func @main_dispatch_2_generic_2_f32xi8() {
    %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
    %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
    memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
    memref.assume_alignment %1, 64 : memref<2xi8>
    %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
    %3 = arith.mulf %2, %cst_1 : vector<2xf32>
    %4 = arith.addf %3, %cst_0 : vector<2xf32>
    %5 = math.roundeven %4 : vector<2xf32>
    %6 = arith.minf %5, %cst : vector<2xf32>
    %7 = arith.maxf %6, %cst_0 : vector<2xf32>
    %8 = arith.fptosi %7 : vector<2xf32> to vector<2xi8>
    vector.store %8, %1[%c0] : memref<2xi8>, vector<2xi8>
    return
  }
}

// -----// IR Dump After ConvertBf16ArithToF32 (iree-convert-bf16-arith-to-f32) //----- //
module {
  func.func @main_dispatch_1_softmax_2xf32() {
    %cst = arith.constant dense<0.000000e+00> : vector<1xf32>
    %c0 = arith.constant 0 : index
    %c64 = arith.constant 64 : index
    %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
    %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
    %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
    memref.assume_alignment %0, 64 : memref<2xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
    memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
    %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
    %3 = vector.extractelement %cst_1[] : vector<f32>
    %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
    %5 = vector.insertelement %4, %cst[%c0 : index] : vector<1xf32>
    %6 = vector.extract %5[0] : vector<1xf32>
    %7 = vector.broadcast %6 : f32 to vector<f32>
    %8 = vector.extractelement %7[] : vector<f32>
    memref.store %8, %alloca[] : memref<f32>
    %9 = memref.load %alloca[] : memref<f32>
    %10 = vector.broadcast %9 : f32 to vector<2xf32>
    %11 = vector.extractelement %cst_0[] : vector<f32>
    %12 = arith.subf %2, %10 : vector<2xf32>
    %13 = math.exp %12 : vector<2xf32>
    %14 = vector.reduction <add>, %13, %11 : vector<2xf32> into f32
    %15 = vector.insertelement %14, %cst[%c0 : index] : vector<1xf32>
    %16 = vector.extract %15[0] : vector<1xf32>
    %17 = vector.broadcast %16 : f32 to vector<f32>
    %18 = vector.extractelement %17[] : vector<f32>
    memref.store %18, %alloca[] : memref<f32>
    %19 = memref.load %alloca[] : memref<f32>
    %20 = vector.broadcast %19 : f32 to vector<2xf32>
    %21 = arith.divf %13, %20 : vector<2xf32>
    vector.store %21, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
    return
  }
}

// -----// IR Dump After ExpandOps (memref-expand) //----- //
func.func @main_dispatch_0_generic_2_i8xf32() {
  %cst = arith.constant dense<0.0125187514> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.000000e+00> : vector<2xf32>
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xi8>
  memref.assume_alignment %0, 64 : memref<2xi8>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xf32>
  memref.assume_alignment %1, 64 : memref<2xf32>
  %2 = vector.load %0[%c0] : memref<2xi8>, vector<2xi8>
  %3 = arith.sitofp %2 : vector<2xi8> to vector<2xf32>
  %4 = arith.subf %3, %cst_0 : vector<2xf32>
  %5 = arith.mulf %4, %cst : vector<2xf32>
  vector.store %5, %1[%c0] : memref<2xf32>, vector<2xf32>
  return
}

// -----// IR Dump After ConvertBf16ToUInt16Buffers (iree-convert-bf16-to-uint16-buffers) //----- //
module {
  func.func @main_dispatch_2_generic_2_f32xi8() {
    %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
    %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
    memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
    memref.assume_alignment %1, 64 : memref<2xi8>
    %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
    %3 = arith.mulf %2, %cst_1 : vector<2xf32>
    %4 = arith.addf %3, %cst_0 : vector<2xf32>
    %5 = math.roundeven %4 : vector<2xf32>
    %6 = arith.minf %5, %cst : vector<2xf32>
    %7 = arith.maxf %6, %cst_0 : vector<2xf32>
    %8 = arith.fptosi %7 : vector<2xf32> to vector<2xi8>
    vector.store %8, %1[%c0] : memref<2xi8>, vector<2xi8>
    return
  }
}

// -----// IR Dump After ConvertBf16ToUInt16Buffers (iree-convert-bf16-to-uint16-buffers) //----- //
module {
  func.func @main_dispatch_1_softmax_2xf32() {
    %cst = arith.constant dense<0.000000e+00> : vector<1xf32>
    %c0 = arith.constant 0 : index
    %c64 = arith.constant 64 : index
    %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
    %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
    %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
    memref.assume_alignment %0, 64 : memref<2xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
    memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
    %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
    %3 = vector.extractelement %cst_1[] : vector<f32>
    %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
    %5 = vector.insertelement %4, %cst[%c0 : index] : vector<1xf32>
    %6 = vector.extract %5[0] : vector<1xf32>
    %7 = vector.broadcast %6 : f32 to vector<f32>
    %8 = vector.extractelement %7[] : vector<f32>
    memref.store %8, %alloca[] : memref<f32>
    %9 = memref.load %alloca[] : memref<f32>
    %10 = vector.broadcast %9 : f32 to vector<2xf32>
    %11 = vector.extractelement %cst_0[] : vector<f32>
    %12 = arith.subf %2, %10 : vector<2xf32>
    %13 = math.exp %12 : vector<2xf32>
    %14 = vector.reduction <add>, %13, %11 : vector<2xf32> into f32
    %15 = vector.insertelement %14, %cst[%c0 : index] : vector<1xf32>
    %16 = vector.extract %15[0] : vector<1xf32>
    %17 = vector.broadcast %16 : f32 to vector<f32>
    %18 = vector.extractelement %17[] : vector<f32>
    memref.store %18, %alloca[] : memref<f32>
    %19 = memref.load %alloca[] : memref<f32>
    %20 = vector.broadcast %19 : f32 to vector<2xf32>
    %21 = arith.divf %13, %20 : vector<2xf32>
    vector.store %21, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
    return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
  memref.assume_alignment %1, 64 : memref<2xi8>
  %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  %3 = arith.mulf %2, %cst_1 : vector<2xf32>
  %4 = arith.addf %3, %cst_0 : vector<2xf32>
  %5 = math.roundeven %4 : vector<2xf32>
  %6 = arith.minf %5, %cst : vector<2xf32>
  %7 = arith.maxf %6, %cst_0 : vector<2xf32>
  %8 = arith.fptosi %7 : vector<2xf32> to vector<2xi8>
  vector.store %8, %1[%c0] : memref<2xi8>, vector<2xi8>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
  memref.assume_alignment %1, 64 : memref<2xi8>
  %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  %3 = arith.mulf %2, %cst_1 : vector<2xf32>
  %4 = arith.addf %3, %cst_0 : vector<2xf32>
  %5 = math.roundeven %4 : vector<2xf32>
  %6 = arith.minf %5, %cst : vector<2xf32>
  %7 = arith.maxf %6, %cst_0 : vector<2xf32>
  %8 = arith.fptosi %7 : vector<2xf32> to vector<2xi8>
  vector.store %8, %1[%c0] : memref<2xi8>, vector<2xi8>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant dense<0.000000e+00> : vector<1xf32>
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
  memref.assume_alignment %0, 64 : memref<2xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
  %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
  %3 = vector.extractelement %cst_1[] : vector<f32>
  %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
  %5 = vector.insertelement %4, %cst[%c0 : index] : vector<1xf32>
  %6 = vector.extract %5[0] : vector<1xf32>
  %7 = vector.broadcast %6 : f32 to vector<f32>
  %8 = vector.extractelement %7[] : vector<f32>
  memref.store %8, %alloca[] : memref<f32>
  %9 = memref.load %alloca[] : memref<f32>
  %10 = vector.broadcast %9 : f32 to vector<2xf32>
  %11 = vector.extractelement %cst_0[] : vector<f32>
  %12 = arith.subf %2, %10 : vector<2xf32>
  %13 = math.exp %12 : vector<2xf32>
  %14 = vector.reduction <add>, %13, %11 : vector<2xf32> into f32
  %15 = vector.insertelement %14, %cst[%c0 : index] : vector<1xf32>
  %16 = vector.extract %15[0] : vector<1xf32>
  %17 = vector.broadcast %16 : f32 to vector<f32>
  %18 = vector.extractelement %17[] : vector<f32>
  memref.store %18, %alloca[] : memref<f32>
  %19 = memref.load %alloca[] : memref<f32>
  %20 = vector.broadcast %19 : f32 to vector<2xf32>
  %21 = arith.divf %13, %20 : vector<2xf32>
  vector.store %21, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  return
}

// -----// IR Dump After ConvertToLLVM (iree-convert-to-llvm) //----- //
module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
  llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
    %0 = llvm.mlir.constant(0 : i32) : i32
    %1 = llvm.mlir.constant(63 : index) : i32
    %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
    %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
    %4 = llvm.mlir.constant(0 : index) : i32
    %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
    %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
    %9 = llvm.and %8, %1  : i32
    %10 = llvm.icmp "eq" %9, %4 : i32
    "llvm.intr.assume"(%10) : (i1) -> ()
    %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
    %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
    %16 = llvm.and %15, %1  : i32
    %17 = llvm.icmp "eq" %16, %4 : i32
    "llvm.intr.assume"(%17) : (i1) -> ()
    %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
    %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
    %20 = llvm.fsub %19, %3  : vector<2xf32>
    %21 = llvm.fmul %20, %2  : vector<2xf32>
    llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
    llvm.return %0 : i32
  }
}

// -----// IR Dump After ArithBufferize (arith-bufferize) //----- //
module {
  func.func @main_dispatch_2_generic_2_f32xi8() {
    %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
    %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
    memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
    memref.assume_alignment %1, 64 : memref<2xi8>
    %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
    %3 = arith.mulf %2, %cst_1 : vector<2xf32>
    %4 = arith.addf %3, %cst_0 : vector<2xf32>
    %5 = math.roundeven %4 : vector<2xf32>
    %6 = arith.minf %5, %cst : vector<2xf32>
    %7 = arith.maxf %6, %cst_0 : vector<2xf32>
    %8 = arith.fptosi %7 : vector<2xf32> to vector<2xi8>
    vector.store %8, %1[%c0] : memref<2xi8>, vector<2xi8>
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant dense<0.000000e+00> : vector<1xf32>
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
  memref.assume_alignment %0, 64 : memref<2xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
  %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
  %3 = vector.extractelement %cst_1[] : vector<f32>
  %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
  %5 = vector.insertelement %4, %cst[%c0 : index] : vector<1xf32>
  %6 = vector.extract %5[0] : vector<1xf32>
  %7 = vector.broadcast %6 : f32 to vector<f32>
  %8 = vector.extractelement %7[] : vector<f32>
  memref.store %8, %alloca[] : memref<f32>
  %9 = memref.load %alloca[] : memref<f32>
  %10 = vector.broadcast %9 : f32 to vector<2xf32>
  %11 = vector.extractelement %cst_0[] : vector<f32>
  %12 = arith.subf %2, %10 : vector<2xf32>
  %13 = math.exp %12 : vector<2xf32>
  %14 = vector.reduction <add>, %13, %11 : vector<2xf32> into f32
  %15 = vector.insertelement %14, %cst[%c0 : index] : vector<1xf32>
  %16 = vector.extract %15[0] : vector<1xf32>
  %17 = vector.broadcast %16 : f32 to vector<f32>
  %18 = vector.extractelement %17[] : vector<f32>
  memref.store %18, %alloca[] : memref<f32>
  %19 = memref.load %alloca[] : memref<f32>
  %20 = vector.broadcast %19 : f32 to vector<2xf32>
  %21 = arith.divf %13, %20 : vector<2xf32>
  vector.store %21, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  return
}

// -----// IR Dump After ReconcileUnrealizedCasts (reconcile-unrealized-casts) //----- //
module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
  llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
    %0 = llvm.mlir.constant(0 : i32) : i32
    %1 = llvm.mlir.constant(63 : index) : i32
    %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
    %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
    %4 = llvm.mlir.constant(0 : index) : i32
    %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
    %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
    %9 = llvm.and %8, %1  : i32
    %10 = llvm.icmp "eq" %9, %4 : i32
    "llvm.intr.assume"(%10) : (i1) -> ()
    %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
    %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
    %16 = llvm.and %15, %1  : i32
    %17 = llvm.icmp "eq" %16, %4 : i32
    "llvm.intr.assume"(%17) : (i1) -> ()
    %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
    %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
    %20 = llvm.fsub %19, %3  : vector<2xf32>
    %21 = llvm.fmul %20, %2  : vector<2xf32>
    llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
    llvm.return %0 : i32
  }
}

// -----// IR Dump After LLVMCPUSynchronizeSymbolVisibility (iree-llvmcpu-synchronize-symbol-visibility) //----- //
module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
  llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
    %0 = llvm.mlir.constant(0 : i32) : i32
    %1 = llvm.mlir.constant(63 : index) : i32
    %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
    %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
    %4 = llvm.mlir.constant(0 : index) : i32
    %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
    %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
    %9 = llvm.and %8, %1  : i32
    %10 = llvm.icmp "eq" %9, %4 : i32
    "llvm.intr.assume"(%10) : (i1) -> ()
    %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
    %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
    %16 = llvm.and %15, %1  : i32
    %17 = llvm.icmp "eq" %16, %4 : i32
    "llvm.intr.assume"(%17) : (i1) -> ()
    %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
    %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
    %20 = llvm.fsub %19, %3  : vector<2xf32>
    %21 = llvm.fmul %20, %2  : vector<2xf32>
    llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
    llvm.return %0 : i32
  }
}

// -----// IR Dump After ArithBufferize (arith-bufferize) //----- //
module {
  func.func @main_dispatch_1_softmax_2xf32() {
    %cst = arith.constant dense<0.000000e+00> : vector<1xf32>
    %c0 = arith.constant 0 : index
    %c64 = arith.constant 64 : index
    %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
    %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
    %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
    memref.assume_alignment %0, 64 : memref<2xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
    memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
    %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
    %3 = vector.extractelement %cst_1[] : vector<f32>
    %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
    %5 = vector.insertelement %4, %cst[%c0 : index] : vector<1xf32>
    %6 = vector.extract %5[0] : vector<1xf32>
    %7 = vector.broadcast %6 : f32 to vector<f32>
    %8 = vector.extractelement %7[] : vector<f32>
    memref.store %8, %alloca[] : memref<f32>
    %9 = memref.load %alloca[] : memref<f32>
    %10 = vector.broadcast %9 : f32 to vector<2xf32>
    %11 = vector.extractelement %cst_0[] : vector<f32>
    %12 = arith.subf %2, %10 : vector<2xf32>
    %13 = math.exp %12 : vector<2xf32>
    %14 = vector.reduction <add>, %13, %11 : vector<2xf32> into f32
    %15 = vector.insertelement %14, %cst[%c0 : index] : vector<1xf32>
    %16 = vector.extract %15[0] : vector<1xf32>
    %17 = vector.broadcast %16 : f32 to vector<f32>
    %18 = vector.extractelement %17[] : vector<f32>
    memref.store %18, %alloca[] : memref<f32>
    %19 = memref.load %alloca[] : memref<f32>
    %20 = vector.broadcast %19 : f32 to vector<2xf32>
    %21 = arith.divf %13, %20 : vector<2xf32>
    vector.store %21, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
    return
  }
}

// -----// IR Dump After FoldTensorExtractOp (iree-codegen-fold-tensor-extract-op) //----- //
module {
  func.func @main_dispatch_2_generic_2_f32xi8() {
    %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
    %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
    memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
    memref.assume_alignment %1, 64 : memref<2xi8>
    %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
    %3 = arith.mulf %2, %cst_1 : vector<2xf32>
    %4 = arith.addf %3, %cst_0 : vector<2xf32>
    %5 = math.roundeven %4 : vector<2xf32>
    %6 = arith.minf %5, %cst : vector<2xf32>
    %7 = arith.maxf %6, %cst_0 : vector<2xf32>
    %8 = arith.fptosi %7 : vector<2xf32> to vector<2xi8>
    vector.store %8, %1[%c0] : memref<2xi8>, vector<2xi8>
    return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
  llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
    %0 = llvm.mlir.constant(0 : i32) : i32
    %1 = llvm.mlir.constant(63 : index) : i32
    %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
    %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
    %4 = llvm.mlir.constant(0 : index) : i32
    %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
    %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
    %9 = llvm.and %8, %1  : i32
    %10 = llvm.icmp "eq" %9, %4 : i32
    "llvm.intr.assume"(%10) : (i1) -> ()
    %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
    %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
    %16 = llvm.and %15, %1  : i32
    %17 = llvm.icmp "eq" %16, %4 : i32
    "llvm.intr.assume"(%17) : (i1) -> ()
    %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
    %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
    %20 = llvm.fsub %19, %3  : vector<2xf32>
    %21 = llvm.fmul %20, %2  : vector<2xf32>
    llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
    llvm.return %0 : i32
  }
}

// -----// IR Dump After FoldTensorExtractOp (iree-codegen-fold-tensor-extract-op) //----- //
module {
  func.func @main_dispatch_1_softmax_2xf32() {
    %cst = arith.constant dense<0.000000e+00> : vector<1xf32>
    %c0 = arith.constant 0 : index
    %c64 = arith.constant 64 : index
    %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
    %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
    %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
    memref.assume_alignment %0, 64 : memref<2xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
    memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
    %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
    %3 = vector.extractelement %cst_1[] : vector<f32>
    %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
    %5 = vector.insertelement %4, %cst[%c0 : index] : vector<1xf32>
    %6 = vector.extract %5[0] : vector<1xf32>
    %7 = vector.broadcast %6 : f32 to vector<f32>
    %8 = vector.extractelement %7[] : vector<f32>
    memref.store %8, %alloca[] : memref<f32>
    %9 = memref.load %alloca[] : memref<f32>
    %10 = vector.broadcast %9 : f32 to vector<2xf32>
    %11 = vector.extractelement %cst_0[] : vector<f32>
    %12 = arith.subf %2, %10 : vector<2xf32>
    %13 = math.exp %12 : vector<2xf32>
    %14 = vector.reduction <add>, %13, %11 : vector<2xf32> into f32
    %15 = vector.insertelement %14, %cst[%c0 : index] : vector<1xf32>
    %16 = vector.extract %15[0] : vector<1xf32>
    %17 = vector.broadcast %16 : f32 to vector<f32>
    %18 = vector.extractelement %17[] : vector<f32>
    memref.store %18, %alloca[] : memref<f32>
    %19 = memref.load %alloca[] : memref<f32>
    %20 = vector.broadcast %19 : f32 to vector<2xf32>
    %21 = arith.divf %13, %20 : vector<2xf32>
    vector.store %21, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
    return
  }
}

// -----// IR Dump After ConvertComplexToStandard (convert-complex-to-standard) //----- //
module {
  func.func @main_dispatch_2_generic_2_f32xi8() {
    %cst = arith.constant dense<1.270000e+02> : vector<2xf32>
    %cst_0 = arith.constant dense<-1.280000e+02> : vector<2xf32>
    %cst_1 = arith.constant dense<2.560000e+02> : vector<2xf32>
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
    memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
    memref.assume_alignment %1, 64 : memref<2xi8>
    %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
    %3 = arith.mulf %2, %cst_1 : vector<2xf32>
    %4 = arith.addf %3, %cst_0 : vector<2xf32>
    %5 = math.roundeven %4 : vector<2xf32>
    %6 = arith.minf %5, %cst : vector<2xf32>
    %7 = arith.maxf %6, %cst_0 : vector<2xf32>
    %8 = arith.fptosi %7 : vector<2xf32> to vector<2xi8>
    vector.store %8, %1[%c0] : memref<2xi8>, vector<2xi8>
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
  llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
    %0 = llvm.mlir.constant(0 : i32) : i32
    %1 = llvm.mlir.constant(63 : index) : i32
    %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
    %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
    %4 = llvm.mlir.constant(0 : index) : i32
    %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
    %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
    %9 = llvm.and %8, %1  : i32
    %10 = llvm.icmp "eq" %9, %4 : i32
    "llvm.intr.assume"(%10) : (i1) -> ()
    %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
    %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
    %16 = llvm.and %15, %1  : i32
    %17 = llvm.icmp "eq" %16, %4 : i32
    "llvm.intr.assume"(%17) : (i1) -> ()
    %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
    %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
    %20 = llvm.fsub %19, %3  : vector<2xf32>
    %21 = llvm.fmul %20, %2  : vector<2xf32>
    llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
    llvm.return %0 : i32
  }
}

// -----// IR Dump After ConvertComplexToStandard (convert-complex-to-standard) //----- //
module {
  func.func @main_dispatch_1_softmax_2xf32() {
    %cst = arith.constant dense<0.000000e+00> : vector<1xf32>
    %c0 = arith.constant 0 : index
    %c64 = arith.constant 64 : index
    %cst_0 = arith.constant dense<0.000000e+00> : vector<f32>
    %cst_1 = arith.constant dense<-1.000000e+30> : vector<f32>
    %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
    memref.assume_alignment %0, 64 : memref<2xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
    memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
    %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
    %3 = vector.extractelement %cst_1[] : vector<f32>
    %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
    %5 = vector.insertelement %4, %cst[%c0 : index] : vector<1xf32>
    %6 = vector.extract %5[0] : vector<1xf32>
    %7 = vector.broadcast %6 : f32 to vector<f32>
    %8 = vector.extractelement %7[] : vector<f32>
    memref.store %8, %alloca[] : memref<f32>
    %9 = memref.load %alloca[] : memref<f32>
    %10 = vector.broadcast %9 : f32 to vector<2xf32>
    %11 = vector.extractelement %cst_0[] : vector<f32>
    %12 = arith.subf %2, %10 : vector<2xf32>
    %13 = math.exp %12 : vector<2xf32>
    %14 = vector.reduction <add>, %13, %11 : vector<2xf32> into f32
    %15 = vector.insertelement %14, %cst[%c0 : index] : vector<1xf32>
    %16 = vector.extract %15[0] : vector<1xf32>
    %17 = vector.broadcast %16 : f32 to vector<f32>
    %18 = vector.extractelement %17[] : vector<f32>
    memref.store %18, %alloca[] : memref<f32>
    %19 = memref.load %alloca[] : memref<f32>
    %20 = vector.broadcast %19 : f32 to vector<2xf32>
    %21 = arith.divf %13, %20 : vector<2xf32>
    vector.store %21, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
    return
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::TranslateTargetExecutableVariantsPass (iree-hal-translate-target-executable-variants) //----- //
hal.executable.variant public @static, target = <"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}> {
  hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert>} {
  ^bb0(%arg0: !hal.device):
    %c1 = arith.constant 1 : index
    hal.return %c1, %c1, %c1 : index, index, index
  }
  builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
    llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
      %0 = llvm.mlir.constant(0 : i32) : i32
      %1 = llvm.mlir.constant(63 : index) : i32
      %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
      %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
      %4 = llvm.mlir.constant(0 : index) : i32
      %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
      %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
      %9 = llvm.and %8, %1  : i32
      %10 = llvm.icmp "eq" %9, %4 : i32
      "llvm.intr.assume"(%10) : (i1) -> ()
      %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
      %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
      %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
      %16 = llvm.and %15, %1  : i32
      %17 = llvm.icmp "eq" %16, %4 : i32
      "llvm.intr.assume"(%17) : (i1) -> ()
      %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
      %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
      %20 = llvm.fsub %19, %3  : vector<2xf32>
      %21 = llvm.fmul %20, %2  : vector<2xf32>
      llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
      llvm.return %0 : i32
    }
  }
}

// -----// IR Dump After PolynomialApproximationPass (iree-codegen-polynomial-approximation) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant dense<0.693147182> : vector<2xf32>
  %cst_0 = arith.constant dense<1.44269502> : vector<2xf32>
  %cst_1 = arith.constant dense<1.000000e+00> : vector<2xf32>
  %cst_2 = arith.constant dense<0.499705136> : vector<2xf32>
  %cst_3 = arith.constant dense<0.168738902> : vector<2xf32>
  %cst_4 = arith.constant dense<0.0366896503> : vector<2xf32>
  %cst_5 = arith.constant dense<1.314350e-02> : vector<2xf32>
  %cst_6 = arith.constant dense<23> : vector<2xi32>
  %cst_7 = arith.constant dense<127> : vector<2xi32>
  %cst_8 = arith.constant dense<0.000000e+00> : vector<2xf32>
  %cst_9 = arith.constant dense<0x7F800000> : vector<2xf32>
  %cst_10 = arith.constant dense<0xFF800000> : vector<2xf32>
  %cst_11 = arith.constant dense<1.17549435E-38> : vector<2xf32>
  %cst_12 = arith.constant dense<-127> : vector<2xi32>
  %cst_13 = arith.constant dense<0.000000e+00> : vector<1xf32>
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %cst_14 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_15 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
  memref.assume_alignment %0, 64 : memref<2xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
  %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
  %3 = vector.extractelement %cst_15[] : vector<f32>
  %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
  %5 = vector.insertelement %4, %cst_13[%c0 : index] : vector<1xf32>
  %6 = vector.extract %5[0] : vector<1xf32>
  %7 = vector.broadcast %6 : f32 to vector<f32>
  %8 = vector.extractelement %7[] : vector<f32>
  memref.store %8, %alloca[] : memref<f32>
  %9 = memref.load %alloca[] : memref<f32>
  %10 = vector.broadcast %9 : f32 to vector<2xf32>
  %11 = vector.extractelement %cst_14[] : vector<f32>
  %12 = arith.subf %2, %10 : vector<2xf32>
  %13 = arith.cmpf uno, %12, %12 : vector<2xf32>
  %14 = arith.mulf %12, %cst_0 : vector<2xf32>
  %15 = math.floor %14 : vector<2xf32>
  %16 = arith.mulf %15, %cst : vector<2xf32>
  %17 = arith.subf %12, %16 : vector<2xf32>
  %18 = arith.mulf %17, %17 : vector<2xf32>
  %19 = arith.mulf %18, %18 : vector<2xf32>
  %20 = math.fma %cst_1, %17, %cst_1 : vector<2xf32>
  %21 = math.fma %cst_3, %17, %cst_2 : vector<2xf32>
  %22 = math.fma %cst_5, %17, %cst_4 : vector<2xf32>
  %23 = math.fma %21, %18, %20 : vector<2xf32>
  %24 = math.fma %22, %19, %23 : vector<2xf32>
  %25 = arith.fptosi %15 : vector<2xf32> to vector<2xi32>
  %26 = arith.addi %25, %cst_7 : vector<2xi32>
  %27 = arith.shli %26, %cst_6 : vector<2xi32>
  %28 = arith.bitcast %27 : vector<2xi32> to vector<2xf32>
  %29 = arith.mulf %24, %28 : vector<2xf32>
  %30 = arith.cmpi sle, %25, %cst_7 : vector<2xi32>
  %31 = arith.cmpi sge, %25, %cst_12 : vector<2xi32>
  %32 = arith.cmpf oeq, %12, %cst_10 : vector<2xf32>
  %33 = arith.cmpf oeq, %12, %cst_9 : vector<2xf32>
  %34 = arith.cmpf ogt, %12, %cst_8 : vector<2xf32>
  %35 = arith.andi %30, %31 : vector<2xi1>
  %36 = arith.select %34, %cst_9, %cst_11 : vector<2xi1>, vector<2xf32>
  %37 = arith.select %35, %29, %36 : vector<2xi1>, vector<2xf32>
  %38 = arith.select %33, %cst_9, %37 : vector<2xi1>, vector<2xf32>
  %39 = arith.select %32, %cst_8, %38 : vector<2xi1>, vector<2xf32>
  %40 = arith.select %13, %12, %39 : vector<2xi1>, vector<2xf32>
  %41 = vector.reduction <add>, %40, %11 : vector<2xf32> into f32
  %42 = vector.insertelement %41, %cst_13[%c0 : index] : vector<1xf32>
  %43 = vector.extract %42[0] : vector<1xf32>
  %44 = vector.broadcast %43 : f32 to vector<f32>
  %45 = vector.extractelement %44[] : vector<f32>
  memref.store %45, %alloca[] : memref<f32>
  %46 = memref.load %alloca[] : memref<f32>
  %47 = vector.broadcast %46 : f32 to vector<2xf32>
  %48 = arith.divf %40, %47 : vector<2xf32>
  vector.store %48, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  return
}

// -----// IR Dump After PolynomialApproximationPass (iree-codegen-polynomial-approximation) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.000000e+00> : vector<2xf32>
  %cst_0 = arith.constant dense<0> : vector<2xi32>
  %cst_1 = arith.constant dense<1> : vector<2xi32>
  %cst_2 = arith.constant dense<-1> : vector<2xi32>
  %cst_3 = arith.constant dense<23> : vector<2xi32>
  %cst_4 = arith.constant dense<31> : vector<2xi32>
  %cst_5 = arith.constant dense<127> : vector<2xi32>
  %cst_6 = arith.constant dense<4194304> : vector<2xi32>
  %cst_7 = arith.constant dense<8388607> : vector<2xi32>
  %cst_8 = arith.constant dense<255> : vector<2xi32>
  %cst_9 = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_10 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_11 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
  memref.assume_alignment %1, 64 : memref<2xi8>
  %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  %3 = arith.mulf %2, %cst_11 : vector<2xf32>
  %4 = arith.addf %3, %cst_10 : vector<2xf32>
  %5 = arith.bitcast %4 : vector<2xf32> to vector<2xi32>
  %6 = math.round %4 : vector<2xf32>
  %7 = arith.bitcast %6 : vector<2xf32> to vector<2xi32>
  %8 = arith.shrui %5, %cst_3 : vector<2xi32>
  %9 = arith.andi %8, %cst_8 : vector<2xi32>
  %10 = arith.subi %9, %cst_5 : vector<2xi32>
  %11 = arith.shrui %7, %cst_3 : vector<2xi32>
  %12 = arith.andi %11, %cst_8 : vector<2xi32>
  %13 = arith.subi %12, %cst_5 : vector<2xi32>
  %14 = arith.cmpi eq, %13, %cst_0 : vector<2xi32>
  %15 = arith.subi %13, %cst_1 : vector<2xi32>
  %16 = arith.maxsi %15, %cst_0 : vector<2xi32>
  %17 = arith.minsi %16, %cst_4 : vector<2xi32>
  %18 = arith.shrui %cst_7, %17 : vector<2xi32>
  %19 = arith.andi %7, %18 : vector<2xi32>
  %20 = arith.cmpi ne, %19, %cst_0 : vector<2xi32>
  %21 = arith.ori %20, %14 : vector<2xi1>
  %22 = arith.cmpi eq, %10, %cst_2 : vector<2xi32>
  %23 = arith.maxsi %10, %cst_0 : vector<2xi32>
  %24 = arith.minsi %23, %cst_4 : vector<2xi32>
  %25 = arith.shrui %cst_6, %24 : vector<2xi32>
  %26 = arith.select %22, %cst_0, %25 : vector<2xi1>, vector<2xi32>
  %27 = arith.maxsi %10, %cst_0 : vector<2xi32>
  %28 = arith.minsi %27, %cst_4 : vector<2xi32>
  %29 = arith.shrui %cst_7, %28 : vector<2xi32>
  %30 = arith.andi %5, %29 : vector<2xi32>
  %31 = arith.cmpi eq, %30, %26 : vector<2xi32>
  %32 = arith.cmpi sge, %10, %cst_2 : vector<2xi32>
  %33 = arith.cmpi slt, %10, %cst_3 : vector<2xi32>
  %34 = arith.andi %31, %33 : vector<2xi1>
  %35 = arith.andi %34, %32 : vector<2xi1>
  %36 = math.copysign %cst, %4 : vector<2xf32>
  %37 = arith.subf %6, %36 : vector<2xf32>
  %38 = arith.andi %21, %35 : vector<2xi1>
  %39 = arith.select %38, %37, %6 : vector<2xi1>, vector<2xf32>
  %40 = math.copysign %39, %4 : vector<2xf32>
  %41 = arith.minf %40, %cst_9 : vector<2xf32>
  %42 = arith.maxf %41, %cst_10 : vector<2xf32>
  %43 = arith.fptosi %42 : vector<2xf32> to vector<2xi8>
  vector.store %43, %1[%c0] : memref<2xi8>, vector<2xi8>
  return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::TranslateExecutablesPass (iree-hal-translate-executables) //----- //
hal.executable private @main_dispatch_0 {
  hal.executable.variant public @static, target = <"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}> {
    hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert>} {
    ^bb0(%arg0: !hal.device):
      %c1 = arith.constant 1 : index
      hal.return %c1, %c1, %c1 : index, index, index
    }
    builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
      llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
        %0 = llvm.mlir.constant(0 : i32) : i32
        %1 = llvm.mlir.constant(63 : index) : i32
        %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
        %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
        %4 = llvm.mlir.constant(0 : index) : i32
        %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
        %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
        %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
        %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
        %9 = llvm.and %8, %1  : i32
        %10 = llvm.icmp "eq" %9, %4 : i32
        "llvm.intr.assume"(%10) : (i1) -> ()
        %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
        %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
        %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
        %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
        %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
        %16 = llvm.and %15, %1  : i32
        %17 = llvm.icmp "eq" %16, %4 : i32
        "llvm.intr.assume"(%17) : (i1) -> ()
        %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
        %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
        %20 = llvm.fsub %19, %3  : vector<2xf32>
        %21 = llvm.fmul %20, %2  : vector<2xf32>
        llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
        llvm.return %0 : i32
      }
    }
  }
}

// -----// IR Dump After HoistStaticallyBoundAllocations (iree-hoist-statically-bound-allocations) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant dense<0.693147182> : vector<2xf32>
  %cst_0 = arith.constant dense<1.44269502> : vector<2xf32>
  %cst_1 = arith.constant dense<1.000000e+00> : vector<2xf32>
  %cst_2 = arith.constant dense<0.499705136> : vector<2xf32>
  %cst_3 = arith.constant dense<0.168738902> : vector<2xf32>
  %cst_4 = arith.constant dense<0.0366896503> : vector<2xf32>
  %cst_5 = arith.constant dense<1.314350e-02> : vector<2xf32>
  %cst_6 = arith.constant dense<23> : vector<2xi32>
  %cst_7 = arith.constant dense<127> : vector<2xi32>
  %cst_8 = arith.constant dense<0.000000e+00> : vector<2xf32>
  %cst_9 = arith.constant dense<0x7F800000> : vector<2xf32>
  %cst_10 = arith.constant dense<0xFF800000> : vector<2xf32>
  %cst_11 = arith.constant dense<1.17549435E-38> : vector<2xf32>
  %cst_12 = arith.constant dense<-127> : vector<2xi32>
  %cst_13 = arith.constant dense<0.000000e+00> : vector<1xf32>
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %cst_14 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_15 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
  memref.assume_alignment %0, 64 : memref<2xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
  %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
  %3 = vector.extractelement %cst_15[] : vector<f32>
  %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
  %5 = vector.insertelement %4, %cst_13[%c0 : index] : vector<1xf32>
  %6 = vector.extract %5[0] : vector<1xf32>
  %7 = vector.broadcast %6 : f32 to vector<f32>
  %8 = vector.extractelement %7[] : vector<f32>
  memref.store %8, %alloca[] : memref<f32>
  %9 = memref.load %alloca[] : memref<f32>
  %10 = vector.broadcast %9 : f32 to vector<2xf32>
  %11 = vector.extractelement %cst_14[] : vector<f32>
  %12 = arith.subf %2, %10 : vector<2xf32>
  %13 = arith.cmpf uno, %12, %12 : vector<2xf32>
  %14 = arith.mulf %12, %cst_0 : vector<2xf32>
  %15 = math.floor %14 : vector<2xf32>
  %16 = arith.mulf %15, %cst : vector<2xf32>
  %17 = arith.subf %12, %16 : vector<2xf32>
  %18 = arith.mulf %17, %17 : vector<2xf32>
  %19 = arith.mulf %18, %18 : vector<2xf32>
  %20 = math.fma %cst_1, %17, %cst_1 : vector<2xf32>
  %21 = math.fma %cst_3, %17, %cst_2 : vector<2xf32>
  %22 = math.fma %cst_5, %17, %cst_4 : vector<2xf32>
  %23 = math.fma %21, %18, %20 : vector<2xf32>
  %24 = math.fma %22, %19, %23 : vector<2xf32>
  %25 = arith.fptosi %15 : vector<2xf32> to vector<2xi32>
  %26 = arith.addi %25, %cst_7 : vector<2xi32>
  %27 = arith.shli %26, %cst_6 : vector<2xi32>
  %28 = arith.bitcast %27 : vector<2xi32> to vector<2xf32>
  %29 = arith.mulf %24, %28 : vector<2xf32>
  %30 = arith.cmpi sle, %25, %cst_7 : vector<2xi32>
  %31 = arith.cmpi sge, %25, %cst_12 : vector<2xi32>
  %32 = arith.cmpf oeq, %12, %cst_10 : vector<2xf32>
  %33 = arith.cmpf oeq, %12, %cst_9 : vector<2xf32>
  %34 = arith.cmpf ogt, %12, %cst_8 : vector<2xf32>
  %35 = arith.andi %30, %31 : vector<2xi1>
  %36 = arith.select %34, %cst_9, %cst_11 : vector<2xi1>, vector<2xf32>
  %37 = arith.select %35, %29, %36 : vector<2xi1>, vector<2xf32>
  %38 = arith.select %33, %cst_9, %37 : vector<2xi1>, vector<2xf32>
  %39 = arith.select %32, %cst_8, %38 : vector<2xi1>, vector<2xf32>
  %40 = arith.select %13, %12, %39 : vector<2xi1>, vector<2xf32>
  %41 = vector.reduction <add>, %40, %11 : vector<2xf32> into f32
  %42 = vector.insertelement %41, %cst_13[%c0 : index] : vector<1xf32>
  %43 = vector.extract %42[0] : vector<1xf32>
  %44 = vector.broadcast %43 : f32 to vector<f32>
  %45 = vector.extractelement %44[] : vector<f32>
  memref.store %45, %alloca[] : memref<f32>
  %46 = memref.load %alloca[] : memref<f32>
  %47 = vector.broadcast %46 : f32 to vector<2xf32>
  %48 = arith.divf %40, %47 : vector<2xf32>
  vector.store %48, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  return
}

// -----// IR Dump After HoistStaticallyBoundAllocations (iree-hoist-statically-bound-allocations) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.000000e+00> : vector<2xf32>
  %cst_0 = arith.constant dense<0> : vector<2xi32>
  %cst_1 = arith.constant dense<1> : vector<2xi32>
  %cst_2 = arith.constant dense<-1> : vector<2xi32>
  %cst_3 = arith.constant dense<23> : vector<2xi32>
  %cst_4 = arith.constant dense<31> : vector<2xi32>
  %cst_5 = arith.constant dense<127> : vector<2xi32>
  %cst_6 = arith.constant dense<4194304> : vector<2xi32>
  %cst_7 = arith.constant dense<8388607> : vector<2xi32>
  %cst_8 = arith.constant dense<255> : vector<2xi32>
  %cst_9 = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_10 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_11 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
  memref.assume_alignment %1, 64 : memref<2xi8>
  %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  %3 = arith.mulf %2, %cst_11 : vector<2xf32>
  %4 = arith.addf %3, %cst_10 : vector<2xf32>
  %5 = arith.bitcast %4 : vector<2xf32> to vector<2xi32>
  %6 = math.round %4 : vector<2xf32>
  %7 = arith.bitcast %6 : vector<2xf32> to vector<2xi32>
  %8 = arith.shrui %5, %cst_3 : vector<2xi32>
  %9 = arith.andi %8, %cst_8 : vector<2xi32>
  %10 = arith.subi %9, %cst_5 : vector<2xi32>
  %11 = arith.shrui %7, %cst_3 : vector<2xi32>
  %12 = arith.andi %11, %cst_8 : vector<2xi32>
  %13 = arith.subi %12, %cst_5 : vector<2xi32>
  %14 = arith.cmpi eq, %13, %cst_0 : vector<2xi32>
  %15 = arith.subi %13, %cst_1 : vector<2xi32>
  %16 = arith.maxsi %15, %cst_0 : vector<2xi32>
  %17 = arith.minsi %16, %cst_4 : vector<2xi32>
  %18 = arith.shrui %cst_7, %17 : vector<2xi32>
  %19 = arith.andi %7, %18 : vector<2xi32>
  %20 = arith.cmpi ne, %19, %cst_0 : vector<2xi32>
  %21 = arith.ori %20, %14 : vector<2xi1>
  %22 = arith.cmpi eq, %10, %cst_2 : vector<2xi32>
  %23 = arith.maxsi %10, %cst_0 : vector<2xi32>
  %24 = arith.minsi %23, %cst_4 : vector<2xi32>
  %25 = arith.shrui %cst_6, %24 : vector<2xi32>
  %26 = arith.select %22, %cst_0, %25 : vector<2xi1>, vector<2xi32>
  %27 = arith.maxsi %10, %cst_0 : vector<2xi32>
  %28 = arith.minsi %27, %cst_4 : vector<2xi32>
  %29 = arith.shrui %cst_7, %28 : vector<2xi32>
  %30 = arith.andi %5, %29 : vector<2xi32>
  %31 = arith.cmpi eq, %30, %26 : vector<2xi32>
  %32 = arith.cmpi sge, %10, %cst_2 : vector<2xi32>
  %33 = arith.cmpi slt, %10, %cst_3 : vector<2xi32>
  %34 = arith.andi %31, %33 : vector<2xi1>
  %35 = arith.andi %34, %32 : vector<2xi1>
  %36 = math.copysign %cst, %4 : vector<2xf32>
  %37 = arith.subf %6, %36 : vector<2xf32>
  %38 = arith.andi %21, %35 : vector<2xi1>
  %39 = arith.select %38, %37, %6 : vector<2xi1>, vector<2xf32>
  %40 = math.copysign %39, %4 : vector<2xf32>
  %41 = arith.minf %40, %cst_9 : vector<2xf32>
  %42 = arith.maxf %41, %cst_10 : vector<2xf32>
  %43 = arith.fptosi %42 : vector<2xf32> to vector<2xi8>
  vector.store %43, %1[%c0] : memref<2xi8>, vector<2xi8>
  return
}

// -----// IR Dump After IREEExpandStridedMetadata (iree-codegen-expand-strided-metadata) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant dense<0.693147182> : vector<2xf32>
  %cst_0 = arith.constant dense<1.44269502> : vector<2xf32>
  %cst_1 = arith.constant dense<1.000000e+00> : vector<2xf32>
  %cst_2 = arith.constant dense<0.499705136> : vector<2xf32>
  %cst_3 = arith.constant dense<0.168738902> : vector<2xf32>
  %cst_4 = arith.constant dense<0.0366896503> : vector<2xf32>
  %cst_5 = arith.constant dense<1.314350e-02> : vector<2xf32>
  %cst_6 = arith.constant dense<23> : vector<2xi32>
  %cst_7 = arith.constant dense<127> : vector<2xi32>
  %cst_8 = arith.constant dense<0.000000e+00> : vector<2xf32>
  %cst_9 = arith.constant dense<0x7F800000> : vector<2xf32>
  %cst_10 = arith.constant dense<0xFF800000> : vector<2xf32>
  %cst_11 = arith.constant dense<1.17549435E-38> : vector<2xf32>
  %cst_12 = arith.constant dense<-127> : vector<2xi32>
  %cst_13 = arith.constant dense<0.000000e+00> : vector<1xf32>
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %cst_14 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_15 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
  memref.assume_alignment %0, 64 : memref<2xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
  %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
  %3 = vector.extractelement %cst_15[] : vector<f32>
  %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
  %5 = vector.insertelement %4, %cst_13[%c0 : index] : vector<1xf32>
  %6 = vector.extract %5[0] : vector<1xf32>
  %7 = vector.broadcast %6 : f32 to vector<f32>
  %8 = vector.extractelement %7[] : vector<f32>
  memref.store %8, %alloca[] : memref<f32>
  %9 = memref.load %alloca[] : memref<f32>
  %10 = vector.broadcast %9 : f32 to vector<2xf32>
  %11 = vector.extractelement %cst_14[] : vector<f32>
  %12 = arith.subf %2, %10 : vector<2xf32>
  %13 = arith.cmpf uno, %12, %12 : vector<2xf32>
  %14 = arith.mulf %12, %cst_0 : vector<2xf32>
  %15 = math.floor %14 : vector<2xf32>
  %16 = arith.mulf %15, %cst : vector<2xf32>
  %17 = arith.subf %12, %16 : vector<2xf32>
  %18 = arith.mulf %17, %17 : vector<2xf32>
  %19 = arith.mulf %18, %18 : vector<2xf32>
  %20 = math.fma %cst_1, %17, %cst_1 : vector<2xf32>
  %21 = math.fma %cst_3, %17, %cst_2 : vector<2xf32>
  %22 = math.fma %cst_5, %17, %cst_4 : vector<2xf32>
  %23 = math.fma %21, %18, %20 : vector<2xf32>
  %24 = math.fma %22, %19, %23 : vector<2xf32>
  %25 = arith.fptosi %15 : vector<2xf32> to vector<2xi32>
  %26 = arith.addi %25, %cst_7 : vector<2xi32>
  %27 = arith.shli %26, %cst_6 : vector<2xi32>
  %28 = arith.bitcast %27 : vector<2xi32> to vector<2xf32>
  %29 = arith.mulf %24, %28 : vector<2xf32>
  %30 = arith.cmpi sle, %25, %cst_7 : vector<2xi32>
  %31 = arith.cmpi sge, %25, %cst_12 : vector<2xi32>
  %32 = arith.cmpf oeq, %12, %cst_10 : vector<2xf32>
  %33 = arith.cmpf oeq, %12, %cst_9 : vector<2xf32>
  %34 = arith.cmpf ogt, %12, %cst_8 : vector<2xf32>
  %35 = arith.andi %30, %31 : vector<2xi1>
  %36 = arith.select %34, %cst_9, %cst_11 : vector<2xi1>, vector<2xf32>
  %37 = arith.select %35, %29, %36 : vector<2xi1>, vector<2xf32>
  %38 = arith.select %33, %cst_9, %37 : vector<2xi1>, vector<2xf32>
  %39 = arith.select %32, %cst_8, %38 : vector<2xi1>, vector<2xf32>
  %40 = arith.select %13, %12, %39 : vector<2xi1>, vector<2xf32>
  %41 = vector.reduction <add>, %40, %11 : vector<2xf32> into f32
  %42 = vector.insertelement %41, %cst_13[%c0 : index] : vector<1xf32>
  %43 = vector.extract %42[0] : vector<1xf32>
  %44 = vector.broadcast %43 : f32 to vector<f32>
  %45 = vector.extractelement %44[] : vector<f32>
  memref.store %45, %alloca[] : memref<f32>
  %46 = memref.load %alloca[] : memref<f32>
  %47 = vector.broadcast %46 : f32 to vector<2xf32>
  %48 = arith.divf %40, %47 : vector<2xf32>
  vector.store %48, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  return
}

// -----// IR Dump After IREEExpandStridedMetadata (iree-codegen-expand-strided-metadata) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.000000e+00> : vector<2xf32>
  %cst_0 = arith.constant dense<0> : vector<2xi32>
  %cst_1 = arith.constant dense<1> : vector<2xi32>
  %cst_2 = arith.constant dense<-1> : vector<2xi32>
  %cst_3 = arith.constant dense<23> : vector<2xi32>
  %cst_4 = arith.constant dense<31> : vector<2xi32>
  %cst_5 = arith.constant dense<127> : vector<2xi32>
  %cst_6 = arith.constant dense<4194304> : vector<2xi32>
  %cst_7 = arith.constant dense<8388607> : vector<2xi32>
  %cst_8 = arith.constant dense<255> : vector<2xi32>
  %cst_9 = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_10 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_11 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
  memref.assume_alignment %1, 64 : memref<2xi8>
  %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  %3 = arith.mulf %2, %cst_11 : vector<2xf32>
  %4 = arith.addf %3, %cst_10 : vector<2xf32>
  %5 = arith.bitcast %4 : vector<2xf32> to vector<2xi32>
  %6 = math.round %4 : vector<2xf32>
  %7 = arith.bitcast %6 : vector<2xf32> to vector<2xi32>
  %8 = arith.shrui %5, %cst_3 : vector<2xi32>
  %9 = arith.andi %8, %cst_8 : vector<2xi32>
  %10 = arith.subi %9, %cst_5 : vector<2xi32>
  %11 = arith.shrui %7, %cst_3 : vector<2xi32>
  %12 = arith.andi %11, %cst_8 : vector<2xi32>
  %13 = arith.subi %12, %cst_5 : vector<2xi32>
  %14 = arith.cmpi eq, %13, %cst_0 : vector<2xi32>
  %15 = arith.subi %13, %cst_1 : vector<2xi32>
  %16 = arith.maxsi %15, %cst_0 : vector<2xi32>
  %17 = arith.minsi %16, %cst_4 : vector<2xi32>
  %18 = arith.shrui %cst_7, %17 : vector<2xi32>
  %19 = arith.andi %7, %18 : vector<2xi32>
  %20 = arith.cmpi ne, %19, %cst_0 : vector<2xi32>
  %21 = arith.ori %20, %14 : vector<2xi1>
  %22 = arith.cmpi eq, %10, %cst_2 : vector<2xi32>
  %23 = arith.maxsi %10, %cst_0 : vector<2xi32>
  %24 = arith.minsi %23, %cst_4 : vector<2xi32>
  %25 = arith.shrui %cst_6, %24 : vector<2xi32>
  %26 = arith.select %22, %cst_0, %25 : vector<2xi1>, vector<2xi32>
  %27 = arith.maxsi %10, %cst_0 : vector<2xi32>
  %28 = arith.minsi %27, %cst_4 : vector<2xi32>
  %29 = arith.shrui %cst_7, %28 : vector<2xi32>
  %30 = arith.andi %5, %29 : vector<2xi32>
  %31 = arith.cmpi eq, %30, %26 : vector<2xi32>
  %32 = arith.cmpi sge, %10, %cst_2 : vector<2xi32>
  %33 = arith.cmpi slt, %10, %cst_3 : vector<2xi32>
  %34 = arith.andi %31, %33 : vector<2xi1>
  %35 = arith.andi %34, %32 : vector<2xi1>
  %36 = math.copysign %cst, %4 : vector<2xf32>
  %37 = arith.subf %6, %36 : vector<2xf32>
  %38 = arith.andi %21, %35 : vector<2xi1>
  %39 = arith.select %38, %37, %6 : vector<2xi1>, vector<2xf32>
  %40 = math.copysign %39, %4 : vector<2xf32>
  %41 = arith.minf %40, %cst_9 : vector<2xf32>
  %42 = arith.maxf %41, %cst_10 : vector<2xf32>
  %43 = arith.fptosi %42 : vector<2xf32> to vector<2xi8>
  vector.store %43, %1[%c0] : memref<2xi8>, vector<2xi8>
  return
}

// -----// IR Dump After CleanupBufferAllocView (iree-codegen-cleanup-buffer-alloc-view) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant dense<0.693147182> : vector<2xf32>
  %cst_0 = arith.constant dense<1.44269502> : vector<2xf32>
  %cst_1 = arith.constant dense<1.000000e+00> : vector<2xf32>
  %cst_2 = arith.constant dense<0.499705136> : vector<2xf32>
  %cst_3 = arith.constant dense<0.168738902> : vector<2xf32>
  %cst_4 = arith.constant dense<0.0366896503> : vector<2xf32>
  %cst_5 = arith.constant dense<1.314350e-02> : vector<2xf32>
  %cst_6 = arith.constant dense<23> : vector<2xi32>
  %cst_7 = arith.constant dense<127> : vector<2xi32>
  %cst_8 = arith.constant dense<0.000000e+00> : vector<2xf32>
  %cst_9 = arith.constant dense<0x7F800000> : vector<2xf32>
  %cst_10 = arith.constant dense<0xFF800000> : vector<2xf32>
  %cst_11 = arith.constant dense<1.17549435E-38> : vector<2xf32>
  %cst_12 = arith.constant dense<-127> : vector<2xi32>
  %cst_13 = arith.constant dense<0.000000e+00> : vector<1xf32>
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %cst_14 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_15 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
  memref.assume_alignment %0, 64 : memref<2xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
  %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
  %3 = vector.extractelement %cst_15[] : vector<f32>
  %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
  %5 = vector.insertelement %4, %cst_13[%c0 : index] : vector<1xf32>
  %6 = vector.extract %5[0] : vector<1xf32>
  %7 = vector.broadcast %6 : f32 to vector<f32>
  %8 = vector.extractelement %7[] : vector<f32>
  memref.store %8, %alloca[] : memref<f32>
  %9 = memref.load %alloca[] : memref<f32>
  %10 = vector.broadcast %9 : f32 to vector<2xf32>
  %11 = vector.extractelement %cst_14[] : vector<f32>
  %12 = arith.subf %2, %10 : vector<2xf32>
  %13 = arith.cmpf uno, %12, %12 : vector<2xf32>
  %14 = arith.mulf %12, %cst_0 : vector<2xf32>
  %15 = math.floor %14 : vector<2xf32>
  %16 = arith.mulf %15, %cst : vector<2xf32>
  %17 = arith.subf %12, %16 : vector<2xf32>
  %18 = arith.mulf %17, %17 : vector<2xf32>
  %19 = arith.mulf %18, %18 : vector<2xf32>
  %20 = math.fma %cst_1, %17, %cst_1 : vector<2xf32>
  %21 = math.fma %cst_3, %17, %cst_2 : vector<2xf32>
  %22 = math.fma %cst_5, %17, %cst_4 : vector<2xf32>
  %23 = math.fma %21, %18, %20 : vector<2xf32>
  %24 = math.fma %22, %19, %23 : vector<2xf32>
  %25 = arith.fptosi %15 : vector<2xf32> to vector<2xi32>
  %26 = arith.addi %25, %cst_7 : vector<2xi32>
  %27 = arith.shli %26, %cst_6 : vector<2xi32>
  %28 = arith.bitcast %27 : vector<2xi32> to vector<2xf32>
  %29 = arith.mulf %24, %28 : vector<2xf32>
  %30 = arith.cmpi sle, %25, %cst_7 : vector<2xi32>
  %31 = arith.cmpi sge, %25, %cst_12 : vector<2xi32>
  %32 = arith.cmpf oeq, %12, %cst_10 : vector<2xf32>
  %33 = arith.cmpf oeq, %12, %cst_9 : vector<2xf32>
  %34 = arith.cmpf ogt, %12, %cst_8 : vector<2xf32>
  %35 = arith.andi %30, %31 : vector<2xi1>
  %36 = arith.select %34, %cst_9, %cst_11 : vector<2xi1>, vector<2xf32>
  %37 = arith.select %35, %29, %36 : vector<2xi1>, vector<2xf32>
  %38 = arith.select %33, %cst_9, %37 : vector<2xi1>, vector<2xf32>
  %39 = arith.select %32, %cst_8, %38 : vector<2xi1>, vector<2xf32>
  %40 = arith.select %13, %12, %39 : vector<2xi1>, vector<2xf32>
  %41 = vector.reduction <add>, %40, %11 : vector<2xf32> into f32
  %42 = vector.insertelement %41, %cst_13[%c0 : index] : vector<1xf32>
  %43 = vector.extract %42[0] : vector<1xf32>
  %44 = vector.broadcast %43 : f32 to vector<f32>
  %45 = vector.extractelement %44[] : vector<f32>
  memref.store %45, %alloca[] : memref<f32>
  %46 = memref.load %alloca[] : memref<f32>
  %47 = vector.broadcast %46 : f32 to vector<2xf32>
  %48 = arith.divf %40, %47 : vector<2xf32>
  vector.store %48, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  return
}

// -----// IR Dump After CleanupBufferAllocView (iree-codegen-cleanup-buffer-alloc-view) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.000000e+00> : vector<2xf32>
  %cst_0 = arith.constant dense<0> : vector<2xi32>
  %cst_1 = arith.constant dense<1> : vector<2xi32>
  %cst_2 = arith.constant dense<-1> : vector<2xi32>
  %cst_3 = arith.constant dense<23> : vector<2xi32>
  %cst_4 = arith.constant dense<31> : vector<2xi32>
  %cst_5 = arith.constant dense<127> : vector<2xi32>
  %cst_6 = arith.constant dense<4194304> : vector<2xi32>
  %cst_7 = arith.constant dense<8388607> : vector<2xi32>
  %cst_8 = arith.constant dense<255> : vector<2xi32>
  %cst_9 = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_10 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_11 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
  memref.assume_alignment %1, 64 : memref<2xi8>
  %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  %3 = arith.mulf %2, %cst_11 : vector<2xf32>
  %4 = arith.addf %3, %cst_10 : vector<2xf32>
  %5 = arith.bitcast %4 : vector<2xf32> to vector<2xi32>
  %6 = math.round %4 : vector<2xf32>
  %7 = arith.bitcast %6 : vector<2xf32> to vector<2xi32>
  %8 = arith.shrui %5, %cst_3 : vector<2xi32>
  %9 = arith.andi %8, %cst_8 : vector<2xi32>
  %10 = arith.subi %9, %cst_5 : vector<2xi32>
  %11 = arith.shrui %7, %cst_3 : vector<2xi32>
  %12 = arith.andi %11, %cst_8 : vector<2xi32>
  %13 = arith.subi %12, %cst_5 : vector<2xi32>
  %14 = arith.cmpi eq, %13, %cst_0 : vector<2xi32>
  %15 = arith.subi %13, %cst_1 : vector<2xi32>
  %16 = arith.maxsi %15, %cst_0 : vector<2xi32>
  %17 = arith.minsi %16, %cst_4 : vector<2xi32>
  %18 = arith.shrui %cst_7, %17 : vector<2xi32>
  %19 = arith.andi %7, %18 : vector<2xi32>
  %20 = arith.cmpi ne, %19, %cst_0 : vector<2xi32>
  %21 = arith.ori %20, %14 : vector<2xi1>
  %22 = arith.cmpi eq, %10, %cst_2 : vector<2xi32>
  %23 = arith.maxsi %10, %cst_0 : vector<2xi32>
  %24 = arith.minsi %23, %cst_4 : vector<2xi32>
  %25 = arith.shrui %cst_6, %24 : vector<2xi32>
  %26 = arith.select %22, %cst_0, %25 : vector<2xi1>, vector<2xi32>
  %27 = arith.maxsi %10, %cst_0 : vector<2xi32>
  %28 = arith.minsi %27, %cst_4 : vector<2xi32>
  %29 = arith.shrui %cst_7, %28 : vector<2xi32>
  %30 = arith.andi %5, %29 : vector<2xi32>
  %31 = arith.cmpi eq, %30, %26 : vector<2xi32>
  %32 = arith.cmpi sge, %10, %cst_2 : vector<2xi32>
  %33 = arith.cmpi slt, %10, %cst_3 : vector<2xi32>
  %34 = arith.andi %31, %33 : vector<2xi1>
  %35 = arith.andi %34, %32 : vector<2xi1>
  %36 = math.copysign %cst, %4 : vector<2xf32>
  %37 = arith.subf %6, %36 : vector<2xf32>
  %38 = arith.andi %21, %35 : vector<2xi1>
  %39 = arith.select %38, %37, %6 : vector<2xi1>, vector<2xf32>
  %40 = math.copysign %39, %4 : vector<2xf32>
  %41 = arith.minf %40, %cst_9 : vector<2xf32>
  %42 = arith.maxf %41, %cst_10 : vector<2xf32>
  %43 = arith.fptosi %42 : vector<2xf32> to vector<2xi8>
  vector.store %43, %1[%c0] : memref<2xi8>, vector<2xi8>
  return
}

// -----// IR Dump After LLVMCPUCheckIRBeforeLLVMConversion (iree-llvmcpu-check-ir-before-llvm-conversion) //----- //
module {
  func.func @main_dispatch_1_softmax_2xf32() {
    %cst = arith.constant dense<0.693147182> : vector<2xf32>
    %cst_0 = arith.constant dense<1.44269502> : vector<2xf32>
    %cst_1 = arith.constant dense<1.000000e+00> : vector<2xf32>
    %cst_2 = arith.constant dense<0.499705136> : vector<2xf32>
    %cst_3 = arith.constant dense<0.168738902> : vector<2xf32>
    %cst_4 = arith.constant dense<0.0366896503> : vector<2xf32>
    %cst_5 = arith.constant dense<1.314350e-02> : vector<2xf32>
    %cst_6 = arith.constant dense<23> : vector<2xi32>
    %cst_7 = arith.constant dense<127> : vector<2xi32>
    %cst_8 = arith.constant dense<0.000000e+00> : vector<2xf32>
    %cst_9 = arith.constant dense<0x7F800000> : vector<2xf32>
    %cst_10 = arith.constant dense<0xFF800000> : vector<2xf32>
    %cst_11 = arith.constant dense<1.17549435E-38> : vector<2xf32>
    %cst_12 = arith.constant dense<-127> : vector<2xi32>
    %cst_13 = arith.constant dense<0.000000e+00> : vector<1xf32>
    %c0 = arith.constant 0 : index
    %c64 = arith.constant 64 : index
    %cst_14 = arith.constant dense<0.000000e+00> : vector<f32>
    %cst_15 = arith.constant dense<-1.000000e+30> : vector<f32>
    %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
    memref.assume_alignment %0, 64 : memref<2xf32>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
    memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
    %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
    %3 = vector.extractelement %cst_15[] : vector<f32>
    %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
    %5 = vector.insertelement %4, %cst_13[%c0 : index] : vector<1xf32>
    %6 = vector.extract %5[0] : vector<1xf32>
    %7 = vector.broadcast %6 : f32 to vector<f32>
    %8 = vector.extractelement %7[] : vector<f32>
    memref.store %8, %alloca[] : memref<f32>
    %9 = memref.load %alloca[] : memref<f32>
    %10 = vector.broadcast %9 : f32 to vector<2xf32>
    %11 = vector.extractelement %cst_14[] : vector<f32>
    %12 = arith.subf %2, %10 : vector<2xf32>
    %13 = arith.cmpf uno, %12, %12 : vector<2xf32>
    %14 = arith.mulf %12, %cst_0 : vector<2xf32>
    %15 = math.floor %14 : vector<2xf32>
    %16 = arith.mulf %15, %cst : vector<2xf32>
    %17 = arith.subf %12, %16 : vector<2xf32>
    %18 = arith.mulf %17, %17 : vector<2xf32>
    %19 = arith.mulf %18, %18 : vector<2xf32>
    %20 = math.fma %cst_1, %17, %cst_1 : vector<2xf32>
    %21 = math.fma %cst_3, %17, %cst_2 : vector<2xf32>
    %22 = math.fma %cst_5, %17, %cst_4 : vector<2xf32>
    %23 = math.fma %21, %18, %20 : vector<2xf32>
    %24 = math.fma %22, %19, %23 : vector<2xf32>
    %25 = arith.fptosi %15 : vector<2xf32> to vector<2xi32>
    %26 = arith.addi %25, %cst_7 : vector<2xi32>
    %27 = arith.shli %26, %cst_6 : vector<2xi32>
    %28 = arith.bitcast %27 : vector<2xi32> to vector<2xf32>
    %29 = arith.mulf %24, %28 : vector<2xf32>
    %30 = arith.cmpi sle, %25, %cst_7 : vector<2xi32>
    %31 = arith.cmpi sge, %25, %cst_12 : vector<2xi32>
    %32 = arith.cmpf oeq, %12, %cst_10 : vector<2xf32>
    %33 = arith.cmpf oeq, %12, %cst_9 : vector<2xf32>
    %34 = arith.cmpf ogt, %12, %cst_8 : vector<2xf32>
    %35 = arith.andi %30, %31 : vector<2xi1>
    %36 = arith.select %34, %cst_9, %cst_11 : vector<2xi1>, vector<2xf32>
    %37 = arith.select %35, %29, %36 : vector<2xi1>, vector<2xf32>
    %38 = arith.select %33, %cst_9, %37 : vector<2xi1>, vector<2xf32>
    %39 = arith.select %32, %cst_8, %38 : vector<2xi1>, vector<2xf32>
    %40 = arith.select %13, %12, %39 : vector<2xi1>, vector<2xf32>
    %41 = vector.reduction <add>, %40, %11 : vector<2xf32> into f32
    %42 = vector.insertelement %41, %cst_13[%c0 : index] : vector<1xf32>
    %43 = vector.extract %42[0] : vector<1xf32>
    %44 = vector.broadcast %43 : f32 to vector<f32>
    %45 = vector.extractelement %44[] : vector<f32>
    memref.store %45, %alloca[] : memref<f32>
    %46 = memref.load %alloca[] : memref<f32>
    %47 = vector.broadcast %46 : f32 to vector<2xf32>
    %48 = arith.divf %40, %47 : vector<2xf32>
    vector.store %48, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
    return
  }
}

// -----// IR Dump After LLVMCPUCheckIRBeforeLLVMConversion (iree-llvmcpu-check-ir-before-llvm-conversion) //----- //
module {
  func.func @main_dispatch_2_generic_2_f32xi8() {
    %cst = arith.constant dense<1.000000e+00> : vector<2xf32>
    %cst_0 = arith.constant dense<0> : vector<2xi32>
    %cst_1 = arith.constant dense<1> : vector<2xi32>
    %cst_2 = arith.constant dense<-1> : vector<2xi32>
    %cst_3 = arith.constant dense<23> : vector<2xi32>
    %cst_4 = arith.constant dense<31> : vector<2xi32>
    %cst_5 = arith.constant dense<127> : vector<2xi32>
    %cst_6 = arith.constant dense<4194304> : vector<2xi32>
    %cst_7 = arith.constant dense<8388607> : vector<2xi32>
    %cst_8 = arith.constant dense<255> : vector<2xi32>
    %cst_9 = arith.constant dense<1.270000e+02> : vector<2xf32>
    %cst_10 = arith.constant dense<-1.280000e+02> : vector<2xf32>
    %cst_11 = arith.constant dense<2.560000e+02> : vector<2xf32>
    %c64 = arith.constant 64 : index
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
    memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
    %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
    memref.assume_alignment %1, 64 : memref<2xi8>
    %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
    %3 = arith.mulf %2, %cst_11 : vector<2xf32>
    %4 = arith.addf %3, %cst_10 : vector<2xf32>
    %5 = arith.bitcast %4 : vector<2xf32> to vector<2xi32>
    %6 = math.round %4 : vector<2xf32>
    %7 = arith.bitcast %6 : vector<2xf32> to vector<2xi32>
    %8 = arith.shrui %5, %cst_3 : vector<2xi32>
    %9 = arith.andi %8, %cst_8 : vector<2xi32>
    %10 = arith.subi %9, %cst_5 : vector<2xi32>
    %11 = arith.shrui %7, %cst_3 : vector<2xi32>
    %12 = arith.andi %11, %cst_8 : vector<2xi32>
    %13 = arith.subi %12, %cst_5 : vector<2xi32>
    %14 = arith.cmpi eq, %13, %cst_0 : vector<2xi32>
    %15 = arith.subi %13, %cst_1 : vector<2xi32>
    %16 = arith.maxsi %15, %cst_0 : vector<2xi32>
    %17 = arith.minsi %16, %cst_4 : vector<2xi32>
    %18 = arith.shrui %cst_7, %17 : vector<2xi32>
    %19 = arith.andi %7, %18 : vector<2xi32>
    %20 = arith.cmpi ne, %19, %cst_0 : vector<2xi32>
    %21 = arith.ori %20, %14 : vector<2xi1>
    %22 = arith.cmpi eq, %10, %cst_2 : vector<2xi32>
    %23 = arith.maxsi %10, %cst_0 : vector<2xi32>
    %24 = arith.minsi %23, %cst_4 : vector<2xi32>
    %25 = arith.shrui %cst_6, %24 : vector<2xi32>
    %26 = arith.select %22, %cst_0, %25 : vector<2xi1>, vector<2xi32>
    %27 = arith.maxsi %10, %cst_0 : vector<2xi32>
    %28 = arith.minsi %27, %cst_4 : vector<2xi32>
    %29 = arith.shrui %cst_7, %28 : vector<2xi32>
    %30 = arith.andi %5, %29 : vector<2xi32>
    %31 = arith.cmpi eq, %30, %26 : vector<2xi32>
    %32 = arith.cmpi sge, %10, %cst_2 : vector<2xi32>
    %33 = arith.cmpi slt, %10, %cst_3 : vector<2xi32>
    %34 = arith.andi %31, %33 : vector<2xi1>
    %35 = arith.andi %34, %32 : vector<2xi1>
    %36 = math.copysign %cst, %4 : vector<2xf32>
    %37 = arith.subf %6, %36 : vector<2xf32>
    %38 = arith.andi %21, %35 : vector<2xi1>
    %39 = arith.select %38, %37, %6 : vector<2xi1>, vector<2xf32>
    %40 = math.copysign %39, %4 : vector<2xf32>
    %41 = arith.minf %40, %cst_9 : vector<2xf32>
    %42 = arith.maxf %41, %cst_10 : vector<2xf32>
    %43 = arith.fptosi %42 : vector<2xf32> to vector<2xi8>
    vector.store %43, %1[%c0] : memref<2xi8>, vector<2xi8>
    return
  }
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant dense<0.693147182> : vector<2xf32>
  %cst_0 = arith.constant dense<1.44269502> : vector<2xf32>
  %cst_1 = arith.constant dense<1.000000e+00> : vector<2xf32>
  %cst_2 = arith.constant dense<0.499705136> : vector<2xf32>
  %cst_3 = arith.constant dense<0.168738902> : vector<2xf32>
  %cst_4 = arith.constant dense<0.0366896503> : vector<2xf32>
  %cst_5 = arith.constant dense<1.314350e-02> : vector<2xf32>
  %cst_6 = arith.constant dense<23> : vector<2xi32>
  %cst_7 = arith.constant dense<127> : vector<2xi32>
  %cst_8 = arith.constant dense<0.000000e+00> : vector<2xf32>
  %cst_9 = arith.constant dense<0x7F800000> : vector<2xf32>
  %cst_10 = arith.constant dense<0xFF800000> : vector<2xf32>
  %cst_11 = arith.constant dense<1.17549435E-38> : vector<2xf32>
  %cst_12 = arith.constant dense<-127> : vector<2xi32>
  %cst_13 = arith.constant dense<0.000000e+00> : vector<1xf32>
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %cst_14 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_15 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
  memref.assume_alignment %0, 64 : memref<2xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
  %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
  %3 = vector.extractelement %cst_15[] : vector<f32>
  %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
  %5 = vector.insertelement %4, %cst_13[%c0 : index] : vector<1xf32>
  %6 = vector.extract %5[0] : vector<1xf32>
  %7 = vector.broadcast %6 : f32 to vector<f32>
  %8 = vector.extractelement %7[] : vector<f32>
  memref.store %8, %alloca[] : memref<f32>
  %9 = memref.load %alloca[] : memref<f32>
  %10 = vector.broadcast %9 : f32 to vector<2xf32>
  %11 = vector.extractelement %cst_14[] : vector<f32>
  %12 = arith.subf %2, %10 : vector<2xf32>
  %13 = arith.cmpf uno, %12, %12 : vector<2xf32>
  %14 = arith.mulf %12, %cst_0 : vector<2xf32>
  %15 = math.floor %14 : vector<2xf32>
  %16 = arith.mulf %15, %cst : vector<2xf32>
  %17 = arith.subf %12, %16 : vector<2xf32>
  %18 = arith.mulf %17, %17 : vector<2xf32>
  %19 = arith.mulf %18, %18 : vector<2xf32>
  %20 = math.fma %cst_1, %17, %cst_1 : vector<2xf32>
  %21 = math.fma %cst_3, %17, %cst_2 : vector<2xf32>
  %22 = math.fma %cst_5, %17, %cst_4 : vector<2xf32>
  %23 = math.fma %21, %18, %20 : vector<2xf32>
  %24 = math.fma %22, %19, %23 : vector<2xf32>
  %25 = arith.fptosi %15 : vector<2xf32> to vector<2xi32>
  %26 = arith.addi %25, %cst_7 : vector<2xi32>
  %27 = arith.shli %26, %cst_6 : vector<2xi32>
  %28 = arith.bitcast %27 : vector<2xi32> to vector<2xf32>
  %29 = arith.mulf %24, %28 : vector<2xf32>
  %30 = arith.cmpi sle, %25, %cst_7 : vector<2xi32>
  %31 = arith.cmpi sge, %25, %cst_12 : vector<2xi32>
  %32 = arith.cmpf oeq, %12, %cst_10 : vector<2xf32>
  %33 = arith.cmpf oeq, %12, %cst_9 : vector<2xf32>
  %34 = arith.cmpf ogt, %12, %cst_8 : vector<2xf32>
  %35 = arith.andi %30, %31 : vector<2xi1>
  %36 = arith.select %34, %cst_9, %cst_11 : vector<2xi1>, vector<2xf32>
  %37 = arith.select %35, %29, %36 : vector<2xi1>, vector<2xf32>
  %38 = arith.select %33, %cst_9, %37 : vector<2xi1>, vector<2xf32>
  %39 = arith.select %32, %cst_8, %38 : vector<2xi1>, vector<2xf32>
  %40 = arith.select %13, %12, %39 : vector<2xi1>, vector<2xf32>
  %41 = vector.reduction <add>, %40, %11 : vector<2xf32> into f32
  %42 = vector.insertelement %41, %cst_13[%c0 : index] : vector<1xf32>
  %43 = vector.extract %42[0] : vector<1xf32>
  %44 = vector.broadcast %43 : f32 to vector<f32>
  %45 = vector.extractelement %44[] : vector<f32>
  memref.store %45, %alloca[] : memref<f32>
  %46 = memref.load %alloca[] : memref<f32>
  %47 = vector.broadcast %46 : f32 to vector<2xf32>
  %48 = arith.divf %40, %47 : vector<2xf32>
  vector.store %48, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.000000e+00> : vector<2xf32>
  %cst_0 = arith.constant dense<0> : vector<2xi32>
  %cst_1 = arith.constant dense<1> : vector<2xi32>
  %cst_2 = arith.constant dense<-1> : vector<2xi32>
  %cst_3 = arith.constant dense<23> : vector<2xi32>
  %cst_4 = arith.constant dense<31> : vector<2xi32>
  %cst_5 = arith.constant dense<127> : vector<2xi32>
  %cst_6 = arith.constant dense<4194304> : vector<2xi32>
  %cst_7 = arith.constant dense<8388607> : vector<2xi32>
  %cst_8 = arith.constant dense<255> : vector<2xi32>
  %cst_9 = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_10 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_11 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
  memref.assume_alignment %1, 64 : memref<2xi8>
  %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  %3 = arith.mulf %2, %cst_11 : vector<2xf32>
  %4 = arith.addf %3, %cst_10 : vector<2xf32>
  %5 = arith.bitcast %4 : vector<2xf32> to vector<2xi32>
  %6 = math.round %4 : vector<2xf32>
  %7 = arith.bitcast %6 : vector<2xf32> to vector<2xi32>
  %8 = arith.shrui %5, %cst_3 : vector<2xi32>
  %9 = arith.andi %8, %cst_8 : vector<2xi32>
  %10 = arith.subi %9, %cst_5 : vector<2xi32>
  %11 = arith.shrui %7, %cst_3 : vector<2xi32>
  %12 = arith.andi %11, %cst_8 : vector<2xi32>
  %13 = arith.subi %12, %cst_5 : vector<2xi32>
  %14 = arith.cmpi eq, %13, %cst_0 : vector<2xi32>
  %15 = arith.subi %13, %cst_1 : vector<2xi32>
  %16 = arith.maxsi %15, %cst_0 : vector<2xi32>
  %17 = arith.minsi %16, %cst_4 : vector<2xi32>
  %18 = arith.shrui %cst_7, %17 : vector<2xi32>
  %19 = arith.andi %7, %18 : vector<2xi32>
  %20 = arith.cmpi ne, %19, %cst_0 : vector<2xi32>
  %21 = arith.ori %20, %14 : vector<2xi1>
  %22 = arith.cmpi eq, %10, %cst_2 : vector<2xi32>
  %23 = arith.maxsi %10, %cst_0 : vector<2xi32>
  %24 = arith.minsi %23, %cst_4 : vector<2xi32>
  %25 = arith.shrui %cst_6, %24 : vector<2xi32>
  %26 = arith.select %22, %cst_0, %25 : vector<2xi1>, vector<2xi32>
  %27 = arith.maxsi %10, %cst_0 : vector<2xi32>
  %28 = arith.minsi %27, %cst_4 : vector<2xi32>
  %29 = arith.shrui %cst_7, %28 : vector<2xi32>
  %30 = arith.andi %5, %29 : vector<2xi32>
  %31 = arith.cmpi eq, %30, %26 : vector<2xi32>
  %32 = arith.cmpi sge, %10, %cst_2 : vector<2xi32>
  %33 = arith.cmpi slt, %10, %cst_3 : vector<2xi32>
  %34 = arith.andi %31, %33 : vector<2xi1>
  %35 = arith.andi %34, %32 : vector<2xi1>
  %36 = math.copysign %cst, %4 : vector<2xf32>
  %37 = arith.subf %6, %36 : vector<2xf32>
  %38 = arith.andi %21, %35 : vector<2xi1>
  %39 = arith.select %38, %37, %6 : vector<2xi1>, vector<2xf32>
  %40 = math.copysign %39, %4 : vector<2xf32>
  %41 = arith.minf %40, %cst_9 : vector<2xf32>
  %42 = arith.maxf %41, %cst_10 : vector<2xf32>
  %43 = arith.fptosi %42 : vector<2xf32> to vector<2xi8>
  vector.store %43, %1[%c0] : memref<2xi8>, vector<2xi8>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant dense<0.693147182> : vector<2xf32>
  %cst_0 = arith.constant dense<1.44269502> : vector<2xf32>
  %cst_1 = arith.constant dense<1.000000e+00> : vector<2xf32>
  %cst_2 = arith.constant dense<0.499705136> : vector<2xf32>
  %cst_3 = arith.constant dense<0.168738902> : vector<2xf32>
  %cst_4 = arith.constant dense<0.0366896503> : vector<2xf32>
  %cst_5 = arith.constant dense<1.314350e-02> : vector<2xf32>
  %cst_6 = arith.constant dense<23> : vector<2xi32>
  %cst_7 = arith.constant dense<127> : vector<2xi32>
  %cst_8 = arith.constant dense<0.000000e+00> : vector<2xf32>
  %cst_9 = arith.constant dense<0x7F800000> : vector<2xf32>
  %cst_10 = arith.constant dense<0xFF800000> : vector<2xf32>
  %cst_11 = arith.constant dense<1.17549435E-38> : vector<2xf32>
  %cst_12 = arith.constant dense<-127> : vector<2xi32>
  %cst_13 = arith.constant dense<0.000000e+00> : vector<1xf32>
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %cst_14 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_15 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
  memref.assume_alignment %0, 64 : memref<2xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
  %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
  %3 = vector.extractelement %cst_15[] : vector<f32>
  %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
  %5 = vector.insertelement %4, %cst_13[%c0 : index] : vector<1xf32>
  %6 = vector.extract %5[0] : vector<1xf32>
  %7 = vector.broadcast %6 : f32 to vector<f32>
  %8 = vector.extractelement %7[] : vector<f32>
  memref.store %8, %alloca[] : memref<f32>
  %9 = memref.load %alloca[] : memref<f32>
  %10 = vector.broadcast %9 : f32 to vector<2xf32>
  %11 = vector.extractelement %cst_14[] : vector<f32>
  %12 = arith.subf %2, %10 : vector<2xf32>
  %13 = arith.cmpf uno, %12, %12 : vector<2xf32>
  %14 = arith.mulf %12, %cst_0 : vector<2xf32>
  %15 = math.floor %14 : vector<2xf32>
  %16 = arith.mulf %15, %cst : vector<2xf32>
  %17 = arith.subf %12, %16 : vector<2xf32>
  %18 = arith.mulf %17, %17 : vector<2xf32>
  %19 = arith.mulf %18, %18 : vector<2xf32>
  %20 = math.fma %cst_1, %17, %cst_1 : vector<2xf32>
  %21 = math.fma %cst_3, %17, %cst_2 : vector<2xf32>
  %22 = math.fma %cst_5, %17, %cst_4 : vector<2xf32>
  %23 = math.fma %21, %18, %20 : vector<2xf32>
  %24 = math.fma %22, %19, %23 : vector<2xf32>
  %25 = arith.fptosi %15 : vector<2xf32> to vector<2xi32>
  %26 = arith.addi %25, %cst_7 : vector<2xi32>
  %27 = arith.shli %26, %cst_6 : vector<2xi32>
  %28 = arith.bitcast %27 : vector<2xi32> to vector<2xf32>
  %29 = arith.mulf %24, %28 : vector<2xf32>
  %30 = arith.cmpi sle, %25, %cst_7 : vector<2xi32>
  %31 = arith.cmpi sge, %25, %cst_12 : vector<2xi32>
  %32 = arith.cmpf oeq, %12, %cst_10 : vector<2xf32>
  %33 = arith.cmpf oeq, %12, %cst_9 : vector<2xf32>
  %34 = arith.cmpf ogt, %12, %cst_8 : vector<2xf32>
  %35 = arith.andi %30, %31 : vector<2xi1>
  %36 = arith.select %34, %cst_9, %cst_11 : vector<2xi1>, vector<2xf32>
  %37 = arith.select %35, %29, %36 : vector<2xi1>, vector<2xf32>
  %38 = arith.select %33, %cst_9, %37 : vector<2xi1>, vector<2xf32>
  %39 = arith.select %32, %cst_8, %38 : vector<2xi1>, vector<2xf32>
  %40 = arith.select %13, %12, %39 : vector<2xi1>, vector<2xf32>
  %41 = vector.reduction <add>, %40, %11 : vector<2xf32> into f32
  %42 = vector.insertelement %41, %cst_13[%c0 : index] : vector<1xf32>
  %43 = vector.extract %42[0] : vector<1xf32>
  %44 = vector.broadcast %43 : f32 to vector<f32>
  %45 = vector.extractelement %44[] : vector<f32>
  memref.store %45, %alloca[] : memref<f32>
  %46 = memref.load %alloca[] : memref<f32>
  %47 = vector.broadcast %46 : f32 to vector<2xf32>
  %48 = arith.divf %40, %47 : vector<2xf32>
  vector.store %48, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.000000e+00> : vector<2xf32>
  %cst_0 = arith.constant dense<0> : vector<2xi32>
  %cst_1 = arith.constant dense<1> : vector<2xi32>
  %cst_2 = arith.constant dense<-1> : vector<2xi32>
  %cst_3 = arith.constant dense<23> : vector<2xi32>
  %cst_4 = arith.constant dense<31> : vector<2xi32>
  %cst_5 = arith.constant dense<127> : vector<2xi32>
  %cst_6 = arith.constant dense<4194304> : vector<2xi32>
  %cst_7 = arith.constant dense<8388607> : vector<2xi32>
  %cst_8 = arith.constant dense<255> : vector<2xi32>
  %cst_9 = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_10 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_11 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
  memref.assume_alignment %1, 64 : memref<2xi8>
  %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  %3 = arith.mulf %2, %cst_11 : vector<2xf32>
  %4 = arith.addf %3, %cst_10 : vector<2xf32>
  %5 = arith.bitcast %4 : vector<2xf32> to vector<2xi32>
  %6 = math.round %4 : vector<2xf32>
  %7 = arith.bitcast %6 : vector<2xf32> to vector<2xi32>
  %8 = arith.shrui %5, %cst_3 : vector<2xi32>
  %9 = arith.andi %8, %cst_8 : vector<2xi32>
  %10 = arith.subi %9, %cst_5 : vector<2xi32>
  %11 = arith.shrui %7, %cst_3 : vector<2xi32>
  %12 = arith.andi %11, %cst_8 : vector<2xi32>
  %13 = arith.subi %12, %cst_5 : vector<2xi32>
  %14 = arith.cmpi eq, %13, %cst_0 : vector<2xi32>
  %15 = arith.subi %13, %cst_1 : vector<2xi32>
  %16 = arith.maxsi %15, %cst_0 : vector<2xi32>
  %17 = arith.minsi %16, %cst_4 : vector<2xi32>
  %18 = arith.shrui %cst_7, %17 : vector<2xi32>
  %19 = arith.andi %7, %18 : vector<2xi32>
  %20 = arith.cmpi ne, %19, %cst_0 : vector<2xi32>
  %21 = arith.ori %20, %14 : vector<2xi1>
  %22 = arith.cmpi eq, %10, %cst_2 : vector<2xi32>
  %23 = arith.maxsi %10, %cst_0 : vector<2xi32>
  %24 = arith.minsi %23, %cst_4 : vector<2xi32>
  %25 = arith.shrui %cst_6, %24 : vector<2xi32>
  %26 = arith.select %22, %cst_0, %25 : vector<2xi1>, vector<2xi32>
  %27 = arith.maxsi %10, %cst_0 : vector<2xi32>
  %28 = arith.minsi %27, %cst_4 : vector<2xi32>
  %29 = arith.shrui %cst_7, %28 : vector<2xi32>
  %30 = arith.andi %5, %29 : vector<2xi32>
  %31 = arith.cmpi eq, %30, %26 : vector<2xi32>
  %32 = arith.cmpi sge, %10, %cst_2 : vector<2xi32>
  %33 = arith.cmpi slt, %10, %cst_3 : vector<2xi32>
  %34 = arith.andi %31, %33 : vector<2xi1>
  %35 = arith.andi %34, %32 : vector<2xi1>
  %36 = math.copysign %cst, %4 : vector<2xf32>
  %37 = arith.subf %6, %36 : vector<2xf32>
  %38 = arith.andi %21, %35 : vector<2xi1>
  %39 = arith.select %38, %37, %6 : vector<2xi1>, vector<2xf32>
  %40 = math.copysign %39, %4 : vector<2xf32>
  %41 = arith.minf %40, %cst_9 : vector<2xf32>
  %42 = arith.maxf %41, %cst_10 : vector<2xf32>
  %43 = arith.fptosi %42 : vector<2xf32> to vector<2xi8>
  vector.store %43, %1[%c0] : memref<2xi8>, vector<2xi8>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant dense<0.693147182> : vector<2xf32>
  %cst_0 = arith.constant dense<1.44269502> : vector<2xf32>
  %cst_1 = arith.constant dense<1.000000e+00> : vector<2xf32>
  %cst_2 = arith.constant dense<0.499705136> : vector<2xf32>
  %cst_3 = arith.constant dense<0.168738902> : vector<2xf32>
  %cst_4 = arith.constant dense<0.0366896503> : vector<2xf32>
  %cst_5 = arith.constant dense<1.314350e-02> : vector<2xf32>
  %cst_6 = arith.constant dense<23> : vector<2xi32>
  %cst_7 = arith.constant dense<127> : vector<2xi32>
  %cst_8 = arith.constant dense<0.000000e+00> : vector<2xf32>
  %cst_9 = arith.constant dense<0x7F800000> : vector<2xf32>
  %cst_10 = arith.constant dense<0xFF800000> : vector<2xf32>
  %cst_11 = arith.constant dense<1.17549435E-38> : vector<2xf32>
  %cst_12 = arith.constant dense<-127> : vector<2xi32>
  %cst_13 = arith.constant dense<0.000000e+00> : vector<1xf32>
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %cst_14 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_15 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
  memref.assume_alignment %0, 64 : memref<2xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
  %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
  %3 = vector.extractelement %cst_15[] : vector<f32>
  %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
  %5 = vector.insertelement %4, %cst_13[%c0 : index] : vector<1xf32>
  %6 = vector.extract %5[0] : vector<1xf32>
  %7 = vector.broadcast %6 : f32 to vector<f32>
  %8 = vector.extractelement %7[] : vector<f32>
  memref.store %8, %alloca[] : memref<f32>
  %9 = memref.load %alloca[] : memref<f32>
  %10 = vector.broadcast %9 : f32 to vector<2xf32>
  %11 = vector.extractelement %cst_14[] : vector<f32>
  %12 = arith.subf %2, %10 : vector<2xf32>
  %13 = arith.cmpf uno, %12, %12 : vector<2xf32>
  %14 = arith.mulf %12, %cst_0 : vector<2xf32>
  %15 = math.floor %14 : vector<2xf32>
  %16 = arith.mulf %15, %cst : vector<2xf32>
  %17 = arith.subf %12, %16 : vector<2xf32>
  %18 = arith.mulf %17, %17 : vector<2xf32>
  %19 = arith.mulf %18, %18 : vector<2xf32>
  %20 = math.fma %cst_1, %17, %cst_1 : vector<2xf32>
  %21 = math.fma %cst_3, %17, %cst_2 : vector<2xf32>
  %22 = math.fma %cst_5, %17, %cst_4 : vector<2xf32>
  %23 = math.fma %21, %18, %20 : vector<2xf32>
  %24 = math.fma %22, %19, %23 : vector<2xf32>
  %25 = arith.fptosi %15 : vector<2xf32> to vector<2xi32>
  %26 = arith.addi %25, %cst_7 : vector<2xi32>
  %27 = arith.shli %26, %cst_6 : vector<2xi32>
  %28 = arith.bitcast %27 : vector<2xi32> to vector<2xf32>
  %29 = arith.mulf %24, %28 : vector<2xf32>
  %30 = arith.cmpi sle, %25, %cst_7 : vector<2xi32>
  %31 = arith.cmpi sge, %25, %cst_12 : vector<2xi32>
  %32 = arith.cmpf oeq, %12, %cst_10 : vector<2xf32>
  %33 = arith.cmpf oeq, %12, %cst_9 : vector<2xf32>
  %34 = arith.cmpf ogt, %12, %cst_8 : vector<2xf32>
  %35 = arith.andi %30, %31 : vector<2xi1>
  %36 = arith.select %34, %cst_9, %cst_11 : vector<2xi1>, vector<2xf32>
  %37 = arith.select %35, %29, %36 : vector<2xi1>, vector<2xf32>
  %38 = arith.select %33, %cst_9, %37 : vector<2xi1>, vector<2xf32>
  %39 = arith.select %32, %cst_8, %38 : vector<2xi1>, vector<2xf32>
  %40 = arith.select %13, %12, %39 : vector<2xi1>, vector<2xf32>
  %41 = vector.reduction <add>, %40, %11 : vector<2xf32> into f32
  %42 = vector.insertelement %41, %cst_13[%c0 : index] : vector<1xf32>
  %43 = vector.extract %42[0] : vector<1xf32>
  %44 = vector.broadcast %43 : f32 to vector<f32>
  %45 = vector.extractelement %44[] : vector<f32>
  memref.store %45, %alloca[] : memref<f32>
  %46 = memref.load %alloca[] : memref<f32>
  %47 = vector.broadcast %46 : f32 to vector<2xf32>
  %48 = arith.divf %40, %47 : vector<2xf32>
  vector.store %48, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.000000e+00> : vector<2xf32>
  %cst_0 = arith.constant dense<0> : vector<2xi32>
  %cst_1 = arith.constant dense<1> : vector<2xi32>
  %cst_2 = arith.constant dense<-1> : vector<2xi32>
  %cst_3 = arith.constant dense<23> : vector<2xi32>
  %cst_4 = arith.constant dense<31> : vector<2xi32>
  %cst_5 = arith.constant dense<127> : vector<2xi32>
  %cst_6 = arith.constant dense<4194304> : vector<2xi32>
  %cst_7 = arith.constant dense<8388607> : vector<2xi32>
  %cst_8 = arith.constant dense<255> : vector<2xi32>
  %cst_9 = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_10 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_11 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
  memref.assume_alignment %1, 64 : memref<2xi8>
  %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  %3 = arith.mulf %2, %cst_11 : vector<2xf32>
  %4 = arith.addf %3, %cst_10 : vector<2xf32>
  %5 = arith.bitcast %4 : vector<2xf32> to vector<2xi32>
  %6 = math.round %4 : vector<2xf32>
  %7 = arith.bitcast %6 : vector<2xf32> to vector<2xi32>
  %8 = arith.shrui %5, %cst_3 : vector<2xi32>
  %9 = arith.andi %8, %cst_8 : vector<2xi32>
  %10 = arith.subi %9, %cst_5 : vector<2xi32>
  %11 = arith.shrui %7, %cst_3 : vector<2xi32>
  %12 = arith.andi %11, %cst_8 : vector<2xi32>
  %13 = arith.subi %12, %cst_5 : vector<2xi32>
  %14 = arith.cmpi eq, %13, %cst_0 : vector<2xi32>
  %15 = arith.subi %13, %cst_1 : vector<2xi32>
  %16 = arith.maxsi %15, %cst_0 : vector<2xi32>
  %17 = arith.minsi %16, %cst_4 : vector<2xi32>
  %18 = arith.shrui %cst_7, %17 : vector<2xi32>
  %19 = arith.andi %7, %18 : vector<2xi32>
  %20 = arith.cmpi ne, %19, %cst_0 : vector<2xi32>
  %21 = arith.ori %20, %14 : vector<2xi1>
  %22 = arith.cmpi eq, %10, %cst_2 : vector<2xi32>
  %23 = arith.maxsi %10, %cst_0 : vector<2xi32>
  %24 = arith.minsi %23, %cst_4 : vector<2xi32>
  %25 = arith.shrui %cst_6, %24 : vector<2xi32>
  %26 = arith.select %22, %cst_0, %25 : vector<2xi1>, vector<2xi32>
  %27 = arith.shrui %cst_7, %24 : vector<2xi32>
  %28 = arith.andi %5, %27 : vector<2xi32>
  %29 = arith.cmpi eq, %28, %26 : vector<2xi32>
  %30 = arith.cmpi sge, %10, %cst_2 : vector<2xi32>
  %31 = arith.cmpi slt, %10, %cst_3 : vector<2xi32>
  %32 = arith.andi %29, %31 : vector<2xi1>
  %33 = arith.andi %32, %30 : vector<2xi1>
  %34 = math.copysign %cst, %4 : vector<2xf32>
  %35 = arith.subf %6, %34 : vector<2xf32>
  %36 = arith.andi %21, %33 : vector<2xi1>
  %37 = arith.select %36, %35, %6 : vector<2xi1>, vector<2xf32>
  %38 = math.copysign %37, %4 : vector<2xf32>
  %39 = arith.minf %38, %cst_9 : vector<2xf32>
  %40 = arith.maxf %39, %cst_10 : vector<2xf32>
  %41 = arith.fptosi %40 : vector<2xf32> to vector<2xi8>
  vector.store %41, %1[%c0] : memref<2xi8>, vector<2xi8>
  return
}

// -----// IR Dump After ArithExpandOps (arith-expand) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant dense<0.693147182> : vector<2xf32>
  %cst_0 = arith.constant dense<1.44269502> : vector<2xf32>
  %cst_1 = arith.constant dense<1.000000e+00> : vector<2xf32>
  %cst_2 = arith.constant dense<0.499705136> : vector<2xf32>
  %cst_3 = arith.constant dense<0.168738902> : vector<2xf32>
  %cst_4 = arith.constant dense<0.0366896503> : vector<2xf32>
  %cst_5 = arith.constant dense<1.314350e-02> : vector<2xf32>
  %cst_6 = arith.constant dense<23> : vector<2xi32>
  %cst_7 = arith.constant dense<127> : vector<2xi32>
  %cst_8 = arith.constant dense<0.000000e+00> : vector<2xf32>
  %cst_9 = arith.constant dense<0x7F800000> : vector<2xf32>
  %cst_10 = arith.constant dense<0xFF800000> : vector<2xf32>
  %cst_11 = arith.constant dense<1.17549435E-38> : vector<2xf32>
  %cst_12 = arith.constant dense<-127> : vector<2xi32>
  %cst_13 = arith.constant dense<0.000000e+00> : vector<1xf32>
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %cst_14 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_15 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
  memref.assume_alignment %0, 64 : memref<2xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
  %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
  %3 = vector.extractelement %cst_15[] : vector<f32>
  %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
  %5 = vector.insertelement %4, %cst_13[%c0 : index] : vector<1xf32>
  %6 = vector.extract %5[0] : vector<1xf32>
  %7 = vector.broadcast %6 : f32 to vector<f32>
  %8 = vector.extractelement %7[] : vector<f32>
  memref.store %8, %alloca[] : memref<f32>
  %9 = memref.load %alloca[] : memref<f32>
  %10 = vector.broadcast %9 : f32 to vector<2xf32>
  %11 = vector.extractelement %cst_14[] : vector<f32>
  %12 = arith.subf %2, %10 : vector<2xf32>
  %13 = arith.cmpf uno, %12, %12 : vector<2xf32>
  %14 = arith.mulf %12, %cst_0 : vector<2xf32>
  %15 = math.floor %14 : vector<2xf32>
  %16 = arith.mulf %15, %cst : vector<2xf32>
  %17 = arith.subf %12, %16 : vector<2xf32>
  %18 = arith.mulf %17, %17 : vector<2xf32>
  %19 = arith.mulf %18, %18 : vector<2xf32>
  %20 = math.fma %cst_1, %17, %cst_1 : vector<2xf32>
  %21 = math.fma %cst_3, %17, %cst_2 : vector<2xf32>
  %22 = math.fma %cst_5, %17, %cst_4 : vector<2xf32>
  %23 = math.fma %21, %18, %20 : vector<2xf32>
  %24 = math.fma %22, %19, %23 : vector<2xf32>
  %25 = arith.fptosi %15 : vector<2xf32> to vector<2xi32>
  %26 = arith.addi %25, %cst_7 : vector<2xi32>
  %27 = arith.shli %26, %cst_6 : vector<2xi32>
  %28 = arith.bitcast %27 : vector<2xi32> to vector<2xf32>
  %29 = arith.mulf %24, %28 : vector<2xf32>
  %30 = arith.cmpi sle, %25, %cst_7 : vector<2xi32>
  %31 = arith.cmpi sge, %25, %cst_12 : vector<2xi32>
  %32 = arith.cmpf oeq, %12, %cst_10 : vector<2xf32>
  %33 = arith.cmpf oeq, %12, %cst_9 : vector<2xf32>
  %34 = arith.cmpf ogt, %12, %cst_8 : vector<2xf32>
  %35 = arith.andi %30, %31 : vector<2xi1>
  %36 = arith.select %34, %cst_9, %cst_11 : vector<2xi1>, vector<2xf32>
  %37 = arith.select %35, %29, %36 : vector<2xi1>, vector<2xf32>
  %38 = arith.select %33, %cst_9, %37 : vector<2xi1>, vector<2xf32>
  %39 = arith.select %32, %cst_8, %38 : vector<2xi1>, vector<2xf32>
  %40 = arith.select %13, %12, %39 : vector<2xi1>, vector<2xf32>
  %41 = vector.reduction <add>, %40, %11 : vector<2xf32> into f32
  %42 = vector.insertelement %41, %cst_13[%c0 : index] : vector<1xf32>
  %43 = vector.extract %42[0] : vector<1xf32>
  %44 = vector.broadcast %43 : f32 to vector<f32>
  %45 = vector.extractelement %44[] : vector<f32>
  memref.store %45, %alloca[] : memref<f32>
  %46 = memref.load %alloca[] : memref<f32>
  %47 = vector.broadcast %46 : f32 to vector<2xf32>
  %48 = arith.divf %40, %47 : vector<2xf32>
  vector.store %48, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  return
}

// -----// IR Dump After ArithExpandOps (arith-expand) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.000000e+00> : vector<2xf32>
  %cst_0 = arith.constant dense<0> : vector<2xi32>
  %cst_1 = arith.constant dense<1> : vector<2xi32>
  %cst_2 = arith.constant dense<-1> : vector<2xi32>
  %cst_3 = arith.constant dense<23> : vector<2xi32>
  %cst_4 = arith.constant dense<31> : vector<2xi32>
  %cst_5 = arith.constant dense<127> : vector<2xi32>
  %cst_6 = arith.constant dense<4194304> : vector<2xi32>
  %cst_7 = arith.constant dense<8388607> : vector<2xi32>
  %cst_8 = arith.constant dense<255> : vector<2xi32>
  %cst_9 = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_10 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_11 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
  memref.assume_alignment %1, 64 : memref<2xi8>
  %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  %3 = arith.mulf %2, %cst_11 : vector<2xf32>
  %4 = arith.addf %3, %cst_10 : vector<2xf32>
  %5 = arith.bitcast %4 : vector<2xf32> to vector<2xi32>
  %6 = math.round %4 : vector<2xf32>
  %7 = arith.bitcast %6 : vector<2xf32> to vector<2xi32>
  %8 = arith.shrui %5, %cst_3 : vector<2xi32>
  %9 = arith.andi %8, %cst_8 : vector<2xi32>
  %10 = arith.subi %9, %cst_5 : vector<2xi32>
  %11 = arith.shrui %7, %cst_3 : vector<2xi32>
  %12 = arith.andi %11, %cst_8 : vector<2xi32>
  %13 = arith.subi %12, %cst_5 : vector<2xi32>
  %14 = arith.cmpi eq, %13, %cst_0 : vector<2xi32>
  %15 = arith.subi %13, %cst_1 : vector<2xi32>
  %16 = arith.maxsi %15, %cst_0 : vector<2xi32>
  %17 = arith.minsi %16, %cst_4 : vector<2xi32>
  %18 = arith.shrui %cst_7, %17 : vector<2xi32>
  %19 = arith.andi %7, %18 : vector<2xi32>
  %20 = arith.cmpi ne, %19, %cst_0 : vector<2xi32>
  %21 = arith.ori %20, %14 : vector<2xi1>
  %22 = arith.cmpi eq, %10, %cst_2 : vector<2xi32>
  %23 = arith.maxsi %10, %cst_0 : vector<2xi32>
  %24 = arith.minsi %23, %cst_4 : vector<2xi32>
  %25 = arith.shrui %cst_6, %24 : vector<2xi32>
  %26 = arith.select %22, %cst_0, %25 : vector<2xi1>, vector<2xi32>
  %27 = arith.shrui %cst_7, %24 : vector<2xi32>
  %28 = arith.andi %5, %27 : vector<2xi32>
  %29 = arith.cmpi eq, %28, %26 : vector<2xi32>
  %30 = arith.cmpi sge, %10, %cst_2 : vector<2xi32>
  %31 = arith.cmpi slt, %10, %cst_3 : vector<2xi32>
  %32 = arith.andi %29, %31 : vector<2xi1>
  %33 = arith.andi %32, %30 : vector<2xi1>
  %34 = math.copysign %cst, %4 : vector<2xf32>
  %35 = arith.subf %6, %34 : vector<2xf32>
  %36 = arith.andi %21, %33 : vector<2xi1>
  %37 = arith.select %36, %35, %6 : vector<2xi1>, vector<2xf32>
  %38 = math.copysign %37, %4 : vector<2xf32>
  %39 = arith.cmpf ult, %38, %cst_9 : vector<2xf32>
  %40 = arith.select %39, %38, %cst_9 : vector<2xi1>, vector<2xf32>
  %41 = arith.cmpf uno, %cst_9, %cst_9 : vector<2xf32>
  %42 = arith.select %41, %cst_9, %40 : vector<2xi1>, vector<2xf32>
  %43 = arith.cmpf ugt, %42, %cst_10 : vector<2xf32>
  %44 = arith.select %43, %42, %cst_10 : vector<2xi1>, vector<2xf32>
  %45 = arith.cmpf uno, %cst_10, %cst_10 : vector<2xf32>
  %46 = arith.select %45, %cst_10, %44 : vector<2xi1>, vector<2xf32>
  %47 = arith.fptosi %46 : vector<2xf32> to vector<2xi8>
  vector.store %47, %1[%c0] : memref<2xi8>, vector<2xi8>
  return
}

// -----// IR Dump After ExpandOps (memref-expand) //----- //
func.func @main_dispatch_1_softmax_2xf32() {
  %cst = arith.constant dense<0.693147182> : vector<2xf32>
  %cst_0 = arith.constant dense<1.44269502> : vector<2xf32>
  %cst_1 = arith.constant dense<1.000000e+00> : vector<2xf32>
  %cst_2 = arith.constant dense<0.499705136> : vector<2xf32>
  %cst_3 = arith.constant dense<0.168738902> : vector<2xf32>
  %cst_4 = arith.constant dense<0.0366896503> : vector<2xf32>
  %cst_5 = arith.constant dense<1.314350e-02> : vector<2xf32>
  %cst_6 = arith.constant dense<23> : vector<2xi32>
  %cst_7 = arith.constant dense<127> : vector<2xi32>
  %cst_8 = arith.constant dense<0.000000e+00> : vector<2xf32>
  %cst_9 = arith.constant dense<0x7F800000> : vector<2xf32>
  %cst_10 = arith.constant dense<0xFF800000> : vector<2xf32>
  %cst_11 = arith.constant dense<1.17549435E-38> : vector<2xf32>
  %cst_12 = arith.constant dense<-127> : vector<2xi32>
  %cst_13 = arith.constant dense<0.000000e+00> : vector<1xf32>
  %c0 = arith.constant 0 : index
  %c64 = arith.constant 64 : index
  %cst_14 = arith.constant dense<0.000000e+00> : vector<f32>
  %cst_15 = arith.constant dense<-1.000000e+30> : vector<f32>
  %alloca = memref.alloca() {alignment = 64 : i64} : memref<f32>
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c0) flags(ReadOnly) : memref<2xf32>
  memref.assume_alignment %0, 64 : memref<2xf32>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c64) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %1, 64 : memref<2xf32, strided<[1], offset: 16>>
  %2 = vector.load %0[%c0] : memref<2xf32>, vector<2xf32>
  %3 = vector.extractelement %cst_15[] : vector<f32>
  %4 = vector.reduction <maxf>, %2, %3 : vector<2xf32> into f32
  %5 = vector.insertelement %4, %cst_13[%c0 : index] : vector<1xf32>
  %6 = vector.extract %5[0] : vector<1xf32>
  %7 = vector.broadcast %6 : f32 to vector<f32>
  %8 = vector.extractelement %7[] : vector<f32>
  memref.store %8, %alloca[] : memref<f32>
  %9 = memref.load %alloca[] : memref<f32>
  %10 = vector.broadcast %9 : f32 to vector<2xf32>
  %11 = vector.extractelement %cst_14[] : vector<f32>
  %12 = arith.subf %2, %10 : vector<2xf32>
  %13 = arith.cmpf uno, %12, %12 : vector<2xf32>
  %14 = arith.mulf %12, %cst_0 : vector<2xf32>
  %15 = math.floor %14 : vector<2xf32>
  %16 = arith.mulf %15, %cst : vector<2xf32>
  %17 = arith.subf %12, %16 : vector<2xf32>
  %18 = arith.mulf %17, %17 : vector<2xf32>
  %19 = arith.mulf %18, %18 : vector<2xf32>
  %20 = math.fma %cst_1, %17, %cst_1 : vector<2xf32>
  %21 = math.fma %cst_3, %17, %cst_2 : vector<2xf32>
  %22 = math.fma %cst_5, %17, %cst_4 : vector<2xf32>
  %23 = math.fma %21, %18, %20 : vector<2xf32>
  %24 = math.fma %22, %19, %23 : vector<2xf32>
  %25 = arith.fptosi %15 : vector<2xf32> to vector<2xi32>
  %26 = arith.addi %25, %cst_7 : vector<2xi32>
  %27 = arith.shli %26, %cst_6 : vector<2xi32>
  %28 = arith.bitcast %27 : vector<2xi32> to vector<2xf32>
  %29 = arith.mulf %24, %28 : vector<2xf32>
  %30 = arith.cmpi sle, %25, %cst_7 : vector<2xi32>
  %31 = arith.cmpi sge, %25, %cst_12 : vector<2xi32>
  %32 = arith.cmpf oeq, %12, %cst_10 : vector<2xf32>
  %33 = arith.cmpf oeq, %12, %cst_9 : vector<2xf32>
  %34 = arith.cmpf ogt, %12, %cst_8 : vector<2xf32>
  %35 = arith.andi %30, %31 : vector<2xi1>
  %36 = arith.select %34, %cst_9, %cst_11 : vector<2xi1>, vector<2xf32>
  %37 = arith.select %35, %29, %36 : vector<2xi1>, vector<2xf32>
  %38 = arith.select %33, %cst_9, %37 : vector<2xi1>, vector<2xf32>
  %39 = arith.select %32, %cst_8, %38 : vector<2xi1>, vector<2xf32>
  %40 = arith.select %13, %12, %39 : vector<2xi1>, vector<2xf32>
  %41 = vector.reduction <add>, %40, %11 : vector<2xf32> into f32
  %42 = vector.insertelement %41, %cst_13[%c0 : index] : vector<1xf32>
  %43 = vector.extract %42[0] : vector<1xf32>
  %44 = vector.broadcast %43 : f32 to vector<f32>
  %45 = vector.extractelement %44[] : vector<f32>
  memref.store %45, %alloca[] : memref<f32>
  %46 = memref.load %alloca[] : memref<f32>
  %47 = vector.broadcast %46 : f32 to vector<2xf32>
  %48 = arith.divf %40, %47 : vector<2xf32>
  vector.store %48, %1[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  return
}

// -----// IR Dump After ExpandOps (memref-expand) //----- //
func.func @main_dispatch_2_generic_2_f32xi8() {
  %cst = arith.constant dense<1.000000e+00> : vector<2xf32>
  %cst_0 = arith.constant dense<0> : vector<2xi32>
  %cst_1 = arith.constant dense<1> : vector<2xi32>
  %cst_2 = arith.constant dense<-1> : vector<2xi32>
  %cst_3 = arith.constant dense<23> : vector<2xi32>
  %cst_4 = arith.constant dense<31> : vector<2xi32>
  %cst_5 = arith.constant dense<127> : vector<2xi32>
  %cst_6 = arith.constant dense<4194304> : vector<2xi32>
  %cst_7 = arith.constant dense<8388607> : vector<2xi32>
  %cst_8 = arith.constant dense<255> : vector<2xi32>
  %cst_9 = arith.constant dense<1.270000e+02> : vector<2xf32>
  %cst_10 = arith.constant dense<-1.280000e+02> : vector<2xf32>
  %cst_11 = arith.constant dense<2.560000e+02> : vector<2xf32>
  %c64 = arith.constant 64 : index
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan set(0) binding(0) type(storage_buffer) alignment(64) offset(%c64) flags(ReadOnly) : memref<2xf32, strided<[1], offset: 16>>
  memref.assume_alignment %0, 64 : memref<2xf32, strided<[1], offset: 16>>
  %1 = hal.interface.binding.subspan set(0) binding(1) type(storage_buffer) alignment(64) offset(%c0) : memref<2xi8>
  memref.assume_alignment %1, 64 : memref<2xi8>
  %2 = vector.load %0[%c0] : memref<2xf32, strided<[1], offset: 16>>, vector<2xf32>
  %3 = arith.mulf %2, %cst_11 : vector<2xf32>
  %4 = arith.addf %3, %cst_10 : vector<2xf32>
  %5 = arith.bitcast %4 : vector<2xf32> to vector<2xi32>
  %6 = math.round %4 : vector<2xf32>
  %7 = arith.bitcast %6 : vector<2xf32> to vector<2xi32>
  %8 = arith.shrui %5, %cst_3 : vector<2xi32>
  %9 = arith.andi %8, %cst_8 : vector<2xi32>
  %10 = arith.subi %9, %cst_5 : vector<2xi32>
  %11 = arith.shrui %7, %cst_3 : vector<2xi32>
  %12 = arith.andi %11, %cst_8 : vector<2xi32>
  %13 = arith.subi %12, %cst_5 : vector<2xi32>
  %14 = arith.cmpi eq, %13, %cst_0 : vector<2xi32>
  %15 = arith.subi %13, %cst_1 : vector<2xi32>
  %16 = arith.maxsi %15, %cst_0 : vector<2xi32>
  %17 = arith.minsi %16, %cst_4 : vector<2xi32>
  %18 = arith.shrui %cst_7, %17 : vector<2xi32>
  %19 = arith.andi %7, %18 : vector<2xi32>
  %20 = arith.cmpi ne, %19, %cst_0 : vector<2xi32>
  %21 = arith.ori %20, %14 : vector<2xi1>
  %22 = arith.cmpi eq, %10, %cst_2 : vector<2xi32>
  %23 = arith.maxsi %10, %cst_0 : vector<2xi32>
  %24 = arith.minsi %23, %cst_4 : vector<2xi32>
  %25 = arith.shrui %cst_6, %24 : vector<2xi32>
  %26 = arith.select %22, %cst_0, %25 : vector<2xi1>, vector<2xi32>
  %27 = arith.shrui %cst_7, %24 : vector<2xi32>
  %28 = arith.andi %5, %27 : vector<2xi32>
  %29 = arith.cmpi eq, %28, %26 : vector<2xi32>
  %30 = arith.cmpi sge, %10, %cst_2 : vector<2xi32>
  %31 = arith.cmpi slt, %10, %cst_3 : vector<2xi32>
  %32 = arith.andi %29, %31 : vector<2xi1>
  %33 = arith.andi %32, %30 : vector<2xi1>
  %34 = math.copysign %cst, %4 : vector<2xf32>
  %35 = arith.subf %6, %34 : vector<2xf32>
  %36 = arith.andi %21, %33 : vector<2xi1>
  %37 = arith.select %36, %35, %6 : vector<2xi1>, vector<2xf32>
  %38 = math.copysign %37, %4 : vector<2xf32>
  %39 = arith.cmpf ult, %38, %cst_9 : vector<2xf32>
  %40 = arith.select %39, %38, %cst_9 : vector<2xi1>, vector<2xf32>
  %41 = arith.cmpf uno, %cst_9, %cst_9 : vector<2xf32>
  %42 = arith.select %41, %cst_9, %40 : vector<2xi1>, vector<2xf32>
  %43 = arith.cmpf ugt, %42, %cst_10 : vector<2xf32>
  %44 = arith.select %43, %42, %cst_10 : vector<2xi1>, vector<2xf32>
  %45 = arith.cmpf uno, %cst_10, %cst_10 : vector<2xf32>
  %46 = arith.select %45, %cst_10, %44 : vector<2xi1>, vector<2xf32>
  %47 = arith.fptosi %46 : vector<2xf32> to vector<2xi8>
  vector.store %47, %1[%c0] : memref<2xi8>, vector<2xi8>
  return
}

// -----// IR Dump After ConvertToLLVM (iree-convert-to-llvm) //----- //
module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
  llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
    %0 = llvm.mlir.constant(0 : i32) : i32
    %1 = llvm.mlir.constant(0 : i64) : i64
    %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
    %3 = llvm.mlir.constant(63 : index) : i32
    %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
    %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
    %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
    %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
    %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
    %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
    %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
    %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
    %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
    %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
    %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
    %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
    %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
    %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
    %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
    %19 = llvm.mlir.constant(0 : index) : i32
    %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
    %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
    %22 = llvm.mlir.constant(1 : index) : i32
    %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
    %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
    %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
    %28 = llvm.and %27, %3  : i32
    %29 = llvm.icmp "eq" %28, %19 : i32
    "llvm.intr.assume"(%29) : (i1) -> ()
    %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
    %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
    %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
    %36 = llvm.and %35, %3  : i32
    %37 = llvm.icmp "eq" %36, %19 : i32
    "llvm.intr.assume"(%37) : (i1) -> ()
    %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
    %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
    %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
    %41 = llvm.fcmp "ogt" %40, %39 : f32
    %42 = llvm.select %41, %40, %39 : i1, f32
    %43 = llvm.fcmp "uno" %40, %39 : f32
    %44 = llvm.select %43, %2, %42 : i1, f32
    %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
    %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
    %47 = llvm.mlir.undef : vector<1xf32>
    %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
    %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
    llvm.store %49, %23 : f32, !llvm.ptr
    %50 = llvm.load %23 : !llvm.ptr -> f32
    %51 = llvm.mlir.undef : vector<2xf32>
    %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
    %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
    %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
    %55 = llvm.fsub %38, %53  : vector<2xf32>
    %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
    %57 = llvm.fmul %55, %5  : vector<2xf32>
    %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
    %59 = llvm.fmul %58, %4  : vector<2xf32>
    %60 = llvm.fsub %55, %59  : vector<2xf32>
    %61 = llvm.fmul %60, %60  : vector<2xf32>
    %62 = llvm.fmul %61, %61  : vector<2xf32>
    %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
    %69 = llvm.add %68, %12  : vector<2xi32>
    %70 = llvm.shl %69, %11  : vector<2xi32>
    %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
    %72 = llvm.fmul %67, %71  : vector<2xf32>
    %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
    %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
    %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
    %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
    %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
    %78 = llvm.and %73, %74  : vector<2xi1>
    %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
    %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
    %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
    %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
    %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
    %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
    %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
    %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
    %87 = llvm.mlir.undef : vector<1xf32>
    %88 = llvm.insertelement %86, %87[%0 : i32] : vector<1xf32>
    %89 = llvm.extractelement %88[%19 : i32] : vector<1xf32>
    llvm.store %89, %23 : f32, !llvm.ptr
    %90 = llvm.load %23 : !llvm.ptr -> f32
    %91 = llvm.mlir.undef : vector<2xf32>
    %92 = llvm.insertelement %90, %91[%0 : i32] : vector<2xf32>
    %93 = llvm.shufflevector %92, %91 [0, 0] : vector<2xf32> 
    %94 = llvm.fdiv %83, %93  : vector<2xf32>
    %95 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
    llvm.store %94, %95 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
    llvm.return %0 : i32
  }
}

// -----// IR Dump After ConvertToLLVM (iree-convert-to-llvm) //----- //
module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
  llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
    %0 = llvm.mlir.constant(0 : i32) : i32
    %1 = llvm.mlir.constant(63 : index) : i32
    %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
    %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
    %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
    %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
    %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
    %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
    %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
    %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
    %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
    %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
    %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
    %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
    %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
    %15 = llvm.mlir.constant(0 : index) : i32
    %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
    %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
    %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
    %21 = llvm.and %20, %1  : i32
    %22 = llvm.icmp "eq" %21, %15 : i32
    "llvm.intr.assume"(%22) : (i1) -> ()
    %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
    %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
    %28 = llvm.and %27, %1  : i32
    %29 = llvm.icmp "eq" %28, %15 : i32
    "llvm.intr.assume"(%29) : (i1) -> ()
    %30 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
    %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
    %32 = llvm.fmul %31, %14  : vector<2xf32>
    %33 = llvm.fadd %32, %13  : vector<2xf32>
    %34 = llvm.bitcast %33 : vector<2xf32> to vector<2xi32>
    %35 = llvm.intr.round(%33)  : (vector<2xf32>) -> vector<2xf32>
    %36 = llvm.bitcast %35 : vector<2xf32> to vector<2xi32>
    %37 = llvm.lshr %34, %6  : vector<2xi32>
    %38 = llvm.and %37, %11  : vector<2xi32>
    %39 = llvm.sub %38, %8  : vector<2xi32>
    %40 = llvm.lshr %36, %6  : vector<2xi32>
    %41 = llvm.and %40, %11  : vector<2xi32>
    %42 = llvm.sub %41, %8  : vector<2xi32>
    %43 = llvm.icmp "eq" %42, %3 : vector<2xi32>
    %44 = llvm.sub %42, %4  : vector<2xi32>
    %45 = llvm.intr.smax(%44, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %46 = llvm.intr.smin(%45, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %47 = llvm.lshr %10, %46  : vector<2xi32>
    %48 = llvm.and %36, %47  : vector<2xi32>
    %49 = llvm.icmp "ne" %48, %3 : vector<2xi32>
    %50 = llvm.or %49, %43  : vector<2xi1>
    %51 = llvm.icmp "eq" %39, %5 : vector<2xi32>
    %52 = llvm.intr.smax(%39, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %53 = llvm.intr.smin(%52, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %54 = llvm.lshr %9, %53  : vector<2xi32>
    %55 = llvm.select %51, %3, %54 : vector<2xi1>, vector<2xi32>
    %56 = llvm.lshr %10, %53  : vector<2xi32>
    %57 = llvm.and %34, %56  : vector<2xi32>
    %58 = llvm.icmp "eq" %57, %55 : vector<2xi32>
    %59 = llvm.icmp "sge" %39, %5 : vector<2xi32>
    %60 = llvm.icmp "slt" %39, %6 : vector<2xi32>
    %61 = llvm.and %58, %60  : vector<2xi1>
    %62 = llvm.and %61, %59  : vector<2xi1>
    %63 = llvm.intr.copysign(%2, %33)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %64 = llvm.fsub %35, %63  : vector<2xf32>
    %65 = llvm.and %50, %62  : vector<2xi1>
    %66 = llvm.select %65, %64, %35 : vector<2xi1>, vector<2xf32>
    %67 = llvm.intr.copysign(%66, %33)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %68 = llvm.fcmp "ult" %67, %12 : vector<2xf32>
    %69 = llvm.select %68, %67, %12 : vector<2xi1>, vector<2xf32>
    %70 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
    %71 = llvm.select %70, %12, %69 : vector<2xi1>, vector<2xf32>
    %72 = llvm.fcmp "ugt" %71, %13 : vector<2xf32>
    %73 = llvm.select %72, %71, %13 : vector<2xi1>, vector<2xf32>
    %74 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
    %75 = llvm.select %74, %13, %73 : vector<2xi1>, vector<2xf32>
    %76 = llvm.fptosi %75 : vector<2xf32> to vector<2xi8>
    llvm.store %76, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
    llvm.return %0 : i32
  }
}

// -----// IR Dump After ReconcileUnrealizedCasts (reconcile-unrealized-casts) //----- //
module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
  llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
    %0 = llvm.mlir.constant(0 : i32) : i32
    %1 = llvm.mlir.constant(0 : i64) : i64
    %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
    %3 = llvm.mlir.constant(63 : index) : i32
    %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
    %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
    %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
    %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
    %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
    %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
    %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
    %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
    %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
    %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
    %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
    %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
    %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
    %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
    %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
    %19 = llvm.mlir.constant(0 : index) : i32
    %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
    %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
    %22 = llvm.mlir.constant(1 : index) : i32
    %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
    %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
    %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
    %28 = llvm.and %27, %3  : i32
    %29 = llvm.icmp "eq" %28, %19 : i32
    "llvm.intr.assume"(%29) : (i1) -> ()
    %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
    %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
    %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
    %36 = llvm.and %35, %3  : i32
    %37 = llvm.icmp "eq" %36, %19 : i32
    "llvm.intr.assume"(%37) : (i1) -> ()
    %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
    %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
    %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
    %41 = llvm.fcmp "ogt" %40, %39 : f32
    %42 = llvm.select %41, %40, %39 : i1, f32
    %43 = llvm.fcmp "uno" %40, %39 : f32
    %44 = llvm.select %43, %2, %42 : i1, f32
    %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
    %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
    %47 = llvm.mlir.undef : vector<1xf32>
    %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
    %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
    llvm.store %49, %23 : f32, !llvm.ptr
    %50 = llvm.load %23 : !llvm.ptr -> f32
    %51 = llvm.mlir.undef : vector<2xf32>
    %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
    %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
    %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
    %55 = llvm.fsub %38, %53  : vector<2xf32>
    %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
    %57 = llvm.fmul %55, %5  : vector<2xf32>
    %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
    %59 = llvm.fmul %58, %4  : vector<2xf32>
    %60 = llvm.fsub %55, %59  : vector<2xf32>
    %61 = llvm.fmul %60, %60  : vector<2xf32>
    %62 = llvm.fmul %61, %61  : vector<2xf32>
    %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
    %69 = llvm.add %68, %12  : vector<2xi32>
    %70 = llvm.shl %69, %11  : vector<2xi32>
    %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
    %72 = llvm.fmul %67, %71  : vector<2xf32>
    %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
    %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
    %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
    %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
    %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
    %78 = llvm.and %73, %74  : vector<2xi1>
    %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
    %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
    %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
    %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
    %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
    %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
    %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
    %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
    %87 = llvm.mlir.undef : vector<1xf32>
    %88 = llvm.insertelement %86, %87[%0 : i32] : vector<1xf32>
    %89 = llvm.extractelement %88[%19 : i32] : vector<1xf32>
    llvm.store %89, %23 : f32, !llvm.ptr
    %90 = llvm.load %23 : !llvm.ptr -> f32
    %91 = llvm.mlir.undef : vector<2xf32>
    %92 = llvm.insertelement %90, %91[%0 : i32] : vector<2xf32>
    %93 = llvm.shufflevector %92, %91 [0, 0] : vector<2xf32> 
    %94 = llvm.fdiv %83, %93  : vector<2xf32>
    %95 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
    llvm.store %94, %95 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
    llvm.return %0 : i32
  }
}

// -----// IR Dump After ReconcileUnrealizedCasts (reconcile-unrealized-casts) //----- //
module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
  llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
    %0 = llvm.mlir.constant(0 : i32) : i32
    %1 = llvm.mlir.constant(63 : index) : i32
    %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
    %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
    %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
    %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
    %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
    %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
    %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
    %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
    %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
    %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
    %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
    %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
    %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
    %15 = llvm.mlir.constant(0 : index) : i32
    %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
    %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
    %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
    %21 = llvm.and %20, %1  : i32
    %22 = llvm.icmp "eq" %21, %15 : i32
    "llvm.intr.assume"(%22) : (i1) -> ()
    %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
    %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
    %28 = llvm.and %27, %1  : i32
    %29 = llvm.icmp "eq" %28, %15 : i32
    "llvm.intr.assume"(%29) : (i1) -> ()
    %30 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
    %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
    %32 = llvm.fmul %31, %14  : vector<2xf32>
    %33 = llvm.fadd %32, %13  : vector<2xf32>
    %34 = llvm.bitcast %33 : vector<2xf32> to vector<2xi32>
    %35 = llvm.intr.round(%33)  : (vector<2xf32>) -> vector<2xf32>
    %36 = llvm.bitcast %35 : vector<2xf32> to vector<2xi32>
    %37 = llvm.lshr %34, %6  : vector<2xi32>
    %38 = llvm.and %37, %11  : vector<2xi32>
    %39 = llvm.sub %38, %8  : vector<2xi32>
    %40 = llvm.lshr %36, %6  : vector<2xi32>
    %41 = llvm.and %40, %11  : vector<2xi32>
    %42 = llvm.sub %41, %8  : vector<2xi32>
    %43 = llvm.icmp "eq" %42, %3 : vector<2xi32>
    %44 = llvm.sub %42, %4  : vector<2xi32>
    %45 = llvm.intr.smax(%44, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %46 = llvm.intr.smin(%45, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %47 = llvm.lshr %10, %46  : vector<2xi32>
    %48 = llvm.and %36, %47  : vector<2xi32>
    %49 = llvm.icmp "ne" %48, %3 : vector<2xi32>
    %50 = llvm.or %49, %43  : vector<2xi1>
    %51 = llvm.icmp "eq" %39, %5 : vector<2xi32>
    %52 = llvm.intr.smax(%39, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %53 = llvm.intr.smin(%52, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %54 = llvm.lshr %9, %53  : vector<2xi32>
    %55 = llvm.select %51, %3, %54 : vector<2xi1>, vector<2xi32>
    %56 = llvm.lshr %10, %53  : vector<2xi32>
    %57 = llvm.and %34, %56  : vector<2xi32>
    %58 = llvm.icmp "eq" %57, %55 : vector<2xi32>
    %59 = llvm.icmp "sge" %39, %5 : vector<2xi32>
    %60 = llvm.icmp "slt" %39, %6 : vector<2xi32>
    %61 = llvm.and %58, %60  : vector<2xi1>
    %62 = llvm.and %61, %59  : vector<2xi1>
    %63 = llvm.intr.copysign(%2, %33)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %64 = llvm.fsub %35, %63  : vector<2xf32>
    %65 = llvm.and %50, %62  : vector<2xi1>
    %66 = llvm.select %65, %64, %35 : vector<2xi1>, vector<2xf32>
    %67 = llvm.intr.copysign(%66, %33)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %68 = llvm.fcmp "ult" %67, %12 : vector<2xf32>
    %69 = llvm.select %68, %67, %12 : vector<2xi1>, vector<2xf32>
    %70 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
    %71 = llvm.select %70, %12, %69 : vector<2xi1>, vector<2xf32>
    %72 = llvm.fcmp "ugt" %71, %13 : vector<2xf32>
    %73 = llvm.select %72, %71, %13 : vector<2xi1>, vector<2xf32>
    %74 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
    %75 = llvm.select %74, %13, %73 : vector<2xi1>, vector<2xf32>
    %76 = llvm.fptosi %75 : vector<2xf32> to vector<2xi8>
    llvm.store %76, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
    llvm.return %0 : i32
  }
}

// -----// IR Dump After LLVMCPUSynchronizeSymbolVisibility (iree-llvmcpu-synchronize-symbol-visibility) //----- //
module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
  llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
    %0 = llvm.mlir.constant(0 : i32) : i32
    %1 = llvm.mlir.constant(0 : i64) : i64
    %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
    %3 = llvm.mlir.constant(63 : index) : i32
    %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
    %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
    %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
    %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
    %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
    %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
    %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
    %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
    %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
    %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
    %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
    %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
    %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
    %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
    %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
    %19 = llvm.mlir.constant(0 : index) : i32
    %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
    %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
    %22 = llvm.mlir.constant(1 : index) : i32
    %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
    %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
    %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
    %28 = llvm.and %27, %3  : i32
    %29 = llvm.icmp "eq" %28, %19 : i32
    "llvm.intr.assume"(%29) : (i1) -> ()
    %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
    %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
    %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
    %36 = llvm.and %35, %3  : i32
    %37 = llvm.icmp "eq" %36, %19 : i32
    "llvm.intr.assume"(%37) : (i1) -> ()
    %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
    %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
    %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
    %41 = llvm.fcmp "ogt" %40, %39 : f32
    %42 = llvm.select %41, %40, %39 : i1, f32
    %43 = llvm.fcmp "uno" %40, %39 : f32
    %44 = llvm.select %43, %2, %42 : i1, f32
    %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
    %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
    %47 = llvm.mlir.undef : vector<1xf32>
    %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
    %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
    llvm.store %49, %23 : f32, !llvm.ptr
    %50 = llvm.load %23 : !llvm.ptr -> f32
    %51 = llvm.mlir.undef : vector<2xf32>
    %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
    %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
    %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
    %55 = llvm.fsub %38, %53  : vector<2xf32>
    %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
    %57 = llvm.fmul %55, %5  : vector<2xf32>
    %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
    %59 = llvm.fmul %58, %4  : vector<2xf32>
    %60 = llvm.fsub %55, %59  : vector<2xf32>
    %61 = llvm.fmul %60, %60  : vector<2xf32>
    %62 = llvm.fmul %61, %61  : vector<2xf32>
    %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
    %69 = llvm.add %68, %12  : vector<2xi32>
    %70 = llvm.shl %69, %11  : vector<2xi32>
    %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
    %72 = llvm.fmul %67, %71  : vector<2xf32>
    %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
    %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
    %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
    %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
    %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
    %78 = llvm.and %73, %74  : vector<2xi1>
    %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
    %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
    %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
    %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
    %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
    %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
    %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
    %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
    %87 = llvm.mlir.undef : vector<1xf32>
    %88 = llvm.insertelement %86, %87[%0 : i32] : vector<1xf32>
    %89 = llvm.extractelement %88[%19 : i32] : vector<1xf32>
    llvm.store %89, %23 : f32, !llvm.ptr
    %90 = llvm.load %23 : !llvm.ptr -> f32
    %91 = llvm.mlir.undef : vector<2xf32>
    %92 = llvm.insertelement %90, %91[%0 : i32] : vector<2xf32>
    %93 = llvm.shufflevector %92, %91 [0, 0] : vector<2xf32> 
    %94 = llvm.fdiv %83, %93  : vector<2xf32>
    %95 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
    llvm.store %94, %95 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
    llvm.return %0 : i32
  }
}

// -----// IR Dump After LLVMCPUSynchronizeSymbolVisibility (iree-llvmcpu-synchronize-symbol-visibility) //----- //
module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
  llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
    %0 = llvm.mlir.constant(0 : i32) : i32
    %1 = llvm.mlir.constant(63 : index) : i32
    %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
    %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
    %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
    %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
    %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
    %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
    %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
    %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
    %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
    %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
    %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
    %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
    %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
    %15 = llvm.mlir.constant(0 : index) : i32
    %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
    %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
    %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
    %21 = llvm.and %20, %1  : i32
    %22 = llvm.icmp "eq" %21, %15 : i32
    "llvm.intr.assume"(%22) : (i1) -> ()
    %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
    %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
    %28 = llvm.and %27, %1  : i32
    %29 = llvm.icmp "eq" %28, %15 : i32
    "llvm.intr.assume"(%29) : (i1) -> ()
    %30 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
    %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
    %32 = llvm.fmul %31, %14  : vector<2xf32>
    %33 = llvm.fadd %32, %13  : vector<2xf32>
    %34 = llvm.bitcast %33 : vector<2xf32> to vector<2xi32>
    %35 = llvm.intr.round(%33)  : (vector<2xf32>) -> vector<2xf32>
    %36 = llvm.bitcast %35 : vector<2xf32> to vector<2xi32>
    %37 = llvm.lshr %34, %6  : vector<2xi32>
    %38 = llvm.and %37, %11  : vector<2xi32>
    %39 = llvm.sub %38, %8  : vector<2xi32>
    %40 = llvm.lshr %36, %6  : vector<2xi32>
    %41 = llvm.and %40, %11  : vector<2xi32>
    %42 = llvm.sub %41, %8  : vector<2xi32>
    %43 = llvm.icmp "eq" %42, %3 : vector<2xi32>
    %44 = llvm.sub %42, %4  : vector<2xi32>
    %45 = llvm.intr.smax(%44, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %46 = llvm.intr.smin(%45, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %47 = llvm.lshr %10, %46  : vector<2xi32>
    %48 = llvm.and %36, %47  : vector<2xi32>
    %49 = llvm.icmp "ne" %48, %3 : vector<2xi32>
    %50 = llvm.or %49, %43  : vector<2xi1>
    %51 = llvm.icmp "eq" %39, %5 : vector<2xi32>
    %52 = llvm.intr.smax(%39, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %53 = llvm.intr.smin(%52, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %54 = llvm.lshr %9, %53  : vector<2xi32>
    %55 = llvm.select %51, %3, %54 : vector<2xi1>, vector<2xi32>
    %56 = llvm.lshr %10, %53  : vector<2xi32>
    %57 = llvm.and %34, %56  : vector<2xi32>
    %58 = llvm.icmp "eq" %57, %55 : vector<2xi32>
    %59 = llvm.icmp "sge" %39, %5 : vector<2xi32>
    %60 = llvm.icmp "slt" %39, %6 : vector<2xi32>
    %61 = llvm.and %58, %60  : vector<2xi1>
    %62 = llvm.and %61, %59  : vector<2xi1>
    %63 = llvm.intr.copysign(%2, %33)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %64 = llvm.fsub %35, %63  : vector<2xf32>
    %65 = llvm.and %50, %62  : vector<2xi1>
    %66 = llvm.select %65, %64, %35 : vector<2xi1>, vector<2xf32>
    %67 = llvm.intr.copysign(%66, %33)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %68 = llvm.fcmp "ult" %67, %12 : vector<2xf32>
    %69 = llvm.select %68, %67, %12 : vector<2xi1>, vector<2xf32>
    %70 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
    %71 = llvm.select %70, %12, %69 : vector<2xi1>, vector<2xf32>
    %72 = llvm.fcmp "ugt" %71, %13 : vector<2xf32>
    %73 = llvm.select %72, %71, %13 : vector<2xi1>, vector<2xf32>
    %74 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
    %75 = llvm.select %74, %13, %73 : vector<2xi1>, vector<2xf32>
    %76 = llvm.fptosi %75 : vector<2xf32> to vector<2xi8>
    llvm.store %76, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
    llvm.return %0 : i32
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
  llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
    %0 = llvm.mlir.constant(0 : i32) : i32
    %1 = llvm.mlir.constant(0 : i64) : i64
    %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
    %3 = llvm.mlir.constant(63 : index) : i32
    %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
    %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
    %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
    %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
    %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
    %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
    %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
    %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
    %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
    %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
    %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
    %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
    %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
    %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
    %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
    %19 = llvm.mlir.constant(0 : index) : i32
    %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
    %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
    %22 = llvm.mlir.constant(1 : index) : i32
    %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
    %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
    %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
    %28 = llvm.and %27, %3  : i32
    %29 = llvm.icmp "eq" %28, %19 : i32
    "llvm.intr.assume"(%29) : (i1) -> ()
    %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
    %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
    %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
    %36 = llvm.and %35, %3  : i32
    %37 = llvm.icmp "eq" %36, %19 : i32
    "llvm.intr.assume"(%37) : (i1) -> ()
    %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
    %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
    %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
    %41 = llvm.fcmp "ogt" %40, %39 : f32
    %42 = llvm.select %41, %40, %39 : i1, f32
    %43 = llvm.fcmp "uno" %40, %39 : f32
    %44 = llvm.select %43, %2, %42 : i1, f32
    %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
    %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
    %47 = llvm.mlir.undef : vector<1xf32>
    %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
    %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
    llvm.store %49, %23 : f32, !llvm.ptr
    %50 = llvm.load %23 : !llvm.ptr -> f32
    %51 = llvm.mlir.undef : vector<2xf32>
    %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
    %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
    %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
    %55 = llvm.fsub %38, %53  : vector<2xf32>
    %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
    %57 = llvm.fmul %55, %5  : vector<2xf32>
    %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
    %59 = llvm.fmul %58, %4  : vector<2xf32>
    %60 = llvm.fsub %55, %59  : vector<2xf32>
    %61 = llvm.fmul %60, %60  : vector<2xf32>
    %62 = llvm.fmul %61, %61  : vector<2xf32>
    %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
    %69 = llvm.add %68, %12  : vector<2xi32>
    %70 = llvm.shl %69, %11  : vector<2xi32>
    %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
    %72 = llvm.fmul %67, %71  : vector<2xf32>
    %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
    %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
    %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
    %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
    %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
    %78 = llvm.and %73, %74  : vector<2xi1>
    %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
    %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
    %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
    %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
    %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
    %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
    %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
    %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
    %87 = llvm.mlir.undef : vector<1xf32>
    %88 = llvm.insertelement %86, %87[%0 : i32] : vector<1xf32>
    %89 = llvm.extractelement %88[%19 : i32] : vector<1xf32>
    llvm.store %89, %23 : f32, !llvm.ptr
    %90 = llvm.load %23 : !llvm.ptr -> f32
    %91 = llvm.mlir.undef : vector<2xf32>
    %92 = llvm.insertelement %90, %91[%0 : i32] : vector<2xf32>
    %93 = llvm.shufflevector %92, %91 [0, 0] : vector<2xf32> 
    %94 = llvm.fdiv %83, %93  : vector<2xf32>
    %95 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
    llvm.store %94, %95 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
    llvm.return %0 : i32
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
  llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
    %0 = llvm.mlir.constant(0 : i32) : i32
    %1 = llvm.mlir.constant(63 : index) : i32
    %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
    %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
    %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
    %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
    %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
    %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
    %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
    %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
    %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
    %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
    %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
    %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
    %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
    %15 = llvm.mlir.constant(0 : index) : i32
    %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
    %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
    %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
    %21 = llvm.and %20, %1  : i32
    %22 = llvm.icmp "eq" %21, %15 : i32
    "llvm.intr.assume"(%22) : (i1) -> ()
    %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
    %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
    %28 = llvm.and %27, %1  : i32
    %29 = llvm.icmp "eq" %28, %15 : i32
    "llvm.intr.assume"(%29) : (i1) -> ()
    %30 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
    %31 = llvm.load %30 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
    %32 = llvm.fmul %31, %14  : vector<2xf32>
    %33 = llvm.fadd %32, %13  : vector<2xf32>
    %34 = llvm.bitcast %33 : vector<2xf32> to vector<2xi32>
    %35 = llvm.intr.round(%33)  : (vector<2xf32>) -> vector<2xf32>
    %36 = llvm.bitcast %35 : vector<2xf32> to vector<2xi32>
    %37 = llvm.lshr %34, %6  : vector<2xi32>
    %38 = llvm.and %37, %11  : vector<2xi32>
    %39 = llvm.sub %38, %8  : vector<2xi32>
    %40 = llvm.lshr %36, %6  : vector<2xi32>
    %41 = llvm.and %40, %11  : vector<2xi32>
    %42 = llvm.sub %41, %8  : vector<2xi32>
    %43 = llvm.icmp "eq" %42, %3 : vector<2xi32>
    %44 = llvm.sub %42, %4  : vector<2xi32>
    %45 = llvm.intr.smax(%44, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %46 = llvm.intr.smin(%45, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %47 = llvm.lshr %10, %46  : vector<2xi32>
    %48 = llvm.and %36, %47  : vector<2xi32>
    %49 = llvm.icmp "ne" %48, %3 : vector<2xi32>
    %50 = llvm.or %49, %43  : vector<2xi1>
    %51 = llvm.icmp "eq" %39, %5 : vector<2xi32>
    %52 = llvm.intr.smax(%39, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %53 = llvm.intr.smin(%52, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %54 = llvm.lshr %9, %53  : vector<2xi32>
    %55 = llvm.select %51, %3, %54 : vector<2xi1>, vector<2xi32>
    %56 = llvm.lshr %10, %53  : vector<2xi32>
    %57 = llvm.and %34, %56  : vector<2xi32>
    %58 = llvm.icmp "eq" %57, %55 : vector<2xi32>
    %59 = llvm.icmp "sge" %39, %5 : vector<2xi32>
    %60 = llvm.icmp "slt" %39, %6 : vector<2xi32>
    %61 = llvm.and %58, %60  : vector<2xi1>
    %62 = llvm.and %61, %59  : vector<2xi1>
    %63 = llvm.intr.copysign(%2, %33)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %64 = llvm.fsub %35, %63  : vector<2xf32>
    %65 = llvm.and %50, %62  : vector<2xi1>
    %66 = llvm.select %65, %64, %35 : vector<2xi1>, vector<2xf32>
    %67 = llvm.intr.copysign(%66, %33)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %68 = llvm.fcmp "ult" %67, %12 : vector<2xf32>
    %69 = llvm.select %68, %67, %12 : vector<2xi1>, vector<2xf32>
    %70 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
    %71 = llvm.select %70, %12, %69 : vector<2xi1>, vector<2xf32>
    %72 = llvm.fcmp "ugt" %71, %13 : vector<2xf32>
    %73 = llvm.select %72, %71, %13 : vector<2xi1>, vector<2xf32>
    %74 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
    %75 = llvm.select %74, %13, %73 : vector<2xi1>, vector<2xf32>
    %76 = llvm.fptosi %75 : vector<2xf32> to vector<2xi8>
    llvm.store %76, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
    llvm.return %0 : i32
  }
}

// -----// IR Dump After CSE (cse) //----- //
module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
  llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
    %0 = llvm.mlir.constant(0 : i32) : i32
    %1 = llvm.mlir.constant(0 : i64) : i64
    %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
    %3 = llvm.mlir.constant(63 : index) : i32
    %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
    %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
    %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
    %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
    %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
    %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
    %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
    %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
    %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
    %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
    %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
    %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
    %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
    %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
    %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
    %19 = llvm.mlir.constant(0 : index) : i32
    %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
    %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
    %22 = llvm.mlir.constant(1 : index) : i32
    %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
    %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
    %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
    %28 = llvm.and %27, %3  : i32
    %29 = llvm.icmp "eq" %28, %19 : i32
    "llvm.intr.assume"(%29) : (i1) -> ()
    %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
    %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
    %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
    %36 = llvm.and %35, %3  : i32
    %37 = llvm.icmp "eq" %36, %19 : i32
    "llvm.intr.assume"(%37) : (i1) -> ()
    %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
    %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
    %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
    %41 = llvm.fcmp "ogt" %40, %39 : f32
    %42 = llvm.select %41, %40, %39 : i1, f32
    %43 = llvm.fcmp "uno" %40, %39 : f32
    %44 = llvm.select %43, %2, %42 : i1, f32
    %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
    %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
    %47 = llvm.mlir.undef : vector<1xf32>
    %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
    %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
    llvm.store %49, %23 : f32, !llvm.ptr
    %50 = llvm.load %23 : !llvm.ptr -> f32
    %51 = llvm.mlir.undef : vector<2xf32>
    %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
    %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
    %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
    %55 = llvm.fsub %38, %53  : vector<2xf32>
    %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
    %57 = llvm.fmul %55, %5  : vector<2xf32>
    %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
    %59 = llvm.fmul %58, %4  : vector<2xf32>
    %60 = llvm.fsub %55, %59  : vector<2xf32>
    %61 = llvm.fmul %60, %60  : vector<2xf32>
    %62 = llvm.fmul %61, %61  : vector<2xf32>
    %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
    %69 = llvm.add %68, %12  : vector<2xi32>
    %70 = llvm.shl %69, %11  : vector<2xi32>
    %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
    %72 = llvm.fmul %67, %71  : vector<2xf32>
    %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
    %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
    %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
    %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
    %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
    %78 = llvm.and %73, %74  : vector<2xi1>
    %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
    %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
    %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
    %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
    %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
    %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
    %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
    %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
    %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
    %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
    llvm.store %88, %23 : f32, !llvm.ptr
    %89 = llvm.load %23 : !llvm.ptr -> f32
    %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
    %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
    %92 = llvm.fdiv %83, %91  : vector<2xf32>
    llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
    llvm.return %0 : i32
  }
}

// -----// IR Dump After CSE (cse) //----- //
module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
  llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
    %0 = llvm.mlir.constant(0 : i32) : i32
    %1 = llvm.mlir.constant(63 : index) : i32
    %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
    %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
    %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
    %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
    %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
    %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
    %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
    %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
    %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
    %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
    %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
    %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
    %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
    %15 = llvm.mlir.constant(0 : index) : i32
    %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
    %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
    %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
    %21 = llvm.and %20, %1  : i32
    %22 = llvm.icmp "eq" %21, %15 : i32
    "llvm.intr.assume"(%22) : (i1) -> ()
    %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
    %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
    %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
    %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
    %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
    %28 = llvm.and %27, %1  : i32
    %29 = llvm.icmp "eq" %28, %15 : i32
    "llvm.intr.assume"(%29) : (i1) -> ()
    %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
    %31 = llvm.fmul %30, %14  : vector<2xf32>
    %32 = llvm.fadd %31, %13  : vector<2xf32>
    %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
    %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
    %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
    %36 = llvm.lshr %33, %6  : vector<2xi32>
    %37 = llvm.and %36, %11  : vector<2xi32>
    %38 = llvm.sub %37, %8  : vector<2xi32>
    %39 = llvm.lshr %35, %6  : vector<2xi32>
    %40 = llvm.and %39, %11  : vector<2xi32>
    %41 = llvm.sub %40, %8  : vector<2xi32>
    %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
    %43 = llvm.sub %41, %4  : vector<2xi32>
    %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %46 = llvm.lshr %10, %45  : vector<2xi32>
    %47 = llvm.and %35, %46  : vector<2xi32>
    %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
    %49 = llvm.or %48, %42  : vector<2xi1>
    %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
    %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
    %53 = llvm.lshr %9, %52  : vector<2xi32>
    %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
    %55 = llvm.lshr %10, %52  : vector<2xi32>
    %56 = llvm.and %33, %55  : vector<2xi32>
    %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
    %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
    %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
    %60 = llvm.and %57, %59  : vector<2xi1>
    %61 = llvm.and %60, %58  : vector<2xi1>
    %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %63 = llvm.fsub %34, %62  : vector<2xf32>
    %64 = llvm.and %49, %61  : vector<2xi1>
    %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
    %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
    %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
    %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
    %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
    %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
    %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
    %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
    %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
    %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
    %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
    llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
    llvm.return %0 : i32
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::TranslateTargetExecutableVariantsPass (iree-hal-translate-target-executable-variants) //----- //
hal.executable.variant public @static, target = <"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}> {
  hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert>} {
  ^bb0(%arg0: !hal.device):
    %c1 = arith.constant 1 : index
    hal.return %c1, %c1, %c1 : index, index, index
  }
  builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
    llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
      %0 = llvm.mlir.constant(0 : i32) : i32
      %1 = llvm.mlir.constant(0 : i64) : i64
      %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
      %3 = llvm.mlir.constant(63 : index) : i32
      %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
      %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
      %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
      %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
      %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
      %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
      %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
      %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
      %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
      %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
      %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
      %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
      %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
      %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
      %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
      %19 = llvm.mlir.constant(0 : index) : i32
      %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
      %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
      %22 = llvm.mlir.constant(1 : index) : i32
      %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
      %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
      %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
      %28 = llvm.and %27, %3  : i32
      %29 = llvm.icmp "eq" %28, %19 : i32
      "llvm.intr.assume"(%29) : (i1) -> ()
      %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
      %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
      %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
      %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
      %36 = llvm.and %35, %3  : i32
      %37 = llvm.icmp "eq" %36, %19 : i32
      "llvm.intr.assume"(%37) : (i1) -> ()
      %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
      %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
      %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
      %41 = llvm.fcmp "ogt" %40, %39 : f32
      %42 = llvm.select %41, %40, %39 : i1, f32
      %43 = llvm.fcmp "uno" %40, %39 : f32
      %44 = llvm.select %43, %2, %42 : i1, f32
      %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
      %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
      %47 = llvm.mlir.undef : vector<1xf32>
      %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
      %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
      llvm.store %49, %23 : f32, !llvm.ptr
      %50 = llvm.load %23 : !llvm.ptr -> f32
      %51 = llvm.mlir.undef : vector<2xf32>
      %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
      %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
      %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
      %55 = llvm.fsub %38, %53  : vector<2xf32>
      %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
      %57 = llvm.fmul %55, %5  : vector<2xf32>
      %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
      %59 = llvm.fmul %58, %4  : vector<2xf32>
      %60 = llvm.fsub %55, %59  : vector<2xf32>
      %61 = llvm.fmul %60, %60  : vector<2xf32>
      %62 = llvm.fmul %61, %61  : vector<2xf32>
      %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
      %69 = llvm.add %68, %12  : vector<2xi32>
      %70 = llvm.shl %69, %11  : vector<2xi32>
      %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
      %72 = llvm.fmul %67, %71  : vector<2xf32>
      %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
      %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
      %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
      %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
      %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
      %78 = llvm.and %73, %74  : vector<2xi1>
      %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
      %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
      %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
      %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
      %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
      %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
      %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
      %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
      %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
      %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
      llvm.store %88, %23 : f32, !llvm.ptr
      %89 = llvm.load %23 : !llvm.ptr -> f32
      %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
      %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
      %92 = llvm.fdiv %83, %91  : vector<2xf32>
      llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
      llvm.return %0 : i32
    }
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::TranslateTargetExecutableVariantsPass (iree-hal-translate-target-executable-variants) //----- //
hal.executable.variant public @static, target = <"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}> {
  hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert>} {
  ^bb0(%arg0: !hal.device):
    %c1 = arith.constant 1 : index
    hal.return %c1, %c1, %c1 : index, index, index
  }
  builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
    llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
      %0 = llvm.mlir.constant(0 : i32) : i32
      %1 = llvm.mlir.constant(63 : index) : i32
      %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
      %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
      %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
      %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
      %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
      %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
      %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
      %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
      %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
      %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
      %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
      %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
      %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
      %15 = llvm.mlir.constant(0 : index) : i32
      %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
      %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
      %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
      %21 = llvm.and %20, %1  : i32
      %22 = llvm.icmp "eq" %21, %15 : i32
      "llvm.intr.assume"(%22) : (i1) -> ()
      %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
      %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
      %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
      %28 = llvm.and %27, %1  : i32
      %29 = llvm.icmp "eq" %28, %15 : i32
      "llvm.intr.assume"(%29) : (i1) -> ()
      %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
      %31 = llvm.fmul %30, %14  : vector<2xf32>
      %32 = llvm.fadd %31, %13  : vector<2xf32>
      %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
      %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
      %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
      %36 = llvm.lshr %33, %6  : vector<2xi32>
      %37 = llvm.and %36, %11  : vector<2xi32>
      %38 = llvm.sub %37, %8  : vector<2xi32>
      %39 = llvm.lshr %35, %6  : vector<2xi32>
      %40 = llvm.and %39, %11  : vector<2xi32>
      %41 = llvm.sub %40, %8  : vector<2xi32>
      %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
      %43 = llvm.sub %41, %4  : vector<2xi32>
      %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
      %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
      %46 = llvm.lshr %10, %45  : vector<2xi32>
      %47 = llvm.and %35, %46  : vector<2xi32>
      %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
      %49 = llvm.or %48, %42  : vector<2xi1>
      %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
      %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
      %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
      %53 = llvm.lshr %9, %52  : vector<2xi32>
      %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
      %55 = llvm.lshr %10, %52  : vector<2xi32>
      %56 = llvm.and %33, %55  : vector<2xi32>
      %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
      %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
      %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
      %60 = llvm.and %57, %59  : vector<2xi1>
      %61 = llvm.and %60, %58  : vector<2xi1>
      %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %63 = llvm.fsub %34, %62  : vector<2xf32>
      %64 = llvm.and %49, %61  : vector<2xi1>
      %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
      %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
      %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
      %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
      %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
      %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
      %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
      %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
      %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
      %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
      llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
      llvm.return %0 : i32
    }
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::TranslateExecutablesPass (iree-hal-translate-executables) //----- //
hal.executable private @main_dispatch_1 {
  hal.executable.variant public @static, target = <"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}> {
    hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert>} {
    ^bb0(%arg0: !hal.device):
      %c1 = arith.constant 1 : index
      hal.return %c1, %c1, %c1 : index, index, index
    }
    builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
      llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
        %0 = llvm.mlir.constant(0 : i32) : i32
        %1 = llvm.mlir.constant(0 : i64) : i64
        %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
        %3 = llvm.mlir.constant(63 : index) : i32
        %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
        %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
        %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
        %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
        %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
        %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
        %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
        %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
        %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
        %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
        %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
        %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
        %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
        %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
        %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
        %19 = llvm.mlir.constant(0 : index) : i32
        %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
        %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
        %22 = llvm.mlir.constant(1 : index) : i32
        %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
        %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
        %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
        %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
        %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
        %28 = llvm.and %27, %3  : i32
        %29 = llvm.icmp "eq" %28, %19 : i32
        "llvm.intr.assume"(%29) : (i1) -> ()
        %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
        %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
        %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
        %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
        %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
        %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
        %36 = llvm.and %35, %3  : i32
        %37 = llvm.icmp "eq" %36, %19 : i32
        "llvm.intr.assume"(%37) : (i1) -> ()
        %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
        %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
        %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
        %41 = llvm.fcmp "ogt" %40, %39 : f32
        %42 = llvm.select %41, %40, %39 : i1, f32
        %43 = llvm.fcmp "uno" %40, %39 : f32
        %44 = llvm.select %43, %2, %42 : i1, f32
        %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
        %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
        %47 = llvm.mlir.undef : vector<1xf32>
        %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
        %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
        llvm.store %49, %23 : f32, !llvm.ptr
        %50 = llvm.load %23 : !llvm.ptr -> f32
        %51 = llvm.mlir.undef : vector<2xf32>
        %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
        %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
        %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
        %55 = llvm.fsub %38, %53  : vector<2xf32>
        %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
        %57 = llvm.fmul %55, %5  : vector<2xf32>
        %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
        %59 = llvm.fmul %58, %4  : vector<2xf32>
        %60 = llvm.fsub %55, %59  : vector<2xf32>
        %61 = llvm.fmul %60, %60  : vector<2xf32>
        %62 = llvm.fmul %61, %61  : vector<2xf32>
        %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
        %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
        %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
        %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
        %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
        %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
        %69 = llvm.add %68, %12  : vector<2xi32>
        %70 = llvm.shl %69, %11  : vector<2xi32>
        %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
        %72 = llvm.fmul %67, %71  : vector<2xf32>
        %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
        %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
        %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
        %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
        %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
        %78 = llvm.and %73, %74  : vector<2xi1>
        %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
        %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
        %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
        %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
        %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
        %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
        %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
        %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
        %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
        %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
        llvm.store %88, %23 : f32, !llvm.ptr
        %89 = llvm.load %23 : !llvm.ptr -> f32
        %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
        %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
        %92 = llvm.fdiv %83, %91  : vector<2xf32>
        llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
        llvm.return %0 : i32
      }
    }
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::TranslateExecutablesPass (iree-hal-translate-executables) //----- //
hal.executable private @main_dispatch_2 {
  hal.executable.variant public @static, target = <"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}> {
    hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>) attributes {translation_info = #iree_codegen.translation_info<CPUDoubleTilingExpert>} {
    ^bb0(%arg0: !hal.device):
      %c1 = arith.constant 1 : index
      hal.return %c1, %c1, %c1 : index, index, index
    }
    builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
      llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
        %0 = llvm.mlir.constant(0 : i32) : i32
        %1 = llvm.mlir.constant(63 : index) : i32
        %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
        %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
        %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
        %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
        %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
        %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
        %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
        %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
        %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
        %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
        %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
        %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
        %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
        %15 = llvm.mlir.constant(0 : index) : i32
        %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
        %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
        %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
        %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
        %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
        %21 = llvm.and %20, %1  : i32
        %22 = llvm.icmp "eq" %21, %15 : i32
        "llvm.intr.assume"(%22) : (i1) -> ()
        %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
        %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
        %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
        %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
        %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
        %28 = llvm.and %27, %1  : i32
        %29 = llvm.icmp "eq" %28, %15 : i32
        "llvm.intr.assume"(%29) : (i1) -> ()
        %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
        %31 = llvm.fmul %30, %14  : vector<2xf32>
        %32 = llvm.fadd %31, %13  : vector<2xf32>
        %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
        %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
        %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
        %36 = llvm.lshr %33, %6  : vector<2xi32>
        %37 = llvm.and %36, %11  : vector<2xi32>
        %38 = llvm.sub %37, %8  : vector<2xi32>
        %39 = llvm.lshr %35, %6  : vector<2xi32>
        %40 = llvm.and %39, %11  : vector<2xi32>
        %41 = llvm.sub %40, %8  : vector<2xi32>
        %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
        %43 = llvm.sub %41, %4  : vector<2xi32>
        %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
        %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
        %46 = llvm.lshr %10, %45  : vector<2xi32>
        %47 = llvm.and %35, %46  : vector<2xi32>
        %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
        %49 = llvm.or %48, %42  : vector<2xi1>
        %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
        %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
        %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
        %53 = llvm.lshr %9, %52  : vector<2xi32>
        %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
        %55 = llvm.lshr %10, %52  : vector<2xi32>
        %56 = llvm.and %33, %55  : vector<2xi32>
        %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
        %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
        %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
        %60 = llvm.and %57, %59  : vector<2xi1>
        %61 = llvm.and %60, %58  : vector<2xi1>
        %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
        %63 = llvm.fsub %34, %62  : vector<2xf32>
        %64 = llvm.and %49, %61  : vector<2xi1>
        %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
        %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
        %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
        %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
        %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
        %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
        %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
        %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
        %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
        %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
        %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
        llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
        llvm.return %0 : i32
      }
    }
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::{anonymous}::ConvertToHALPass (iree-hal-conversion) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  hal.executable private @main_dispatch_0 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  hal.executable private @main_dispatch_1 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  hal.executable private @main_dispatch_2 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %device_0 = hal.ex.shared_device : !hal.device
    %allocator_1 = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    %buffer_2 = hal.allocator.allocate<%allocator_1 : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %device_3 = hal.ex.shared_device : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_3 : !hal.device) flags("None") : !hal.fence
    %c0_i64 = arith.constant 0 : i64
    %transient_buffer = hal.device.queue.alloca<%device_3 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %device_4 = hal.ex.shared_device : !hal.device
    %c-1_i64_5 = arith.constant -1 : i64
    %cmd = hal.command_buffer.create device(%device_4 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    hal.device.switch<%1 : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%1 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      %c0_15 = arith.constant 0 : index
      %c1_16 = arith.constant 1 : index
      %c0_17 = arith.constant 0 : index
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0_17] bindings([
        %c0_15 = (%buffer : !hal.buffer)[%c0, %c2], 
        %c1_16 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      %c1_18 = arith.constant 1 : index
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_0::@static::@main_dispatch_0_generic_2_i8xf32) workgroups([%c1_18, %c1_18, %c1_18])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    %2 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    hal.device.switch<%2 : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%2 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      %c0_15 = arith.constant 0 : index
      %c1_16 = arith.constant 1 : index
      %c0_17 = arith.constant 0 : index
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0_17] bindings([
        %c0_15 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1_16 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      %c1_18 = arith.constant 1 : index
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_1::@static::@main_dispatch_1_softmax_2xf32) workgroups([%c1_18, %c1_18, %c1_18])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    %3 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    hal.device.switch<%3 : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%3 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      %c0_15 = arith.constant 0 : index
      %c1_16 = arith.constant 1 : index
      %c0_17 = arith.constant 0 : index
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0_17] bindings([
        %c0_15 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1_16 = (%buffer_2 : !hal.buffer)[%c0, %c2]
      ])
      %c1_18 = arith.constant 1 : index
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_2::@static::@main_dispatch_2_generic_2_f32xi8) workgroups([%c1_18, %c1_18, %c1_18])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_6 = hal.fence.create device(%device_4 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_4 : !hal.device> affinity(%c-1_i64_5) wait(%fence) signal(%fence_6) commands([%cmd])
    %device_7 = hal.ex.shared_device : !hal.device
    %c-1_i64_8 = arith.constant -1 : i64
    %fence_9 = hal.fence.create device(%device_7 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device_7 : !hal.device> affinity(%c-1_i64_8) wait(%fence_6) signal(%fence_9) buffer(%transient_buffer : !hal.buffer)
    %c-1_i32 = arith.constant -1 : i32
    %status = hal.fence.await until([%fence_9]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %c1_10 = arith.constant 1 : index
    %c2_11 = arith.constant 2 : index
    %c0_12 = arith.constant 0 : index
    %c1_i32_13 = arith.constant 1 : i32
    %c268435464_i32_14 = arith.constant 268435464 : i32
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0_12, %c2] shape([%c1_10, %c2_11]) type(%c268435464_i32_14) encoding(%c1_i32_13) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::FixupLegacySyncPass (iree-hal-fixup-legacy-sync) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  hal.executable private @main_dispatch_0 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  hal.executable private @main_dispatch_1 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  hal.executable private @main_dispatch_2 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %device_0 = hal.ex.shared_device : !hal.device
    %allocator_1 = hal.device.allocator<%device_0 : !hal.device> : !hal.allocator
    %buffer_2 = hal.allocator.allocate<%allocator_1 : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %device_3 = hal.ex.shared_device : !hal.device
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device_3 : !hal.device) flags("None") : !hal.fence
    %c0_i64 = arith.constant 0 : i64
    %transient_buffer = hal.device.queue.alloca<%device_3 : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %device_4 = hal.ex.shared_device : !hal.device
    %c-1_i64_5 = arith.constant -1 : i64
    %cmd = hal.command_buffer.create device(%device_4 : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    hal.device.switch<%1 : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%1 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      %c0_15 = arith.constant 0 : index
      %c1_16 = arith.constant 1 : index
      %c0_17 = arith.constant 0 : index
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0_17] bindings([
        %c0_15 = (%buffer : !hal.buffer)[%c0, %c2], 
        %c1_16 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      %c1_18 = arith.constant 1 : index
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_0::@static::@main_dispatch_0_generic_2_i8xf32) workgroups([%c1_18, %c1_18, %c1_18])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    %2 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    hal.device.switch<%2 : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%2 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      %c0_15 = arith.constant 0 : index
      %c1_16 = arith.constant 1 : index
      %c0_17 = arith.constant 0 : index
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0_17] bindings([
        %c0_15 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1_16 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      %c1_18 = arith.constant 1 : index
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_1::@static::@main_dispatch_1_softmax_2xf32) workgroups([%c1_18, %c1_18, %c1_18])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    %3 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    hal.device.switch<%3 : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%3 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      %c0_15 = arith.constant 0 : index
      %c1_16 = arith.constant 1 : index
      %c0_17 = arith.constant 0 : index
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0_17] bindings([
        %c0_15 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1_16 = (%buffer_2 : !hal.buffer)[%c0, %c2]
      ])
      %c1_18 = arith.constant 1 : index
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_2::@static::@main_dispatch_2_generic_2_f32xi8) workgroups([%c1_18, %c1_18, %c1_18])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_6 = hal.fence.create device(%device_4 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device_4 : !hal.device> affinity(%c-1_i64_5) wait(%fence) signal(%fence_6) commands([%cmd])
    %device_7 = hal.ex.shared_device : !hal.device
    %c-1_i64_8 = arith.constant -1 : i64
    %fence_9 = hal.fence.create device(%device_7 : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device_7 : !hal.device> affinity(%c-1_i64_8) wait(%fence_6) signal(%fence_9) buffer(%transient_buffer : !hal.buffer)
    %c-1_i32 = arith.constant -1 : i32
    %status = hal.fence.await until([%fence_9]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %c1_10 = arith.constant 1 : index
    %c2_11 = arith.constant 2 : index
    %c0_12 = arith.constant 0 : index
    %c1_i32_13 = arith.constant 1 : i32
    %c268435464_i32_14 = arith.constant 268435464 : i32
    %view = hal.buffer_view.create buffer(%buffer_2 : !hal.buffer)[%c0_12, %c2] shape([%c1_10, %c2_11]) type(%c268435464_i32_14) encoding(%c1_i32_13) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  hal.executable private @main_dispatch_0 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  hal.executable private @main_dispatch_1 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  hal.executable private @main_dispatch_2 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %c-1_i64 = arith.constant -1 : i64
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %c0_i64 = arith.constant 0 : i64
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %1 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    hal.device.switch<%1 : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%1 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_0::@static::@main_dispatch_0_generic_2_i8xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%1 : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%1 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_1::@static::@main_dispatch_1_softmax_2xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%1 : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%1 : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_2::@static::@main_dispatch_2_generic_2_f32xi8) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %c-1_i32 = arith.constant -1 : i32
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  hal.executable private @main_dispatch_0 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  hal.executable private @main_dispatch_1 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  hal.executable private @main_dispatch_2 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_0::@static::@main_dispatch_0_generic_2_i8xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_1::@static::@main_dispatch_1_softmax_2xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_2::@static::@main_dispatch_2_generic_2_f32xi8) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  hal.executable private @main_dispatch_0 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  hal.executable private @main_dispatch_1 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  hal.executable private @main_dispatch_2 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_0::@static::@main_dispatch_0_generic_2_i8xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_1::@static::@main_dispatch_1_softmax_2xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_2::@static::@main_dispatch_2_generic_2_f32xi8) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128 = arith.constant 128 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
    %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>) : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_0::@static::@main_dispatch_0_generic_2_i8xf32) workgroups([%c1, %c1, %c1])
    hal.return
  }
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
    %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>) : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_1::@static::@main_dispatch_1_softmax_2xf32) workgroups([%c1, %c1, %c1])
    hal.return
  }
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
    %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>) : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_2::@static::@main_dispatch_2_generic_2_f32xi8) workgroups([%c1, %c1, %c1])
    hal.return
  }
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
  %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  hal.executable private @main_dispatch_0 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  hal.executable private @main_dispatch_1 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  hal.executable private @main_dispatch_2 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_0::@static::@main_dispatch_0_generic_2_i8xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_1::@static::@main_dispatch_1_softmax_2xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_2::@static::@main_dispatch_2_generic_2_f32xi8) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  hal.executable private @main_dispatch_0 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  hal.executable private @main_dispatch_1 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  hal.executable private @main_dispatch_2 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_0::@static::@main_dispatch_0_generic_2_i8xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_1::@static::@main_dispatch_1_softmax_2xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_2::@static::@main_dispatch_2_generic_2_f32xi8) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#translation = #iree_codegen.translation_info<CPUDoubleTilingExpert>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  hal.executable private @main_dispatch_0 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  hal.executable private @main_dispatch_1 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  hal.executable private @main_dispatch_2 {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(0) layout(#pipeline_layout) attributes {translation_info = #translation} {
      ^bb0(%arg0: !hal.device):
        %c1 = arith.constant 1 : index
        hal.return %c1, %c1, %c1 : index, index, index
      }
      builtin.module attributes {llvm.data_layout = "e-m:e-p:32:32-i64:64-n32-S128", llvm.target_triple = "riscv32-pc-linux-elf"} {
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_0::@static::@main_dispatch_0_generic_2_i8xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_1::@static::@main_dispatch_1_softmax_2xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@main_dispatch_2::@static::@main_dispatch_2_generic_2_f32xi8) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After LLVMCPULinkExecutables (iree-llvmcpu-link-executables) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(1) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(2) layout(#pipeline_layout)
      builtin.module {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@llvm_module_linked_llvm_cpu::@static::@main_dispatch_0_generic_2_i8xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@llvm_module_linked_llvm_cpu::@static::@main_dispatch_1_softmax_2xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@llvm_module_linked_llvm_cpu::@static::@main_dispatch_2_generic_2_f32xi8) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
hal.executable private @llvm_module_linked_llvm_cpu {
  hal.executable.variant public @static, target = <"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}> {
    hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>)
    hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(1) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>)
    hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(2) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>)
    builtin.module {
      llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
        %0 = llvm.mlir.constant(0 : i32) : i32
        %1 = llvm.mlir.constant(63 : index) : i32
        %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
        %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
        %4 = llvm.mlir.constant(0 : index) : i32
        %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
        %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
        %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
        %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
        %9 = llvm.and %8, %1  : i32
        %10 = llvm.icmp "eq" %9, %4 : i32
        "llvm.intr.assume"(%10) : (i1) -> ()
        %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
        %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
        %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
        %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
        %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
        %16 = llvm.and %15, %1  : i32
        %17 = llvm.icmp "eq" %16, %4 : i32
        "llvm.intr.assume"(%17) : (i1) -> ()
        %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
        %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
        %20 = llvm.fsub %19, %3  : vector<2xf32>
        %21 = llvm.fmul %20, %2  : vector<2xf32>
        llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
        llvm.return %0 : i32
      }
      llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
        %0 = llvm.mlir.constant(0 : i32) : i32
        %1 = llvm.mlir.constant(0 : i64) : i64
        %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
        %3 = llvm.mlir.constant(63 : index) : i32
        %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
        %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
        %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
        %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
        %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
        %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
        %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
        %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
        %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
        %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
        %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
        %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
        %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
        %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
        %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
        %19 = llvm.mlir.constant(0 : index) : i32
        %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
        %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
        %22 = llvm.mlir.constant(1 : index) : i32
        %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
        %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
        %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
        %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
        %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
        %28 = llvm.and %27, %3  : i32
        %29 = llvm.icmp "eq" %28, %19 : i32
        "llvm.intr.assume"(%29) : (i1) -> ()
        %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
        %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
        %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
        %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
        %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
        %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
        %36 = llvm.and %35, %3  : i32
        %37 = llvm.icmp "eq" %36, %19 : i32
        "llvm.intr.assume"(%37) : (i1) -> ()
        %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
        %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
        %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
        %41 = llvm.fcmp "ogt" %40, %39 : f32
        %42 = llvm.select %41, %40, %39 : i1, f32
        %43 = llvm.fcmp "uno" %40, %39 : f32
        %44 = llvm.select %43, %2, %42 : i1, f32
        %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
        %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
        %47 = llvm.mlir.undef : vector<1xf32>
        %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
        %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
        llvm.store %49, %23 : f32, !llvm.ptr
        %50 = llvm.load %23 : !llvm.ptr -> f32
        %51 = llvm.mlir.undef : vector<2xf32>
        %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
        %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
        %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
        %55 = llvm.fsub %38, %53  : vector<2xf32>
        %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
        %57 = llvm.fmul %55, %5  : vector<2xf32>
        %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
        %59 = llvm.fmul %58, %4  : vector<2xf32>
        %60 = llvm.fsub %55, %59  : vector<2xf32>
        %61 = llvm.fmul %60, %60  : vector<2xf32>
        %62 = llvm.fmul %61, %61  : vector<2xf32>
        %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
        %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
        %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
        %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
        %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
        %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
        %69 = llvm.add %68, %12  : vector<2xi32>
        %70 = llvm.shl %69, %11  : vector<2xi32>
        %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
        %72 = llvm.fmul %67, %71  : vector<2xf32>
        %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
        %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
        %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
        %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
        %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
        %78 = llvm.and %73, %74  : vector<2xi1>
        %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
        %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
        %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
        %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
        %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
        %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
        %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
        %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
        %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
        %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
        llvm.store %88, %23 : f32, !llvm.ptr
        %89 = llvm.load %23 : !llvm.ptr -> f32
        %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
        %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
        %92 = llvm.fdiv %83, %91  : vector<2xf32>
        llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
        llvm.return %0 : i32
      }
      llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
        %0 = llvm.mlir.constant(0 : i32) : i32
        %1 = llvm.mlir.constant(63 : index) : i32
        %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
        %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
        %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
        %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
        %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
        %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
        %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
        %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
        %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
        %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
        %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
        %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
        %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
        %15 = llvm.mlir.constant(0 : index) : i32
        %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
        %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
        %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
        %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
        %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
        %21 = llvm.and %20, %1  : i32
        %22 = llvm.icmp "eq" %21, %15 : i32
        "llvm.intr.assume"(%22) : (i1) -> ()
        %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
        %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
        %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
        %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
        %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
        %28 = llvm.and %27, %1  : i32
        %29 = llvm.icmp "eq" %28, %15 : i32
        "llvm.intr.assume"(%29) : (i1) -> ()
        %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
        %31 = llvm.fmul %30, %14  : vector<2xf32>
        %32 = llvm.fadd %31, %13  : vector<2xf32>
        %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
        %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
        %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
        %36 = llvm.lshr %33, %6  : vector<2xi32>
        %37 = llvm.and %36, %11  : vector<2xi32>
        %38 = llvm.sub %37, %8  : vector<2xi32>
        %39 = llvm.lshr %35, %6  : vector<2xi32>
        %40 = llvm.and %39, %11  : vector<2xi32>
        %41 = llvm.sub %40, %8  : vector<2xi32>
        %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
        %43 = llvm.sub %41, %4  : vector<2xi32>
        %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
        %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
        %46 = llvm.lshr %10, %45  : vector<2xi32>
        %47 = llvm.and %35, %46  : vector<2xi32>
        %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
        %49 = llvm.or %48, %42  : vector<2xi1>
        %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
        %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
        %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
        %53 = llvm.lshr %9, %52  : vector<2xi32>
        %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
        %55 = llvm.lshr %10, %52  : vector<2xi32>
        %56 = llvm.and %33, %55  : vector<2xi32>
        %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
        %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
        %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
        %60 = llvm.and %57, %59  : vector<2xi1>
        %61 = llvm.and %60, %58  : vector<2xi1>
        %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
        %63 = llvm.fsub %34, %62  : vector<2xf32>
        %64 = llvm.and %49, %61  : vector<2xi1>
        %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
        %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
        %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
        %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
        %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
        %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
        %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
        %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
        %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
        %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
        %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
        llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
        llvm.return %0 : i32
      }
    }
  }
}

// -----// IR Dump After LLVMCPUAssignConstantOrdinals (iree-llvmcpu-assign-constant-ordinals) //----- //
hal.executable.variant public @static, target = <"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}> {
  hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>)
  hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(1) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>)
  hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(2) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>)
  builtin.module {
    llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
      %0 = llvm.mlir.constant(0 : i32) : i32
      %1 = llvm.mlir.constant(63 : index) : i32
      %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
      %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
      %4 = llvm.mlir.constant(0 : index) : i32
      %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
      %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
      %9 = llvm.and %8, %1  : i32
      %10 = llvm.icmp "eq" %9, %4 : i32
      "llvm.intr.assume"(%10) : (i1) -> ()
      %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
      %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
      %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
      %16 = llvm.and %15, %1  : i32
      %17 = llvm.icmp "eq" %16, %4 : i32
      "llvm.intr.assume"(%17) : (i1) -> ()
      %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
      %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
      %20 = llvm.fsub %19, %3  : vector<2xf32>
      %21 = llvm.fmul %20, %2  : vector<2xf32>
      llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
      llvm.return %0 : i32
    }
    llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
      %0 = llvm.mlir.constant(0 : i32) : i32
      %1 = llvm.mlir.constant(0 : i64) : i64
      %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
      %3 = llvm.mlir.constant(63 : index) : i32
      %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
      %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
      %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
      %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
      %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
      %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
      %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
      %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
      %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
      %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
      %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
      %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
      %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
      %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
      %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
      %19 = llvm.mlir.constant(0 : index) : i32
      %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
      %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
      %22 = llvm.mlir.constant(1 : index) : i32
      %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
      %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
      %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
      %28 = llvm.and %27, %3  : i32
      %29 = llvm.icmp "eq" %28, %19 : i32
      "llvm.intr.assume"(%29) : (i1) -> ()
      %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
      %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
      %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
      %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
      %36 = llvm.and %35, %3  : i32
      %37 = llvm.icmp "eq" %36, %19 : i32
      "llvm.intr.assume"(%37) : (i1) -> ()
      %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
      %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
      %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
      %41 = llvm.fcmp "ogt" %40, %39 : f32
      %42 = llvm.select %41, %40, %39 : i1, f32
      %43 = llvm.fcmp "uno" %40, %39 : f32
      %44 = llvm.select %43, %2, %42 : i1, f32
      %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
      %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
      %47 = llvm.mlir.undef : vector<1xf32>
      %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
      %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
      llvm.store %49, %23 : f32, !llvm.ptr
      %50 = llvm.load %23 : !llvm.ptr -> f32
      %51 = llvm.mlir.undef : vector<2xf32>
      %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
      %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
      %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
      %55 = llvm.fsub %38, %53  : vector<2xf32>
      %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
      %57 = llvm.fmul %55, %5  : vector<2xf32>
      %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
      %59 = llvm.fmul %58, %4  : vector<2xf32>
      %60 = llvm.fsub %55, %59  : vector<2xf32>
      %61 = llvm.fmul %60, %60  : vector<2xf32>
      %62 = llvm.fmul %61, %61  : vector<2xf32>
      %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
      %69 = llvm.add %68, %12  : vector<2xi32>
      %70 = llvm.shl %69, %11  : vector<2xi32>
      %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
      %72 = llvm.fmul %67, %71  : vector<2xf32>
      %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
      %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
      %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
      %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
      %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
      %78 = llvm.and %73, %74  : vector<2xi1>
      %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
      %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
      %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
      %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
      %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
      %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
      %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
      %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
      %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
      %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
      llvm.store %88, %23 : f32, !llvm.ptr
      %89 = llvm.load %23 : !llvm.ptr -> f32
      %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
      %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
      %92 = llvm.fdiv %83, %91  : vector<2xf32>
      llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
      llvm.return %0 : i32
    }
    llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
      %0 = llvm.mlir.constant(0 : i32) : i32
      %1 = llvm.mlir.constant(63 : index) : i32
      %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
      %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
      %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
      %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
      %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
      %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
      %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
      %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
      %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
      %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
      %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
      %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
      %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
      %15 = llvm.mlir.constant(0 : index) : i32
      %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
      %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
      %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
      %21 = llvm.and %20, %1  : i32
      %22 = llvm.icmp "eq" %21, %15 : i32
      "llvm.intr.assume"(%22) : (i1) -> ()
      %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
      %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
      %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
      %28 = llvm.and %27, %1  : i32
      %29 = llvm.icmp "eq" %28, %15 : i32
      "llvm.intr.assume"(%29) : (i1) -> ()
      %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
      %31 = llvm.fmul %30, %14  : vector<2xf32>
      %32 = llvm.fadd %31, %13  : vector<2xf32>
      %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
      %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
      %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
      %36 = llvm.lshr %33, %6  : vector<2xi32>
      %37 = llvm.and %36, %11  : vector<2xi32>
      %38 = llvm.sub %37, %8  : vector<2xi32>
      %39 = llvm.lshr %35, %6  : vector<2xi32>
      %40 = llvm.and %39, %11  : vector<2xi32>
      %41 = llvm.sub %40, %8  : vector<2xi32>
      %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
      %43 = llvm.sub %41, %4  : vector<2xi32>
      %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
      %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
      %46 = llvm.lshr %10, %45  : vector<2xi32>
      %47 = llvm.and %35, %46  : vector<2xi32>
      %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
      %49 = llvm.or %48, %42  : vector<2xi1>
      %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
      %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
      %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
      %53 = llvm.lshr %9, %52  : vector<2xi32>
      %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
      %55 = llvm.lshr %10, %52  : vector<2xi32>
      %56 = llvm.and %33, %55  : vector<2xi32>
      %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
      %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
      %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
      %60 = llvm.and %57, %59  : vector<2xi1>
      %61 = llvm.and %60, %58  : vector<2xi1>
      %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %63 = llvm.fsub %34, %62  : vector<2xf32>
      %64 = llvm.and %49, %61  : vector<2xi1>
      %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
      %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
      %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
      %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
      %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
      %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
      %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
      %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
      %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
      %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
      llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
      llvm.return %0 : i32
    }
  }
}

// -----// IR Dump After LLVMCPUAssignImportOrdinals (iree-llvmcpu-assign-import-ordinals) //----- //
hal.executable.variant public @static, target = <"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}> {
  hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>)
  hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(1) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>)
  hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(2) layout(#hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>)
  builtin.module {
    llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
      %0 = llvm.mlir.constant(0 : i32) : i32
      %1 = llvm.mlir.constant(63 : index) : i32
      %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
      %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
      %4 = llvm.mlir.constant(0 : index) : i32
      %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
      %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
      %9 = llvm.and %8, %1  : i32
      %10 = llvm.icmp "eq" %9, %4 : i32
      "llvm.intr.assume"(%10) : (i1) -> ()
      %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
      %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
      %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
      %16 = llvm.and %15, %1  : i32
      %17 = llvm.icmp "eq" %16, %4 : i32
      "llvm.intr.assume"(%17) : (i1) -> ()
      %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
      %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
      %20 = llvm.fsub %19, %3  : vector<2xf32>
      %21 = llvm.fmul %20, %2  : vector<2xf32>
      llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
      llvm.return %0 : i32
    }
    llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
      %0 = llvm.mlir.constant(0 : i32) : i32
      %1 = llvm.mlir.constant(0 : i64) : i64
      %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
      %3 = llvm.mlir.constant(63 : index) : i32
      %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
      %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
      %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
      %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
      %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
      %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
      %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
      %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
      %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
      %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
      %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
      %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
      %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
      %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
      %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
      %19 = llvm.mlir.constant(0 : index) : i32
      %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
      %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
      %22 = llvm.mlir.constant(1 : index) : i32
      %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
      %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
      %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
      %28 = llvm.and %27, %3  : i32
      %29 = llvm.icmp "eq" %28, %19 : i32
      "llvm.intr.assume"(%29) : (i1) -> ()
      %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
      %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
      %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
      %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
      %36 = llvm.and %35, %3  : i32
      %37 = llvm.icmp "eq" %36, %19 : i32
      "llvm.intr.assume"(%37) : (i1) -> ()
      %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
      %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
      %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
      %41 = llvm.fcmp "ogt" %40, %39 : f32
      %42 = llvm.select %41, %40, %39 : i1, f32
      %43 = llvm.fcmp "uno" %40, %39 : f32
      %44 = llvm.select %43, %2, %42 : i1, f32
      %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
      %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
      %47 = llvm.mlir.undef : vector<1xf32>
      %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
      %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
      llvm.store %49, %23 : f32, !llvm.ptr
      %50 = llvm.load %23 : !llvm.ptr -> f32
      %51 = llvm.mlir.undef : vector<2xf32>
      %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
      %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
      %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
      %55 = llvm.fsub %38, %53  : vector<2xf32>
      %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
      %57 = llvm.fmul %55, %5  : vector<2xf32>
      %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
      %59 = llvm.fmul %58, %4  : vector<2xf32>
      %60 = llvm.fsub %55, %59  : vector<2xf32>
      %61 = llvm.fmul %60, %60  : vector<2xf32>
      %62 = llvm.fmul %61, %61  : vector<2xf32>
      %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
      %69 = llvm.add %68, %12  : vector<2xi32>
      %70 = llvm.shl %69, %11  : vector<2xi32>
      %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
      %72 = llvm.fmul %67, %71  : vector<2xf32>
      %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
      %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
      %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
      %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
      %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
      %78 = llvm.and %73, %74  : vector<2xi1>
      %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
      %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
      %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
      %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
      %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
      %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
      %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
      %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
      %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
      %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
      llvm.store %88, %23 : f32, !llvm.ptr
      %89 = llvm.load %23 : !llvm.ptr -> f32
      %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
      %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
      %92 = llvm.fdiv %83, %91  : vector<2xf32>
      llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
      llvm.return %0 : i32
    }
    llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
      %0 = llvm.mlir.constant(0 : i32) : i32
      %1 = llvm.mlir.constant(63 : index) : i32
      %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
      %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
      %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
      %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
      %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
      %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
      %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
      %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
      %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
      %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
      %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
      %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
      %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
      %15 = llvm.mlir.constant(0 : index) : i32
      %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
      %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
      %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
      %21 = llvm.and %20, %1  : i32
      %22 = llvm.icmp "eq" %21, %15 : i32
      "llvm.intr.assume"(%22) : (i1) -> ()
      %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
      %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
      %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
      %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
      %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
      %28 = llvm.and %27, %1  : i32
      %29 = llvm.icmp "eq" %28, %15 : i32
      "llvm.intr.assume"(%29) : (i1) -> ()
      %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
      %31 = llvm.fmul %30, %14  : vector<2xf32>
      %32 = llvm.fadd %31, %13  : vector<2xf32>
      %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
      %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
      %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
      %36 = llvm.lshr %33, %6  : vector<2xi32>
      %37 = llvm.and %36, %11  : vector<2xi32>
      %38 = llvm.sub %37, %8  : vector<2xi32>
      %39 = llvm.lshr %35, %6  : vector<2xi32>
      %40 = llvm.and %39, %11  : vector<2xi32>
      %41 = llvm.sub %40, %8  : vector<2xi32>
      %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
      %43 = llvm.sub %41, %4  : vector<2xi32>
      %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
      %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
      %46 = llvm.lshr %10, %45  : vector<2xi32>
      %47 = llvm.and %35, %46  : vector<2xi32>
      %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
      %49 = llvm.or %48, %42  : vector<2xi1>
      %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
      %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
      %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
      %53 = llvm.lshr %9, %52  : vector<2xi32>
      %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
      %55 = llvm.lshr %10, %52  : vector<2xi32>
      %56 = llvm.and %33, %55  : vector<2xi32>
      %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
      %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
      %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
      %60 = llvm.and %57, %59  : vector<2xi1>
      %61 = llvm.and %60, %58  : vector<2xi1>
      %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %63 = llvm.fsub %34, %62  : vector<2xf32>
      %64 = llvm.and %49, %61  : vector<2xi1>
      %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
      %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
      %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
      %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
      %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
      %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
      %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
      %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
      %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
      %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
      %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
      llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
      llvm.return %0 : i32
    }
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::LinkTargetExecutablesPass (iree-hal-link-target-executables) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(1) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(2) layout(#pipeline_layout)
      builtin.module {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@llvm_module_linked_llvm_cpu::@static::@main_dispatch_0_generic_2_i8xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@llvm_module_linked_llvm_cpu::@static::@main_dispatch_1_softmax_2xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@llvm_module_linked_llvm_cpu::@static::@main_dispatch_2_generic_2_f32xi8) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(1) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(2) layout(#pipeline_layout)
      builtin.module {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@llvm_module_linked_llvm_cpu::@static::@main_dispatch_0_generic_2_i8xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@llvm_module_linked_llvm_cpu::@static::@main_dispatch_1_softmax_2xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@llvm_module_linked_llvm_cpu::@static::@main_dispatch_2_generic_2_f32xi8) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::LinkExecutablesPass (iree-hal-link-executables) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(1) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(2) layout(#pipeline_layout)
      builtin.module {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@llvm_module_linked_llvm_cpu::@static::@main_dispatch_0_generic_2_i8xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@llvm_module_linked_llvm_cpu::@static::@main_dispatch_1_softmax_2xf32) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
      ])
      hal.command_buffer.dispatch.symbol<%cmd : !hal.command_buffer> target(@llvm_module_linked_llvm_cpu::@static::@main_dispatch_2_generic_2_f32xi8) workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ResolveExportOrdinalsPass (iree-hal-resolve-export-ordinals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(1) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(2) layout(#pipeline_layout)
      builtin.module {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      %1 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
      %exe = hal.executable.lookup device(%1 : !hal.device) executable(@llvm_module_linked_llvm_cpu) : !hal.executable
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[0] workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      %1 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
      %exe = hal.executable.lookup device(%1 : !hal.device) executable(@llvm_module_linked_llvm_cpu) : !hal.executable
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[1] workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %pipeline_layout = hal.pipeline_layout.lookup device(%device : !hal.device) layout(#pipeline_layout) : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%pipeline_layout : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
      ])
      %1 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
      %exe = hal.executable.lookup device(%1 : !hal.device) executable(@llvm_module_linked_llvm_cpu) : !hal.executable
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%exe : !hal.executable)[2] workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::MaterializeResourceCachesPass (iree-hal-materialize-resource-caches) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %0 = hal.device.switch<%device : !hal.device> -> !hal.executable
    #hal.device.match.executable.format<"static"> {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %_pipeline_layout_0_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %_pipeline_layout_0_1 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0_0, %_pipeline_layout_0_1]) : !hal.executable
      hal.return %exe : !hal.executable
    },
    #hal.match.always {
      %1 = util.null : !hal.executable
      hal.return %1 : !hal.executable
    }
    util.global.store %0, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(1) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(2) layout(#pipeline_layout)
      builtin.module {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      %1 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
      %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
      ])
      %1 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
      %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.device.switch<%device : !hal.device>
    #hal.device.match.executable.format<"static"> {
      %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
      hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
        %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
        %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
      ])
      %1 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
      %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
      hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
      hal.return
    }
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::InlineDeviceSwitchesPass (iree-hal-inline-device-switches) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::InlineDeviceSwitchesPass (iree-hal-inline-device-switches) //----- //
util.initializer {
  %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  %device = hal.ex.shared_device : !hal.device
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::InlineDeviceSwitchesPass (iree-hal-inline-device-switches) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_pipeline_layout_0_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_pipeline_layout_0_1 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0_0, %_pipeline_layout_0_1]) : !hal.executable
  cf.br ^bb5(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %true = arith.constant true
  cf.cond_br %true, ^bb3, ^bb4
^bb3:  // pred: ^bb2
  %0 = util.null : !hal.executable
  cf.br ^bb5(%0 : !hal.executable)
^bb4:  // pred: ^bb2
  util.unreachable "device not supported in the compiled configuration"
^bb5(%1: !hal.executable):  // 2 preds: ^bb1, ^bb3
  util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::InlineDeviceSwitchesPass (iree-hal-inline-device-switches) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128 = arith.constant 128 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  %1 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
  %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
  cf.br ^bb3
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
^bb3:  // pred: ^bb1
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  %ok_1, %value_2 = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
  cf.cond_br %value_2, ^bb4, ^bb5
^bb4:  // pred: ^bb3
  %_pipeline_layout_0_3 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0_3 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  %2 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
  %_executable_llvm_module_linked_llvm_cpu_4 = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu_4 : !hal.executable)[1] workgroups([%c1, %c1, %c1])
  cf.br ^bb6
^bb5:  // pred: ^bb3
  util.unreachable "device not supported in the compiled configuration"
^bb6:  // pred: ^bb4
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  %ok_5, %value_6 = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
  cf.cond_br %value_6, ^bb7, ^bb8
^bb7:  // pred: ^bb6
  %_pipeline_layout_0_7 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0_7 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
  ])
  %3 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
  %_executable_llvm_module_linked_llvm_cpu_8 = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu_8 : !hal.executable)[2] workgroups([%c1, %c1, %c1])
  cf.br ^bb9
^bb8:  // pred: ^bb6
  util.unreachable "device not supported in the compiled configuration"
^bb9:  // pred: ^bb7
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_9 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_9) commands([%cmd])
  %fence_10 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_9) signal(%fence_10) buffer(%transient_buffer : !hal.buffer)
  %status = hal.fence.await until([%fence_10]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::MemoizeDeviceQueriesPass (iree-hal-memoize-device-queries) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    util.global.store %ok, @_device_query_0_ok : i1
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %_device_query_0_ok = util.global.load @_device_query_0_ok : i1
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_pipeline_layout_0_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_pipeline_layout_0_1 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0_0, %_pipeline_layout_0_1]) : !hal.executable
    cf.br ^bb5(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %true = arith.constant true
    cf.cond_br %true, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %0 = util.null : !hal.executable
    cf.br ^bb5(%0 : !hal.executable)
  ^bb4:  // pred: ^bb2
    util.unreachable "device not supported in the compiled configuration"
  ^bb5(%1: !hal.executable):  // 2 preds: ^bb1, ^bb3
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(1) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(2) layout(#pipeline_layout)
      builtin.module {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %_device_query_0_ok = util.global.load @_device_query_0_ok : i1
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    %1 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    cf.br ^bb3
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  ^bb3:  // pred: ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    %_device_query_0_ok_1 = util.global.load @_device_query_0_ok : i1
    %_device_query_0_2 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0_2, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %_pipeline_layout_0_3 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0_3 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    %2 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    %_executable_llvm_module_linked_llvm_cpu_4 = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu_4 : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    cf.br ^bb6
  ^bb5:  // pred: ^bb3
    util.unreachable "device not supported in the compiled configuration"
  ^bb6:  // pred: ^bb4
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    %_device_query_0_ok_5 = util.global.load @_device_query_0_ok : i1
    %_device_query_0_6 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0_6, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    %_pipeline_layout_0_7 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0_7 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    %3 = hal.command_buffer.device<%cmd : !hal.command_buffer> : !hal.device
    %_executable_llvm_module_linked_llvm_cpu_8 = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu_8 : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    cf.br ^bb9
  ^bb8:  // pred: ^bb6
    util.unreachable "device not supported in the compiled configuration"
  ^bb9:  // pred: ^bb7
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_9 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_9) commands([%cmd])
    %fence_10 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_9) signal(%fence_10) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_10]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    util.global.store %ok, @_device_query_0_ok : i1
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb5(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %true = arith.constant true
    cf.cond_br %true, ^bb3, ^bb4
  ^bb3:  // pred: ^bb2
    %0 = util.null : !hal.executable
    cf.br ^bb5(%0 : !hal.executable)
  ^bb4:  // pred: ^bb2
    util.unreachable "device not supported in the compiled configuration"
  ^bb5(%1: !hal.executable):  // 2 preds: ^bb1, ^bb3
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(1) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(2) layout(#pipeline_layout)
      builtin.module {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    cf.br ^bb3
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  ^bb3:  // pred: ^bb1
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    cf.cond_br %_device_query_0, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    cf.br ^bb6
  ^bb5:  // pred: ^bb3
    util.unreachable "device not supported in the compiled configuration"
  ^bb6:  // pred: ^bb4
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    cf.cond_br %_device_query_0, ^bb7, ^bb8
  ^bb7:  // pred: ^bb6
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    cf.br ^bb9
  ^bb8:  // pred: ^bb6
    util.unreachable "device not supported in the compiled configuration"
  ^bb9:  // pred: ^bb7
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    util.global.store %ok, @_device_query_0_ok : i1
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(1) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(2) layout(#pipeline_layout)
      builtin.module {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    util.global.store %ok, @_device_query_0_ok : i1
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(1) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(2) layout(#pipeline_layout)
      builtin.module {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
  util.global.store %value, @_device_query_0 : i1
  util.global.store %ok, @_device_query_0_ok : i1
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  %device = hal.ex.shared_device : !hal.device
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %device = hal.ex.shared_device : !hal.device
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128 = arith.constant 128 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
  %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_device_query_0 : i1
  util.global private @_device_query_0_ok : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.global.store %ok, @_device_query_0_ok : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(1) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(2) layout(#pipeline_layout)
      builtin.module {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c1 = arith.constant 1 : index
    %c1_i32 = arith.constant 1 : i32
    %c268435464_i32 = arith.constant 268435464 : i32
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c128 = arith.constant 128 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_device_query_0 : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(1) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(2) layout(#pipeline_layout)
      builtin.module {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c1 = arith.constant 1 : index
    %c1_i32 = arith.constant 1 : i32
    %c268435464_i32 = arith.constant 268435464 : i32
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c128 = arith.constant 128 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_device_query_0 : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(1) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(2) layout(#pipeline_layout)
      builtin.module {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c1 = arith.constant 1 : index
    %c1_i32 = arith.constant 1 : i32
    %c268435464_i32 = arith.constant 268435464 : i32
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c128 = arith.constant 128 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
  util.global.store %value, @_device_query_0 : i1
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  %device = hal.ex.shared_device : !hal.device
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
util.initializer {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %device = hal.ex.shared_device : !hal.device
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer.return
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::ElideRedundantCommandsPass (iree-hal-elide-redundant-commands) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c1 = arith.constant 1 : index
  %c1_i32 = arith.constant 1 : i32
  %c268435464_i32 = arith.constant 268435464 : i32
  %c0 = arith.constant 0 : index
  %c2 = arith.constant 2 : index
  %c128 = arith.constant 128 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c-1_i32 = arith.constant -1 : i32
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
  %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_device_query_0 : i1
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    util.initializer.return
  }
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.initializer.return
  }
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer {
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    util.initializer.return
  }
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(1) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(2) layout(#pipeline_layout)
      builtin.module {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c1 = arith.constant 1 : index
    %c1_i32 = arith.constant 1 : i32
    %c268435464_i32 = arith.constant 268435464 : i32
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c128 = arith.constant 128 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
  util.global.store %value, @_device_query_0 : i1
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %device = hal.ex.shared_device : !hal.device
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  %device = hal.ex.shared_device : !hal.device
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c1 = arith.constant 1 : index
  %c1_i32 = arith.constant 1 : i32
  %c268435464_i32 = arith.constant 268435464 : i32
  %c0 = arith.constant 0 : index
  %c2 = arith.constant 2 : index
  %c128 = arith.constant 128 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c-1_i32 = arith.constant -1 : i32
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
  %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After CombineInitializers (iree-util-combine-initializers) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#pipeline_layout = #hal.pipeline.layout<push_constants = 0, sets = [<0, bindings = [<0, storage_buffer, ReadOnly>, <1, storage_buffer>]>]>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %device_0 = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_1 = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_1 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device_2 = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_2 : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    cf.br ^bb4
  ^bb4:  // pred: ^bb3
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.variant public @static, target = #executable_target_static {
      hal.executable.export public @main_dispatch_0_generic_2_i8xf32 ordinal(0) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_1_softmax_2xf32 ordinal(1) layout(#pipeline_layout)
      hal.executable.export public @main_dispatch_2_generic_2_f32xi8 ordinal(2) layout(#pipeline_layout)
      builtin.module {
        llvm.func @main_dispatch_0_generic_2_i8xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<0.0125187514> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<-1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %4 = llvm.mlir.constant(0 : index) : i32
          %5 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %6 = llvm.extractvalue %5[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %7 = llvm.load %6 : !llvm.ptr -> !llvm.ptr
          %8 = llvm.ptrtoint %7 : !llvm.ptr to i32
          %9 = llvm.and %8, %1  : i32
          %10 = llvm.icmp "eq" %9, %4 : i32
          "llvm.intr.assume"(%10) : (i1) -> ()
          %11 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %12 = llvm.extractvalue %11[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %13 = llvm.getelementptr %12[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %14 = llvm.load %13 : !llvm.ptr -> !llvm.ptr
          %15 = llvm.ptrtoint %14 : !llvm.ptr to i32
          %16 = llvm.and %15, %1  : i32
          %17 = llvm.icmp "eq" %16, %4 : i32
          "llvm.intr.assume"(%17) : (i1) -> ()
          %18 = llvm.load %7 {alignment = 1 : i64} : !llvm.ptr -> vector<2xi8>
          %19 = llvm.sitofp %18 : vector<2xi8> to vector<2xf32>
          %20 = llvm.fsub %19, %3  : vector<2xf32>
          %21 = llvm.fmul %20, %2  : vector<2xf32>
          llvm.store %21, %14 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_1_softmax_2xf32(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(0 : i64) : i64
          %2 = llvm.mlir.constant(0x7FC00000 : f32) : f32
          %3 = llvm.mlir.constant(63 : index) : i32
          %4 = llvm.mlir.constant(dense<0.693147182> : vector<2xf32>) : vector<2xf32>
          %5 = llvm.mlir.constant(dense<1.44269502> : vector<2xf32>) : vector<2xf32>
          %6 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %7 = llvm.mlir.constant(dense<0.499705136> : vector<2xf32>) : vector<2xf32>
          %8 = llvm.mlir.constant(dense<0.168738902> : vector<2xf32>) : vector<2xf32>
          %9 = llvm.mlir.constant(dense<0.0366896503> : vector<2xf32>) : vector<2xf32>
          %10 = llvm.mlir.constant(dense<1.314350e-02> : vector<2xf32>) : vector<2xf32>
          %11 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %13 = llvm.mlir.constant(dense<0.000000e+00> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<0x7F800000> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(dense<0xFF800000> : vector<2xf32>) : vector<2xf32>
          %16 = llvm.mlir.constant(dense<1.17549435E-38> : vector<2xf32>) : vector<2xf32>
          %17 = llvm.mlir.constant(dense<-127> : vector<2xi32>) : vector<2xi32>
          %18 = llvm.mlir.constant(dense<0.000000e+00> : vector<1xf32>) : vector<1xf32>
          %19 = llvm.mlir.constant(0 : index) : i32
          %20 = llvm.mlir.constant(dense<0.000000e+00> : vector<f32>) : vector<1xf32>
          %21 = llvm.mlir.constant(dense<-1.000000e+30> : vector<f32>) : vector<1xf32>
          %22 = llvm.mlir.constant(1 : index) : i32
          %23 = llvm.alloca %22 x f32 {alignment = 64 : i64} : (i32) -> !llvm.ptr
          %24 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %25 = llvm.extractvalue %24[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %3  : i32
          %29 = llvm.icmp "eq" %28, %19 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %31 = llvm.extractvalue %30[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %32 = llvm.getelementptr %31[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %33 = llvm.load %32 : !llvm.ptr -> !llvm.ptr
          %34 = llvm.getelementptr %33[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %35 = llvm.ptrtoint %34 : !llvm.ptr to i32
          %36 = llvm.and %35, %3  : i32
          %37 = llvm.icmp "eq" %36, %19 : i32
          "llvm.intr.assume"(%37) : (i1) -> ()
          %38 = llvm.load %26 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %39 = llvm.extractelement %21[%19 : i32] : vector<1xf32>
          %40 = llvm.intr.vector.reduce.fmax(%38)  : (vector<2xf32>) -> f32
          %41 = llvm.fcmp "ogt" %40, %39 : f32
          %42 = llvm.select %41, %40, %39 : i1, f32
          %43 = llvm.fcmp "uno" %40, %39 : f32
          %44 = llvm.select %43, %2, %42 : i1, f32
          %45 = llvm.insertelement %44, %18[%19 : i32] : vector<1xf32>
          %46 = llvm.extractelement %45[%1 : i64] : vector<1xf32>
          %47 = llvm.mlir.undef : vector<1xf32>
          %48 = llvm.insertelement %46, %47[%0 : i32] : vector<1xf32>
          %49 = llvm.extractelement %48[%19 : i32] : vector<1xf32>
          llvm.store %49, %23 : f32, !llvm.ptr
          %50 = llvm.load %23 : !llvm.ptr -> f32
          %51 = llvm.mlir.undef : vector<2xf32>
          %52 = llvm.insertelement %50, %51[%0 : i32] : vector<2xf32>
          %53 = llvm.shufflevector %52, %51 [0, 0] : vector<2xf32> 
          %54 = llvm.extractelement %20[%19 : i32] : vector<1xf32>
          %55 = llvm.fsub %38, %53  : vector<2xf32>
          %56 = llvm.fcmp "uno" %55, %55 : vector<2xf32>
          %57 = llvm.fmul %55, %5  : vector<2xf32>
          %58 = llvm.intr.floor(%57)  : (vector<2xf32>) -> vector<2xf32>
          %59 = llvm.fmul %58, %4  : vector<2xf32>
          %60 = llvm.fsub %55, %59  : vector<2xf32>
          %61 = llvm.fmul %60, %60  : vector<2xf32>
          %62 = llvm.fmul %61, %61  : vector<2xf32>
          %63 = llvm.intr.fma(%6, %60, %6)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %64 = llvm.intr.fma(%8, %60, %7)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %65 = llvm.intr.fma(%10, %60, %9)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %66 = llvm.intr.fma(%64, %61, %63)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.intr.fma(%65, %62, %66)  : (vector<2xf32>, vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %68 = llvm.fptosi %58 : vector<2xf32> to vector<2xi32>
          %69 = llvm.add %68, %12  : vector<2xi32>
          %70 = llvm.shl %69, %11  : vector<2xi32>
          %71 = llvm.bitcast %70 : vector<2xi32> to vector<2xf32>
          %72 = llvm.fmul %67, %71  : vector<2xf32>
          %73 = llvm.icmp "sle" %68, %12 : vector<2xi32>
          %74 = llvm.icmp "sge" %68, %17 : vector<2xi32>
          %75 = llvm.fcmp "oeq" %55, %15 : vector<2xf32>
          %76 = llvm.fcmp "oeq" %55, %14 : vector<2xf32>
          %77 = llvm.fcmp "ogt" %55, %13 : vector<2xf32>
          %78 = llvm.and %73, %74  : vector<2xi1>
          %79 = llvm.select %77, %14, %16 : vector<2xi1>, vector<2xf32>
          %80 = llvm.select %78, %72, %79 : vector<2xi1>, vector<2xf32>
          %81 = llvm.select %76, %14, %80 : vector<2xi1>, vector<2xf32>
          %82 = llvm.select %75, %13, %81 : vector<2xi1>, vector<2xf32>
          %83 = llvm.select %56, %55, %82 : vector<2xi1>, vector<2xf32>
          %84 = "llvm.intr.vector.reduce.fadd"(%54, %83) <{reassoc = false}> : (f32, vector<2xf32>) -> f32
          %85 = llvm.insertelement %84, %18[%19 : i32] : vector<1xf32>
          %86 = llvm.extractelement %85[%1 : i64] : vector<1xf32>
          %87 = llvm.insertelement %86, %47[%0 : i32] : vector<1xf32>
          %88 = llvm.extractelement %87[%19 : i32] : vector<1xf32>
          llvm.store %88, %23 : f32, !llvm.ptr
          %89 = llvm.load %23 : !llvm.ptr -> f32
          %90 = llvm.insertelement %89, %51[%0 : i32] : vector<2xf32>
          %91 = llvm.shufflevector %90, %51 [0, 0] : vector<2xf32> 
          %92 = llvm.fdiv %83, %91  : vector<2xf32>
          llvm.store %92, %34 {alignment = 4 : i64} : vector<2xf32>, !llvm.ptr
          llvm.return %0 : i32
        }
        llvm.func @main_dispatch_2_generic_2_f32xi8(%arg0: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg1: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}, %arg2: !llvm.ptr {llvm.align = 16 : i64, llvm.noalias}) -> i32 {
          %0 = llvm.mlir.constant(0 : i32) : i32
          %1 = llvm.mlir.constant(63 : index) : i32
          %2 = llvm.mlir.constant(dense<1.000000e+00> : vector<2xf32>) : vector<2xf32>
          %3 = llvm.mlir.constant(dense<0> : vector<2xi32>) : vector<2xi32>
          %4 = llvm.mlir.constant(dense<1> : vector<2xi32>) : vector<2xi32>
          %5 = llvm.mlir.constant(dense<-1> : vector<2xi32>) : vector<2xi32>
          %6 = llvm.mlir.constant(dense<23> : vector<2xi32>) : vector<2xi32>
          %7 = llvm.mlir.constant(dense<31> : vector<2xi32>) : vector<2xi32>
          %8 = llvm.mlir.constant(dense<127> : vector<2xi32>) : vector<2xi32>
          %9 = llvm.mlir.constant(dense<4194304> : vector<2xi32>) : vector<2xi32>
          %10 = llvm.mlir.constant(dense<8388607> : vector<2xi32>) : vector<2xi32>
          %11 = llvm.mlir.constant(dense<255> : vector<2xi32>) : vector<2xi32>
          %12 = llvm.mlir.constant(dense<1.270000e+02> : vector<2xf32>) : vector<2xf32>
          %13 = llvm.mlir.constant(dense<-1.280000e+02> : vector<2xf32>) : vector<2xf32>
          %14 = llvm.mlir.constant(dense<2.560000e+02> : vector<2xf32>) : vector<2xf32>
          %15 = llvm.mlir.constant(0 : index) : i32
          %16 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %17 = llvm.extractvalue %16[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %18 = llvm.load %17 : !llvm.ptr -> !llvm.ptr
          %19 = llvm.getelementptr %18[16] : (!llvm.ptr) -> !llvm.ptr, f32
          %20 = llvm.ptrtoint %19 : !llvm.ptr to i32
          %21 = llvm.and %20, %1  : i32
          %22 = llvm.icmp "eq" %21, %15 : i32
          "llvm.intr.assume"(%22) : (i1) -> ()
          %23 = llvm.load %arg1 : !llvm.ptr -> !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)>
          %24 = llvm.extractvalue %23[10] : !llvm.struct<"iree_hal_executable_dispatch_state_v0_t", (i32, i32, i16, i16, i32, i32, i16, i8, i8, ptr, ptr, ptr)> 
          %25 = llvm.getelementptr %24[1] : (!llvm.ptr) -> !llvm.ptr, !llvm.ptr
          %26 = llvm.load %25 : !llvm.ptr -> !llvm.ptr
          %27 = llvm.ptrtoint %26 : !llvm.ptr to i32
          %28 = llvm.and %27, %1  : i32
          %29 = llvm.icmp "eq" %28, %15 : i32
          "llvm.intr.assume"(%29) : (i1) -> ()
          %30 = llvm.load %19 {alignment = 4 : i64} : !llvm.ptr -> vector<2xf32>
          %31 = llvm.fmul %30, %14  : vector<2xf32>
          %32 = llvm.fadd %31, %13  : vector<2xf32>
          %33 = llvm.bitcast %32 : vector<2xf32> to vector<2xi32>
          %34 = llvm.intr.round(%32)  : (vector<2xf32>) -> vector<2xf32>
          %35 = llvm.bitcast %34 : vector<2xf32> to vector<2xi32>
          %36 = llvm.lshr %33, %6  : vector<2xi32>
          %37 = llvm.and %36, %11  : vector<2xi32>
          %38 = llvm.sub %37, %8  : vector<2xi32>
          %39 = llvm.lshr %35, %6  : vector<2xi32>
          %40 = llvm.and %39, %11  : vector<2xi32>
          %41 = llvm.sub %40, %8  : vector<2xi32>
          %42 = llvm.icmp "eq" %41, %3 : vector<2xi32>
          %43 = llvm.sub %41, %4  : vector<2xi32>
          %44 = llvm.intr.smax(%43, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %45 = llvm.intr.smin(%44, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %46 = llvm.lshr %10, %45  : vector<2xi32>
          %47 = llvm.and %35, %46  : vector<2xi32>
          %48 = llvm.icmp "ne" %47, %3 : vector<2xi32>
          %49 = llvm.or %48, %42  : vector<2xi1>
          %50 = llvm.icmp "eq" %38, %5 : vector<2xi32>
          %51 = llvm.intr.smax(%38, %3)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %52 = llvm.intr.smin(%51, %7)  : (vector<2xi32>, vector<2xi32>) -> vector<2xi32>
          %53 = llvm.lshr %9, %52  : vector<2xi32>
          %54 = llvm.select %50, %3, %53 : vector<2xi1>, vector<2xi32>
          %55 = llvm.lshr %10, %52  : vector<2xi32>
          %56 = llvm.and %33, %55  : vector<2xi32>
          %57 = llvm.icmp "eq" %56, %54 : vector<2xi32>
          %58 = llvm.icmp "sge" %38, %5 : vector<2xi32>
          %59 = llvm.icmp "slt" %38, %6 : vector<2xi32>
          %60 = llvm.and %57, %59  : vector<2xi1>
          %61 = llvm.and %60, %58  : vector<2xi1>
          %62 = llvm.intr.copysign(%2, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %63 = llvm.fsub %34, %62  : vector<2xf32>
          %64 = llvm.and %49, %61  : vector<2xi1>
          %65 = llvm.select %64, %63, %34 : vector<2xi1>, vector<2xf32>
          %66 = llvm.intr.copysign(%65, %32)  : (vector<2xf32>, vector<2xf32>) -> vector<2xf32>
          %67 = llvm.fcmp "ult" %66, %12 : vector<2xf32>
          %68 = llvm.select %67, %66, %12 : vector<2xi1>, vector<2xf32>
          %69 = llvm.fcmp "uno" %12, %12 : vector<2xf32>
          %70 = llvm.select %69, %12, %68 : vector<2xi1>, vector<2xf32>
          %71 = llvm.fcmp "ugt" %70, %13 : vector<2xf32>
          %72 = llvm.select %71, %70, %13 : vector<2xi1>, vector<2xf32>
          %73 = llvm.fcmp "uno" %13, %13 : vector<2xf32>
          %74 = llvm.select %73, %13, %72 : vector<2xi1>, vector<2xf32>
          %75 = llvm.fptosi %74 : vector<2xf32> to vector<2xi8>
          llvm.store %75, %26 {alignment = 1 : i64} : vector<2xi8>, !llvm.ptr
          llvm.return %0 : i32
        }
      }
    }
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c1 = arith.constant 1 : index
    %c1_i32 = arith.constant 1 : i32
    %c268435464_i32 = arith.constant 268435464 : i32
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c128 = arith.constant 128 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::HAL::SerializeTargetExecutablesPass (iree-hal-serialize-target-executables) //----- //
hal.executable private @llvm_module_linked_llvm_cpu {
  hal.executable.binary public @static attributes {data = dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>, format = "static"}
}

// -----// IR Dump After mlir::iree_compiler::IREE::HAL::SerializeExecutablesPass (iree-hal-serialize-executables) //----- //
hal.executable private @llvm_module_linked_llvm_cpu {
  hal.executable.binary public @static attributes {data = dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>, format = "static"}
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %device_0 = hal.ex.shared_device : !hal.device
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device_0 : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %device_1 = hal.ex.shared_device : !hal.device
    %pipeline_layout = hal.pipeline_layout.create device(%device_1 : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %device_2 = hal.ex.shared_device : !hal.device
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device_2 : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    cf.br ^bb4
  ^bb4:  // pred: ^bb3
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.binary public @static attributes {data = dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>, format = "static"}
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c1 = arith.constant 1 : index
    %c1_i32 = arith.constant 1 : i32
    %c268435464_i32 = arith.constant 268435464 : i32
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c128 = arith.constant 128 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    cf.br ^bb4
  ^bb4:  // pred: ^bb3
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.binary public @static attributes {data = dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>, format = "static"}
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c1 = arith.constant 1 : index
    %c1_i32 = arith.constant 1 : i32
    %c268435464_i32 = arith.constant 268435464 : i32
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c128 = arith.constant 128 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.binary public @static attributes {data = dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>, format = "static"}
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c1 = arith.constant 1 : index
    %c1_i32 = arith.constant 1 : i32
    %c268435464_i32 = arith.constant 268435464 : i32
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c128 = arith.constant 128 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    util.global.store %value, @_device_query_0 : i1
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %_descriptor_set_layout_0 = util.global.load @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%_descriptor_set_layout_0]) : !hal.pipeline_layout
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    %_device_query_0 = util.global.load @_device_query_0 : i1
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.binary public @static attributes {data = dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>, format = "static"}
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c1 = arith.constant 1 : index
    %c1_i32 = arith.constant 1 : i32
    %c268435464_i32 = arith.constant 268435464 : i32
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c128 = arith.constant 128 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  %c1 = arith.constant 1 : index
  %c1_i32 = arith.constant 1 : i32
  %c268435464_i32 = arith.constant 268435464 : i32
  %c0 = arith.constant 0 : index
  %c2 = arith.constant 2 : index
  %c128 = arith.constant 128 : index
  %c-1_i64 = arith.constant -1 : i64
  %c0_i64 = arith.constant 0 : i64
  %c-1_i32 = arith.constant -1 : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
  %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_descriptor_set_layout_0 : !hal.descriptor_set_layout
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %descriptor_set_layout, @_descriptor_set_layout_0 : !hal.descriptor_set_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.binary public @static attributes {data = dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>, format = "static"}
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.binary public @static attributes {data = dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>, format = "static"}
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.binary public @static attributes {data = dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>, format = "static"}
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After IPO (iree-util-ipo) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], iree.fixedpoint.iteration = 0 : index} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.binary public @static attributes {data = dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>, format = "static"}
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FixedPointIterator (iree-util-fixed-point-iterator) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.binary public @static attributes {data = dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>, format = "static"}
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128 = arith.constant 128 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
  %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After Inliner (inline) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.binary public @static attributes {data = dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>, format = "static"}
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.binary public @static attributes {data = dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>, format = "static"}
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After SCFForLoopCanonicalization (scf-for-loop-canonicalization) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer.return
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SCFForLoopCanonicalization (scf-for-loop-canonicalization) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128 = arith.constant 128 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
  %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer.return
}

// -----// IR Dump After LoopCoalescing (affine-loop-coalescing) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128 = arith.constant 128 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
  %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer.return
}

// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128 = arith.constant 128 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
  %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ArithUnsignedWhenEquivalent (arith-unsigned-when-equivalent) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SCFToControlFlow (convert-scf-to-cf) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128 = arith.constant 128 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
  %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ConvertAffineToStandard (lower-affine) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128 = arith.constant 128 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
  %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ArithUnsignedWhenEquivalent (arith-unsigned-when-equivalent) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128 = arith.constant 128 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
  %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After PropagateSubranges (iree-util-propagate-subranges) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.binary public @static attributes {data = dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>, format = "static"}
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.binary public @static attributes {data = dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>, format = "static"}
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.binary public @static attributes {data = dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>, format = "static"}
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c-1_i32 = arith.constant -1 : i32
    %c0_i64 = arith.constant 0 : i64
    %c-1_i64 = arith.constant -1 : i64
    %c128 = arith.constant 128 : index
    %c2 = arith.constant 2 : index
    %c0 = arith.constant 0 : index
    %c268435464_i32 = arith.constant 268435464 : i32
    %c1_i32 = arith.constant 1 : i32
    %c1 = arith.constant 1 : index
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
util.initializer {
  %device = hal.ex.shared_device : !hal.device
  %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
  %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
  %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
  util.global.store %value, @_device_query_0 : i1
  util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
  cf.cond_br %value, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
  cf.br ^bb3(%exe : !hal.executable)
^bb2:  // pred: ^bb0
  %0 = util.null : !hal.executable
  cf.br ^bb3(%0 : !hal.executable)
^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
  util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer.return
}

// -----// IR Dump After SimplifyGlobalAccesses (iree-util-simplify-global-accesses) //----- //
func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
  %_device_query_0 = util.global.load @_device_query_0 : i1
  %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
  %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  %c-1_i32 = arith.constant -1 : i32
  %c0_i64 = arith.constant 0 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c128 = arith.constant 128 : index
  %c2 = arith.constant 2 : index
  %c0 = arith.constant 0 : index
  %c268435464_i32 = arith.constant 268435464 : i32
  %c1_i32 = arith.constant 1 : i32
  %c1 = arith.constant 1 : index
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
  %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
  %device = hal.ex.shared_device : !hal.device
  %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
  hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
  %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
  %0 = util.null : !hal.fence
  %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
  %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
  cf.cond_br %_device_query_0, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
    %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
    %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
  ])
  hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
  hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
  hal.command_buffer.finalize<%cmd : !hal.command_buffer>
  %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
  %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
  hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
  %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
  util.status.check_ok %status, "failed to wait on timepoint"
  %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
  return %view : !hal.buffer_view
^bb2:  // pred: ^bb0
  util.unreachable "device not supported in the compiled configuration"
}

// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.binary public @static attributes {data = dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>, format = "static"}
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c1 = arith.constant 1 : index
    %c1_i32 = arith.constant 1 : i32
    %c268435464_i32 = arith.constant 268435464 : i32
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c128 = arith.constant 128 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.binary public @static attributes {data = dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>, format = "static"}
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c1 = arith.constant 1 : index
    %c1_i32 = arith.constant 1 : i32
    %c268435464_i32 = arith.constant 268435464 : i32
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c128 = arith.constant 128 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu]} {
  util.global private @_device_query_0 : i1
  util.global private @_pipeline_layout_0 : !hal.pipeline_layout
  util.global private @_executable_llvm_module_linked_llvm_cpu : !hal.executable
  util.initializer {
    %device = hal.ex.shared_device : !hal.device
    %ok, %value = hal.device.query<%device : !hal.device> key("hal.executable.format" :: "static") : i1, i1 = false
    %descriptor_set_layout = hal.descriptor_set_layout.create device(%device : !hal.device) flags("None") bindings([#hal.descriptor_set.binding<0, storage_buffer, ReadOnly>, #hal.descriptor_set.binding<1, storage_buffer>]) : !hal.descriptor_set_layout
    %pipeline_layout = hal.pipeline_layout.create device(%device : !hal.device) push_constants(0) layouts([%descriptor_set_layout]) : !hal.pipeline_layout
    util.global.store %value, @_device_query_0 : i1
    util.global.store %pipeline_layout, @_pipeline_layout_0 : !hal.pipeline_layout
    cf.cond_br %value, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %exe = hal.executable.create device(%device : !hal.device) target(@llvm_module_linked_llvm_cpu::@static) layouts([%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) : !hal.executable
    cf.br ^bb3(%exe : !hal.executable)
  ^bb2:  // pred: ^bb0
    %0 = util.null : !hal.executable
    cf.br ^bb3(%0 : !hal.executable)
  ^bb3(%1: !hal.executable):  // 2 preds: ^bb1, ^bb2
    util.global.store %1, @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    util.initializer.return
  }
  hal.executable private @llvm_module_linked_llvm_cpu {
    hal.executable.binary public @static attributes {data = dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>, format = "static"}
  }
  func.func @main(%arg0: !hal.buffer_view {ml_program.identifier = "MobilenetV1/Logits/SpatialSqueeze"}) -> (!hal.buffer_view {ml_program.identifier = "MobilenetV1/Predictions/Reshape_1"}) attributes {iree.abi.stub} {
    %c1 = arith.constant 1 : index
    %c1_i32 = arith.constant 1 : i32
    %c268435464_i32 = arith.constant 268435464 : i32
    %c0 = arith.constant 0 : index
    %c2 = arith.constant 2 : index
    %c128 = arith.constant 128 : index
    %c-1_i64 = arith.constant -1 : i64
    %c0_i64 = arith.constant 0 : i64
    %c-1_i32 = arith.constant -1 : i32
    %_device_query_0 = util.global.load @_device_query_0 : i1
    %_pipeline_layout_0 = util.global.load @_pipeline_layout_0 : !hal.pipeline_layout
    %_executable_llvm_module_linked_llvm_cpu = util.global.load @_executable_llvm_module_linked_llvm_cpu : !hal.executable
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input 0") shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32)
    %buffer = hal.buffer_view.buffer<%arg0 : !hal.buffer_view> : !hal.buffer
    %device = hal.ex.shared_device : !hal.device
    %allocator = hal.device.allocator<%device : !hal.device> : !hal.allocator
    hal.buffer.assert<%buffer : !hal.buffer> message("tensor") allocator(%allocator : !hal.allocator) minimum_length(%c2) type(DeviceVisible) usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage")
    %buffer_0 = hal.allocator.allocate<%allocator : !hal.allocator> type("HostVisible|DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage|MappingScoped|MappingAccessRandom|Mapping") : !hal.buffer{%c2}
    %0 = util.null : !hal.fence
    %fence = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    %transient_buffer = hal.device.queue.alloca<%device : !hal.device> affinity(%c-1_i64) wait(%0) signal(%fence) pool(%c0_i64) type("DeviceVisible|DeviceLocal") usage("TransferSource|TransferTarget|Transfer|DispatchStorageRead|DispatchStorageWrite|DispatchStorage") : !hal.buffer{%c128}
    %cmd = hal.command_buffer.create device(%device : !hal.device) mode(OneShot) categories("Transfer|Dispatch") : !hal.command_buffer
    cf.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%buffer : !hal.buffer)[%c0, %c2], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[0] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%transient_buffer : !hal.buffer)[%c0, %c128]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[1] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.push_descriptor_set<%cmd : !hal.command_buffer> layout(%_pipeline_layout_0 : !hal.pipeline_layout)[%c0] bindings([
      %c0 = (%transient_buffer : !hal.buffer)[%c0, %c128], 
      %c1 = (%buffer_0 : !hal.buffer)[%c0, %c2]
    ])
    hal.command_buffer.dispatch<%cmd : !hal.command_buffer> target(%_executable_llvm_module_linked_llvm_cpu : !hal.executable)[2] workgroups([%c1, %c1, %c1])
    hal.command_buffer.execution_barrier<%cmd : !hal.command_buffer> source("Dispatch|Transfer|CommandRetire") target("CommandIssue|Dispatch|Transfer") flags("None")
    hal.command_buffer.finalize<%cmd : !hal.command_buffer>
    %fence_1 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.execute<%device : !hal.device> affinity(%c-1_i64) wait(%fence) signal(%fence_1) commands([%cmd])
    %fence_2 = hal.fence.create device(%device : !hal.device) flags("None") : !hal.fence
    hal.device.queue.dealloca<%device : !hal.device> affinity(%c-1_i64) wait(%fence_1) signal(%fence_2) buffer(%transient_buffer : !hal.buffer)
    %status = hal.fence.await until([%fence_2]) timeout_millis(%c-1_i32) : i32
    util.status.check_ok %status, "failed to wait on timepoint"
    %view = hal.buffer_view.create buffer(%buffer_0 : !hal.buffer)[%c0, %c2] shape([%c1, %c2]) type(%c268435464_i32) encoding(%c1_i32) : !hal.buffer_view
    return %view : !hal.buffer_view
  ^bb2:  // pred: ^bb0
    util.unreachable "device not supported in the compiled configuration"
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::ConversionPass (iree-vm-conversion) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
    vm.initializer {
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %buffer = vm.rodata.inline "_utf8_hal_executable_format_EAB228F999C2D3A1" {alignment = 1 : i64} : !vm.buffer = "hal.executable.format"
      %buffer_0 = vm.rodata.inline "_utf8_static_96B31E405495E0B6" {alignment = 1 : i64} : !vm.buffer = "static"
      %0:2 = vm.call @hal.device.query.i64(%ref, %buffer, %buffer_0) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %c1 = vm.const.i32 1
      %2 = vm.and.i32 %1, %c1 : i32
      %zero = vm.const.i32.zero
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %c1_1 = vm.const.i32 1
      %zero_2 = vm.const.i32.zero
      %zero_3 = vm.const.i32.zero
      %c7 = vm.const.i32 7
      %c1_4 = vm.const.i32 1
      %c1_5 = vm.const.i32 1
      %c7_6 = vm.const.i32 7
      %zero_7 = vm.const.i32.zero
      %ref_8 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_2, [(%zero_3, %c7, %c1_4), (%c1_5, %c7_6, %zero_7)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %zero_9 = vm.const.i32.zero
      %ref_10 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_9, [%ref_8]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_10, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
      %buffer_11 = vm.rodata.inline "_utf8_static_96B31E405495E0B6" {alignment = 1 : i64} : !vm.buffer = "static"
      %null = vm.const.ref.zero : !vm.buffer
      %ref_12 = vm.call.variadic @hal.executable.create(%ref, %buffer_11, %llvm_module_linked_llvm_cpu_static, %null, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_12 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      %null_13 = vm.const.ref.zero : !vm.ref<!hal.executable>
      vm.br ^bb3(%null_13 : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c1 = vm.const.i64 1
      %c1_0 = vm.const.i32 1
      %c268435464 = vm.const.i32 268435464
      %zero = vm.const.i64.zero
      %c2 = vm.const.i64 2
      %c128 = vm.const.i64 128
      %c-1 = vm.const.i64 -1
      %zero_1 = vm.const.i64.zero
      %c-1_2 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      %buffer = vm.rodata.inline "_utf8_input_0_5FD512E67BEFDEEC" {alignment = 1 : i64} : !vm.buffer = "input 0"
      vm.call.variadic @hal.buffer_view.assert(%arg0, %buffer, %c268435464, %c1_0, [%c1, %c2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_3 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_4 = vm.call @hal.device.allocator(%ref_3) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %buffer_5 = vm.rodata.inline "_utf8_tensor_3C6209B4FD120BDC" {alignment = 1 : i64} : !vm.buffer = "tensor"
      %c16 = vm.const.i32 16
      %c3075 = vm.const.i32 3075
      vm.call @hal.buffer.assert(%ref, %buffer_5, %ref_4, %c2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %c50 = vm.const.i32 50
      %c150998019 = vm.const.i32 150998019
      %ref_6 = vm.call @hal.allocator.allocate(%ref_4, %c50, %c150998019, %c2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %zero_7 = vm.const.i32.zero
      %ref_8 = vm.call @hal.fence.create(%ref_3, %zero_7) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %zero_9 = vm.const.i32.zero
      %c48 = vm.const.i32 48
      %c3075_10 = vm.const.i32 3075
      %ref_11 = vm.call @hal.device.queue.alloca(%ref_3, %c-1, %null, %ref_8, %zero_9, %c48, %c3075_10, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %c1_12 = vm.const.i32 1
      %c3 = vm.const.i32 3
      %zero_13 = vm.const.i32.zero
      %ref_14 = vm.call @hal.command_buffer.create(%ref_3, %c1_12, %c3, %zero_13) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %zero_15 = vm.const.i32.zero
      %zero_16 = vm.const.i32.zero
      %zero_17 = vm.const.i32.zero
      %c1_18 = vm.const.i32 1
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero_15, [(%zero_16, %zero_17, %ref, %zero, %c2), (%c1_18, %zero_17, %ref_11, %zero, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      %zero_19 = vm.const.i32.zero
      %c1_20 = vm.const.i32 1
      %c1_21 = vm.const.i32 1
      %c1_22 = vm.const.i32 1
      vm.call @hal.command_buffer.dispatch(%ref_14, %_executable_llvm_module_linked_llvm_cpu, %zero_19, %c1_20, %c1_21, %c1_22) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      %c28 = vm.const.i32 28
      %c13 = vm.const.i32 13
      %zero_23 = vm.const.i32.zero
      vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28, %c13, %zero_23) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      %zero_24 = vm.const.i32.zero
      %zero_25 = vm.const.i32.zero
      %zero_26 = vm.const.i32.zero
      %c1_27 = vm.const.i32 1
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero_24, [(%zero_25, %zero_26, %ref_11, %zero, %c128), (%c1_27, %zero_26, %ref_11, %zero, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      %c1_28 = vm.const.i32 1
      %c1_29 = vm.const.i32 1
      %c1_30 = vm.const.i32 1
      %c1_31 = vm.const.i32 1
      vm.call @hal.command_buffer.dispatch(%ref_14, %_executable_llvm_module_linked_llvm_cpu, %c1_28, %c1_29, %c1_30, %c1_31) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      %c28_32 = vm.const.i32 28
      %c13_33 = vm.const.i32 13
      %zero_34 = vm.const.i32.zero
      vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28_32, %c13_33, %zero_34) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      %zero_35 = vm.const.i32.zero
      %zero_36 = vm.const.i32.zero
      %zero_37 = vm.const.i32.zero
      %c1_38 = vm.const.i32 1
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_14, %_pipeline_layout_0, %zero_35, [(%zero_36, %zero_37, %ref_11, %zero, %c128), (%c1_38, %zero_37, %ref_6, %zero, %c2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      %c2_39 = vm.const.i32 2
      %c1_40 = vm.const.i32 1
      %c1_41 = vm.const.i32 1
      %c1_42 = vm.const.i32 1
      vm.call @hal.command_buffer.dispatch(%ref_14, %_executable_llvm_module_linked_llvm_cpu, %c2_39, %c1_40, %c1_41, %c1_42) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      %c28_43 = vm.const.i32 28
      %c13_44 = vm.const.i32 13
      %zero_45 = vm.const.i32.zero
      vm.call @hal.command_buffer.execution_barrier(%ref_14, %c28_43, %c13_44, %zero_45) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_14) : (!vm.ref<!hal.command_buffer>) -> ()
      %zero_46 = vm.const.i32.zero
      %ref_47 = vm.call @hal.fence.create(%ref_3, %zero_46) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_3, %c-1, %ref_8, %ref_47, [%ref_14]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %zero_48 = vm.const.i32.zero
      %ref_49 = vm.call @hal.fence.create(%ref_3, %zero_48) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call @hal.device.queue.dealloca(%ref_3, %c-1, %ref_47, %ref_49, %ref_11) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
      %0 = vm.call.variadic @hal.fence.await(%c-1_2, [%ref_49]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_fail %0, "failed to wait on timepoint"
      %ref_50 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero, %c2, %c268435464, %c1_0, [%c1, %c2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_50 : !vm.ref<!hal.buffer_view>
    ^bb2:  // pred: ^bb0
      %c2_51 = vm.const.i32 2
      vm.fail %c2_51, "device not supported in the compiled configuration"
    }
    vm.export @main attributes {iree.abi.stub}
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::HoistInlinedRodataPass (iree-vm-hoist-inlined-rodata) //----- //
vm.module public @module {
  vm.global.i32 private @_device_query_0 : i32
  vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
  vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
  vm.rodata private @_utf8_static_96B31E405495E0B6_0 {alignment = 1 : i64} "static"
  vm.initializer {
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %c1 = vm.const.i32 1
    %2 = vm.and.i32 %1, %c1 : i32
    %zero = vm.const.i32.zero
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %c1_0 = vm.const.i32 1
    %zero_1 = vm.const.i32.zero
    %zero_2 = vm.const.i32.zero
    %c7 = vm.const.i32 7
    %c1_3 = vm.const.i32 1
    %c1_4 = vm.const.i32 1
    %c7_5 = vm.const.i32 7
    %zero_6 = vm.const.i32.zero
    %ref_7 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_1, [(%zero_2, %c7, %c1_3), (%c1_4, %c7_5, %zero_6)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %zero_8 = vm.const.i32.zero
    %ref_9 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_8, [%ref_7]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_9, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
    %_utf8_static_96B31E405495E0B6_0 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6_0 : !vm.buffer
    %null = vm.const.ref.zero : !vm.buffer
    %ref_10 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6_0, %llvm_module_linked_llvm_cpu_static, %null, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_10 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    %null_11 = vm.const.ref.zero : !vm.ref<!hal.executable>
    vm.br ^bb3(%null_11 : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.return
  }
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
  vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
  vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
  vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
  vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
  vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
  vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
  vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
  vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
  vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c1 = vm.const.i64 1
    %c1_0 = vm.const.i32 1
    %c268435464 = vm.const.i32 268435464
    %zero = vm.const.i64.zero
    %c2 = vm.const.i64 2
    %c128 = vm.const.i64 128
    %c-1 = vm.const.i64 -1
    %zero_1 = vm.const.i64.zero
    %c-1_2 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_3 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_4 = vm.call @hal.device.allocator(%ref_3) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    %c16 = vm.const.i32 16
    %c3075 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_4, %c2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %c50 = vm.const.i32 50
    %c150998019 = vm.const.i32 150998019
    %ref_5 = vm.call @hal.allocator.allocate(%ref_4, %c50, %c150998019, %c2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %zero_6 = vm.const.i32.zero
    %ref_7 = vm.call @hal.fence.create(%ref_3, %zero_6) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %zero_8 = vm.const.i32.zero
    %c48 = vm.const.i32 48
    %c3075_9 = vm.const.i32 3075
    %ref_10 = vm.call @hal.device.queue.alloca(%ref_3, %c-1, %null, %ref_7, %zero_8, %c48, %c3075_9, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %c1_11 = vm.const.i32 1
    %c3 = vm.const.i32 3
    %zero_12 = vm.const.i32.zero
    %ref_13 = vm.call @hal.command_buffer.create(%ref_3, %c1_11, %c3, %zero_12) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %zero_14 = vm.const.i32.zero
    %zero_15 = vm.const.i32.zero
    %zero_16 = vm.const.i32.zero
    %c1_17 = vm.const.i32 1
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_13, %_pipeline_layout_0, %zero_14, [(%zero_15, %zero_16, %ref, %zero, %c2), (%c1_17, %zero_16, %ref_10, %zero, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    %zero_18 = vm.const.i32.zero
    %c1_19 = vm.const.i32 1
    %c1_20 = vm.const.i32 1
    %c1_21 = vm.const.i32 1
    vm.call @hal.command_buffer.dispatch(%ref_13, %_executable_llvm_module_linked_llvm_cpu, %zero_18, %c1_19, %c1_20, %c1_21) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    %c28 = vm.const.i32 28
    %c13 = vm.const.i32 13
    %zero_22 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref_13, %c28, %c13, %zero_22) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    %zero_23 = vm.const.i32.zero
    %zero_24 = vm.const.i32.zero
    %zero_25 = vm.const.i32.zero
    %c1_26 = vm.const.i32 1
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_13, %_pipeline_layout_0, %zero_23, [(%zero_24, %zero_25, %ref_10, %zero, %c128), (%c1_26, %zero_25, %ref_10, %zero, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    %c1_27 = vm.const.i32 1
    %c1_28 = vm.const.i32 1
    %c1_29 = vm.const.i32 1
    %c1_30 = vm.const.i32 1
    vm.call @hal.command_buffer.dispatch(%ref_13, %_executable_llvm_module_linked_llvm_cpu, %c1_27, %c1_28, %c1_29, %c1_30) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    %c28_31 = vm.const.i32 28
    %c13_32 = vm.const.i32 13
    %zero_33 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref_13, %c28_31, %c13_32, %zero_33) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    %zero_34 = vm.const.i32.zero
    %zero_35 = vm.const.i32.zero
    %zero_36 = vm.const.i32.zero
    %c1_37 = vm.const.i32 1
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_13, %_pipeline_layout_0, %zero_34, [(%zero_35, %zero_36, %ref_10, %zero, %c128), (%c1_37, %zero_36, %ref_5, %zero, %c2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    %c2_38 = vm.const.i32 2
    %c1_39 = vm.const.i32 1
    %c1_40 = vm.const.i32 1
    %c1_41 = vm.const.i32 1
    vm.call @hal.command_buffer.dispatch(%ref_13, %_executable_llvm_module_linked_llvm_cpu, %c2_38, %c1_39, %c1_40, %c1_41) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    %c28_42 = vm.const.i32 28
    %c13_43 = vm.const.i32 13
    %zero_44 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref_13, %c28_42, %c13_43, %zero_44) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_13) : (!vm.ref<!hal.command_buffer>) -> ()
    %zero_45 = vm.const.i32.zero
    %ref_46 = vm.call @hal.fence.create(%ref_3, %zero_45) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_3, %c-1, %ref_7, %ref_46, [%ref_13]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %zero_47 = vm.const.i32.zero
    %ref_48 = vm.call @hal.fence.create(%ref_3, %zero_47) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call @hal.device.queue.dealloca(%ref_3, %c-1, %ref_46, %ref_48, %ref_10) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
    %0 = vm.call.variadic @hal.fence.await(%c-1_2, [%ref_48]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_fail %0, "failed to wait on timepoint"
    %ref_49 = vm.call.variadic @hal.buffer_view.create(%ref_5, %zero, %c2, %c268435464, %c1_0, [%c1, %c2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_49 : !vm.ref<!hal.buffer_view>
  ^bb2:  // pred: ^bb0
    %c2_50 = vm.const.i32 2
    vm.fail %c2_50, "device not supported in the compiled configuration"
  }
  vm.export @main attributes {iree.abi.stub}
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::DeduplicateRodataPass (iree-vm-deduplicate-rodata) //----- //
vm.module public @module {
  vm.global.i32 private @_device_query_0 : i32
  vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
  vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
  vm.initializer {
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %c1 = vm.const.i32 1
    %2 = vm.and.i32 %1, %c1 : i32
    %zero = vm.const.i32.zero
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %c1_0 = vm.const.i32 1
    %zero_1 = vm.const.i32.zero
    %zero_2 = vm.const.i32.zero
    %c7 = vm.const.i32 7
    %c1_3 = vm.const.i32 1
    %c1_4 = vm.const.i32 1
    %c7_5 = vm.const.i32 7
    %zero_6 = vm.const.i32.zero
    %ref_7 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero_1, [(%zero_2, %c7, %c1_3), (%c1_4, %c7_5, %zero_6)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %zero_8 = vm.const.i32.zero
    %ref_9 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero_8, [%ref_7]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_9, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
    %_utf8_static_96B31E405495E0B6_10 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
    %null = vm.const.ref.zero : !vm.buffer
    %ref_11 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6_10, %llvm_module_linked_llvm_cpu_static, %null, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_11 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    %null_12 = vm.const.ref.zero : !vm.ref<!hal.executable>
    vm.br ^bb3(%null_12 : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.return
  }
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
  vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
  vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
  vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
  vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
  vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
  vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
  vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
  vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
  vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c1 = vm.const.i64 1
    %c1_0 = vm.const.i32 1
    %c268435464 = vm.const.i32 268435464
    %zero = vm.const.i64.zero
    %c2 = vm.const.i64 2
    %c128 = vm.const.i64 128
    %c-1 = vm.const.i64 -1
    %zero_1 = vm.const.i64.zero
    %c-1_2 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_3 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_4 = vm.call @hal.device.allocator(%ref_3) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    %c16 = vm.const.i32 16
    %c3075 = vm.const.i32 3075
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_4, %c2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %c50 = vm.const.i32 50
    %c150998019 = vm.const.i32 150998019
    %ref_5 = vm.call @hal.allocator.allocate(%ref_4, %c50, %c150998019, %c2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %zero_6 = vm.const.i32.zero
    %ref_7 = vm.call @hal.fence.create(%ref_3, %zero_6) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %zero_8 = vm.const.i32.zero
    %c48 = vm.const.i32 48
    %c3075_9 = vm.const.i32 3075
    %ref_10 = vm.call @hal.device.queue.alloca(%ref_3, %c-1, %null, %ref_7, %zero_8, %c48, %c3075_9, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %c1_11 = vm.const.i32 1
    %c3 = vm.const.i32 3
    %zero_12 = vm.const.i32.zero
    %ref_13 = vm.call @hal.command_buffer.create(%ref_3, %c1_11, %c3, %zero_12) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %zero_14 = vm.const.i32.zero
    %zero_15 = vm.const.i32.zero
    %zero_16 = vm.const.i32.zero
    %c1_17 = vm.const.i32 1
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_13, %_pipeline_layout_0, %zero_14, [(%zero_15, %zero_16, %ref, %zero, %c2), (%c1_17, %zero_16, %ref_10, %zero, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    %zero_18 = vm.const.i32.zero
    %c1_19 = vm.const.i32 1
    %c1_20 = vm.const.i32 1
    %c1_21 = vm.const.i32 1
    vm.call @hal.command_buffer.dispatch(%ref_13, %_executable_llvm_module_linked_llvm_cpu, %zero_18, %c1_19, %c1_20, %c1_21) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    %c28 = vm.const.i32 28
    %c13 = vm.const.i32 13
    %zero_22 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref_13, %c28, %c13, %zero_22) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    %zero_23 = vm.const.i32.zero
    %zero_24 = vm.const.i32.zero
    %zero_25 = vm.const.i32.zero
    %c1_26 = vm.const.i32 1
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_13, %_pipeline_layout_0, %zero_23, [(%zero_24, %zero_25, %ref_10, %zero, %c128), (%c1_26, %zero_25, %ref_10, %zero, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    %c1_27 = vm.const.i32 1
    %c1_28 = vm.const.i32 1
    %c1_29 = vm.const.i32 1
    %c1_30 = vm.const.i32 1
    vm.call @hal.command_buffer.dispatch(%ref_13, %_executable_llvm_module_linked_llvm_cpu, %c1_27, %c1_28, %c1_29, %c1_30) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    %c28_31 = vm.const.i32 28
    %c13_32 = vm.const.i32 13
    %zero_33 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref_13, %c28_31, %c13_32, %zero_33) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    %zero_34 = vm.const.i32.zero
    %zero_35 = vm.const.i32.zero
    %zero_36 = vm.const.i32.zero
    %c1_37 = vm.const.i32 1
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_13, %_pipeline_layout_0, %zero_34, [(%zero_35, %zero_36, %ref_10, %zero, %c128), (%c1_37, %zero_36, %ref_5, %zero, %c2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    %c2_38 = vm.const.i32 2
    %c1_39 = vm.const.i32 1
    %c1_40 = vm.const.i32 1
    %c1_41 = vm.const.i32 1
    vm.call @hal.command_buffer.dispatch(%ref_13, %_executable_llvm_module_linked_llvm_cpu, %c2_38, %c1_39, %c1_40, %c1_41) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    %c28_42 = vm.const.i32 28
    %c13_43 = vm.const.i32 13
    %zero_44 = vm.const.i32.zero
    vm.call @hal.command_buffer.execution_barrier(%ref_13, %c28_42, %c13_43, %zero_44) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_13) : (!vm.ref<!hal.command_buffer>) -> ()
    %zero_45 = vm.const.i32.zero
    %ref_46 = vm.call @hal.fence.create(%ref_3, %zero_45) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_3, %c-1, %ref_7, %ref_46, [%ref_13]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %zero_47 = vm.const.i32.zero
    %ref_48 = vm.call @hal.fence.create(%ref_3, %zero_47) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call @hal.device.queue.dealloca(%ref_3, %c-1, %ref_46, %ref_48, %ref_10) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
    %0 = vm.call.variadic @hal.fence.await(%c-1_2, [%ref_48]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_fail %0, "failed to wait on timepoint"
    %ref_49 = vm.call.variadic @hal.buffer_view.create(%ref_5, %zero, %c2, %c268435464, %c1_0, [%c1, %c2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_49 : !vm.ref<!hal.buffer_view>
  ^bb2:  // pred: ^bb0
    %c2_50 = vm.const.i32 2
    vm.fail %c2_50, "device not supported in the compiled configuration"
  }
  vm.export @main attributes {iree.abi.stub}
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
      %_utf8_static_96B31E405495E0B6_3 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
      %ref_4 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6_3, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_4 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i64 1
      %c1_0 = vm.const.i32 1
      %c268435464 = vm.const.i32 268435464
      %zero_1 = vm.const.i64.zero
      %c2_2 = vm.const.i64 2
      %c128 = vm.const.i64 128
      %c-1 = vm.const.i64 -1
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3(%0 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb3(%1: i32):  // pred: ^bb1
      vm.fail %1, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @main attributes {iree.abi.stub}
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i64 1
      %c1_0 = vm.const.i32 1
      %c268435464 = vm.const.i32 268435464
      %zero_1 = vm.const.i64.zero
      %c2_2 = vm.const.i64 2
      %c128 = vm.const.i64 128
      %c-1 = vm.const.i64 -1
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3(%0 : i32), ^bb2
    ^bb2:  // pred: ^bb1
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb3(%1: i32):  // pred: ^bb1
      vm.fail %1, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @main attributes {iree.abi.stub}
  }
}


// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i64 1
      %c1_0 = vm.const.i32 1
      %c268435464 = vm.const.i32 268435464
      %zero_1 = vm.const.i64.zero
      %c2_2 = vm.const.i64 2
      %c128 = vm.const.i64 128
      %c-1 = vm.const.i64 -1
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @main attributes {iree.abi.stub}
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i64 1
      %c1_0 = vm.const.i32 1
      %c268435464 = vm.const.i32 268435464
      %zero_1 = vm.const.i64.zero
      %c2_2 = vm.const.i64 2
      %c128 = vm.const.i64 128
      %c-1 = vm.const.i64 -1
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @main attributes {iree.abi.stub}
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i64 1
      %c1_0 = vm.const.i32 1
      %c268435464 = vm.const.i32 268435464
      %zero_1 = vm.const.i64.zero
      %c2_2 = vm.const.i64 2
      %c128 = vm.const.i64 128
      %c-1 = vm.const.i64 -1
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @main attributes {iree.abi.stub}
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::ResolveRodataLoadsPass (iree-vm-resolve-rodata-loads) //----- //
vm.module public @module {
  vm.global.i32 private @_device_query_0 : i32
  vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
  vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
  vm.initializer {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.return
  }
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
  vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
  vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
  vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
  vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
  vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
  vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
  vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
  vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
  vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
  vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
  vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
  vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
  vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i64 1
    %c1_0 = vm.const.i32 1
    %c268435464 = vm.const.i32 268435464
    %zero_1 = vm.const.i64.zero
    %c2_2 = vm.const.i64 2
    %c128 = vm.const.i64 128
    %c-1 = vm.const.i64 -1
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @main attributes {iree.abi.stub}
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.initializer {
  %null = vm.const.ref.zero : !vm.ref<!hal.executable>
  %null_0 = vm.const.ref.zero : !vm.buffer
  %c7 = vm.const.i32 7
  %zero = vm.const.i32.zero
  %c1 = vm.const.i32 1
  %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
  %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
  %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
  %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
  %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
  %2 = vm.and.i32 %1, %c1 : i32
  %3 = vm.select.i32 %0#0, %2, %zero : i32
  %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
  %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
  vm.global.store.i32 %3, @_device_query_0 : i32
  vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.cond_br %3, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
  %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
  vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
^bb2:  // pred: ^bb0
  vm.br ^bb3(%null : !vm.ref<!hal.executable>)
^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
  vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
  vm.return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
  %c2 = vm.const.i32 2
  %c13 = vm.const.i32 13
  %c28 = vm.const.i32 28
  %c3 = vm.const.i32 3
  %c48 = vm.const.i32 48
  %zero = vm.const.i32.zero
  %null = vm.const.ref.zero : !vm.ref<!hal.fence>
  %c150998019 = vm.const.i32 150998019
  %c50 = vm.const.i32 50
  %c3075 = vm.const.i32 3075
  %c16 = vm.const.i32 16
  %c1 = vm.const.i64 1
  %c1_0 = vm.const.i32 1
  %c268435464 = vm.const.i32 268435464
  %zero_1 = vm.const.i64.zero
  %c2_2 = vm.const.i64 2
  %c128 = vm.const.i64 128
  %c-1 = vm.const.i64 -1
  %c-1_3 = vm.const.i32 -1
  %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
  %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
  vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
  %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
  %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
  vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
  %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
  %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
  vm.cond_br %_device_query_0, ^bb1, ^bb4
^bb1:  // pred: ^bb0
  vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
  %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
  %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
  %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.cond_br %0, ^bb3, ^bb2
^bb2:  // pred: ^bb1
  %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
  vm.return %ref_12 : !vm.ref<!hal.buffer_view>
^bb3:  // pred: ^bb1
  vm.fail %0, "failed to wait on timepoint"
^bb4:  // pred: ^bb0
  vm.fail %c2, "device not supported in the compiled configuration"
}

// -----// IR Dump After Inliner (inline) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.allocator.allocate.initialized(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.allocator.map.byte_buffer(%allocator : !vm.ref<!hal.allocator>, %try : i32, %memory_types : i32, %buffer_usage : i32, %source : !vm.buffer, %offset : i64, %length : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer.subspan(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i64) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer.length(%buffer : !vm.ref<!hal.buffer>) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer.load(%source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %length : i32) -> i32
    vm.import private @hal.buffer.store(%value : i32, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.buffer_view.element_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.encoding_type(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.rank(%buffer_view : !vm.ref<!hal.buffer_view>) -> i32 attributes {nosideeffects}
    vm.import private @hal.buffer_view.dim(%buffer_view : !vm.ref<!hal.buffer_view>, %index : i32) -> i64 attributes {nosideeffects}
    vm.import private @hal.buffer_view.trace(%key : !vm.buffer, %operands : !vm.ref<!hal.buffer_view> ...)
    vm.import private @hal.channel.create(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %flags : i32, %id : !vm.buffer, %group : !vm.buffer, %rank : i32, %count : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.split(%channel : !vm.ref<!hal.channel>, %color : i32, %key : i32, %flags : i32) -> !vm.ref<!hal.channel> attributes {nosideeffects}
    vm.import private @hal.channel.rank_and_count(%channel : !vm.ref<!hal.channel>) -> (i32, i32) attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.begin_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>, %label : !vm.buffer)
    vm.import private @hal.command_buffer.end_debug_group(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.fill_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64, %pattern : i32, %pattern_length : i32)
    vm.import private @hal.command_buffer.copy_buffer(%command_buffer : !vm.ref<!hal.command_buffer>, %source_buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %target_buffer : !vm.ref<!hal.buffer>, %target_offset : i64, %length : i64)
    vm.import private @hal.command_buffer.collective(%command_buffer : !vm.ref<!hal.command_buffer>, %channel : !vm.ref<!hal.channel>, %op : i32, %param : i32, %send_buffer : !vm.ref<!hal.buffer>, %send_offset : i64, %send_length : i64, %recv_buffer : !vm.ref<!hal.buffer>, %recv_offset : i64, %recv_length : i64, %element_count : i64)
    vm.import private @hal.command_buffer.push_constants(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %offset : i32, %values : i32 ...)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.command_buffer.dispatch.indirect(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroups_buffer : !vm.ref<!hal.buffer>, %workgroups_offset : i64)
    vm.import private @hal.command_buffer.execute.commands(%command_buffer : !vm.ref<!hal.command_buffer>, %commands : !vm.ref<!hal.command_buffer>, %bindings : tuple<!vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.device.queue.flush(%device : !vm.ref<!hal.device>, %queue_affinity : i64)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.join(%fences : !vm.ref<!hal.fence> ...) -> !vm.ref<!hal.fence> attributes {nosideeffects}
    vm.import private @hal.fence.query(%fence : !vm.ref<!hal.fence>) -> i32
    vm.import private @hal.fence.signal(%fence : !vm.ref<!hal.fence>)
    vm.import private @hal.fence.fail(%fence : !vm.ref<!hal.fence>, %status : i32)
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i64 1
      %c1_0 = vm.const.i32 1
      %c268435464 = vm.const.i32 268435464
      %zero_1 = vm.const.i64.zero
      %c2_2 = vm.const.i64 2
      %c128 = vm.const.i64 128
      %c-1 = vm.const.i64 -1
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @main attributes {iree.abi.stub}
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i64 1
      %c1_0 = vm.const.i32 1
      %c268435464 = vm.const.i32 268435464
      %zero_1 = vm.const.i64.zero
      %c2_2 = vm.const.i64 2
      %c128 = vm.const.i64 128
      %c-1 = vm.const.i64 -1
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @main attributes {iree.abi.stub}
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i64 1
      %c1_0 = vm.const.i32 1
      %c268435464 = vm.const.i32 268435464
      %zero_1 = vm.const.i64.zero
      %c2_2 = vm.const.i64 2
      %c128 = vm.const.i64 128
      %c-1 = vm.const.i64 -1
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @main attributes {iree.abi.stub}
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i64 1
      %c1_0 = vm.const.i32 1
      %c268435464 = vm.const.i32 268435464
      %zero_1 = vm.const.i64.zero
      %c2_2 = vm.const.i64 2
      %c128 = vm.const.i64 128
      %c-1 = vm.const.i64 -1
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @main attributes {iree.abi.stub}
  }
}


// -----// IR Dump After ApplyPatterns (iree-util-apply-patterns) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i64 1
      %c1_0 = vm.const.i32 1
      %c268435464 = vm.const.i32 268435464
      %zero_1 = vm.const.i64.zero
      %c2_2 = vm.const.i64 2
      %c128 = vm.const.i64 128
      %c-1 = vm.const.i64 -1
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @main attributes {iree.abi.stub}
  }
}


// -----// IR Dump After FoldGlobals (iree-util-fold-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i64 1
      %c1_0 = vm.const.i32 1
      %c268435464 = vm.const.i32 268435464
      %zero_1 = vm.const.i64.zero
      %c2_2 = vm.const.i64 2
      %c128 = vm.const.i64 128
      %c-1 = vm.const.i64 -1
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @main attributes {iree.abi.stub}
  }
}


// -----// IR Dump After FuseGlobals (iree-util-fuse-globals) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private @_device_query_0 : i32
    vm.global.ref private @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
    vm.initializer {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      vm.return
    }
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i64 1
      %c1_0 = vm.const.i32 1
      %c268435464 = vm.const.i32 268435464
      %zero_1 = vm.const.i64.zero
      %c2_2 = vm.const.i64 2
      %c128 = vm.const.i64 128
      %c-1 = vm.const.i64 -1
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @main attributes {iree.abi.stub}
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::GlobalInitializationPass (iree-vm-global-initialization) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
  vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i64 1
    %c1_0 = vm.const.i32 1
    %c268435464 = vm.const.i32 268435464
    %zero_1 = vm.const.i64.zero
    %c2_2 = vm.const.i64 2
    %c128 = vm.const.i64 128
    %c-1 = vm.const.i64 -1
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @main attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.br ^bb4
  ^bb4:  // pred: ^bb3
    vm.return
  }
  vm.export @__deinit
  vm.func private @__deinit() {
    vm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i64 1
      %c1_0 = vm.const.i32 1
      %c268435464 = vm.const.i32 268435464
      %zero_1 = vm.const.i64.zero
      %c2_2 = vm.const.i64 2
      %c128 = vm.const.i64 128
      %c-1 = vm.const.i64 -1
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @main attributes {iree.abi.stub}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      vm.return
    }
    vm.export @__deinit
    vm.func private @__deinit() {
      vm.return
    }
  }
}


// -----// IR Dump After CSE (cse) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i64 1
      %c1_0 = vm.const.i32 1
      %c268435464 = vm.const.i32 268435464
      %zero_1 = vm.const.i64.zero
      %c2_2 = vm.const.i64 2
      %c128 = vm.const.i64 128
      %c-1 = vm.const.i64 -1
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @main attributes {iree.abi.stub}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      vm.return
    }
    vm.export @__deinit
    vm.func private @__deinit() {
      vm.return
    }
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i64 1
      %c1_0 = vm.const.i32 1
      %c268435464 = vm.const.i32 268435464
      %zero_1 = vm.const.i64.zero
      %c2_2 = vm.const.i64 2
      %c128 = vm.const.i64 128
      %c-1 = vm.const.i64 -1
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @main attributes {iree.abi.stub}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      vm.return
    }
    vm.export @__deinit
    vm.func private @__deinit() {
      vm.return
    }
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::DropEmptyModuleInitializersPass (iree-vm-drop-empty-module-initializers) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
  vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i64 1
    %c1_0 = vm.const.i32 1
    %c268435464 = vm.const.i32 268435464
    %zero_1 = vm.const.i64.zero
    %c2_2 = vm.const.i64 2
    %c128 = vm.const.i64 128
    %c-1 = vm.const.i64 -1
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @main attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After DropCompilerHints (iree-util-drop-compiler-hints) //----- //
#executable_target_static = #hal.executable.target<"llvm-cpu", "static", {cpu = "generic-rv32", cpu_features = "+m", data_layout = "e-m:e-p:32:32-i64:64-n32-S128", native_vector_size = 16 : index, target_triple = "riscv32-pc-linux-elf", ukernels = false}>
#device_target_llvm_cpu = #hal.device.target<"llvm-cpu", {executable_targets = [#executable_target_static]}>
module attributes {hal.device.targets = [#device_target_llvm_cpu], vm.toplevel} {
  vm.module public @module {
    vm.global.i32 private mutable @_device_query_0 : i32
    vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.global.ref private mutable @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
    vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
    vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
    vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
    vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
    vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
    vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
    vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
    vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
    vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
    vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
    vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
    vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
    vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
    vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
    vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
    vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
    vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
    vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
    vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
    vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
    vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
    vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
    vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
    vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
      %c2 = vm.const.i32 2
      %c13 = vm.const.i32 13
      %c28 = vm.const.i32 28
      %c3 = vm.const.i32 3
      %c48 = vm.const.i32 48
      %zero = vm.const.i32.zero
      %null = vm.const.ref.zero : !vm.ref<!hal.fence>
      %c150998019 = vm.const.i32 150998019
      %c50 = vm.const.i32 50
      %c3075 = vm.const.i32 3075
      %c16 = vm.const.i32 16
      %c1 = vm.const.i64 1
      %c1_0 = vm.const.i32 1
      %c268435464 = vm.const.i32 268435464
      %zero_1 = vm.const.i64.zero
      %c2_2 = vm.const.i64 2
      %c128 = vm.const.i64 128
      %c-1 = vm.const.i64 -1
      %c-1_3 = vm.const.i32 -1
      %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
      vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
      %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
      %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
      %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
      vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
      %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
      %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
      vm.cond_br %_device_query_0, ^bb1, ^bb4
    ^bb1:  // pred: ^bb0
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
      vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
      vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
      %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
      %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
      vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
      %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
      vm.cond_br %0, ^bb3, ^bb2
    ^bb2:  // pred: ^bb1
      %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
      vm.return %ref_12 : !vm.ref<!hal.buffer_view>
    ^bb3:  // pred: ^bb1
      vm.fail %0, "failed to wait on timepoint"
    ^bb4:  // pred: ^bb0
      vm.fail %c2, "device not supported in the compiled configuration"
    }
    vm.export @main attributes {iree.abi.stub}
    vm.export @__init
    vm.func private @__init() {
      %null = vm.const.ref.zero : !vm.ref<!hal.executable>
      %null_0 = vm.const.ref.zero : !vm.buffer
      %c7 = vm.const.i32 7
      %zero = vm.const.i32.zero
      %c1 = vm.const.i32 1
      %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
      %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
      %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
      %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
      %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
      %2 = vm.and.i32 %1, %c1 : i32
      %3 = vm.select.i32 %0#0, %2, %zero : i32
      %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
      %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
      vm.global.store.i32 %3, @_device_query_0 : i32
      vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      vm.cond_br %3, ^bb1, ^bb2
    ^bb1:  // pred: ^bb0
      %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
      %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
      %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
      vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
    ^bb2:  // pred: ^bb0
      vm.br ^bb3(%null : !vm.ref<!hal.executable>)
    ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
      vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
      vm.return
    }
  }
}


// -----// IR Dump After mlir::iree_compiler::IREE::VM::GlobalInitializationPass (iree-vm-global-initialization) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
  vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i64 1
    %c1_0 = vm.const.i32 1
    %c268435464 = vm.const.i32 268435464
    %zero_1 = vm.const.i64.zero
    %c2_2 = vm.const.i64 2
    %c128 = vm.const.i64 128
    %c-1 = vm.const.i64 -1
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @main attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.br ^bb4
  ^bb4:  // pred: ^bb3
    vm.return
  }
  vm.export @__deinit
  vm.func private @__deinit() {
    vm.return
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::DropEmptyModuleInitializersPass (iree-vm-drop-empty-module-initializers) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
  vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i64 1
    %c1_0 = vm.const.i32 1
    %c268435464 = vm.const.i32 268435464
    %zero_1 = vm.const.i64.zero
    %c2_2 = vm.const.i64 2
    %c128 = vm.const.i64 128
    %c-1 = vm.const.i64 -1
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @main attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.br ^bb4
  ^bb4:  // pred: ^bb3
    vm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
  %c2 = vm.const.i32 2
  %c13 = vm.const.i32 13
  %c28 = vm.const.i32 28
  %c3 = vm.const.i32 3
  %c48 = vm.const.i32 48
  %zero = vm.const.i32.zero
  %null = vm.const.ref.zero : !vm.ref<!hal.fence>
  %c150998019 = vm.const.i32 150998019
  %c50 = vm.const.i32 50
  %c3075 = vm.const.i32 3075
  %c16 = vm.const.i32 16
  %c1 = vm.const.i64 1
  %c1_0 = vm.const.i32 1
  %c268435464 = vm.const.i32 268435464
  %zero_1 = vm.const.i64.zero
  %c2_2 = vm.const.i64 2
  %c128 = vm.const.i64 128
  %c-1 = vm.const.i64 -1
  %c-1_3 = vm.const.i32 -1
  %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
  %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
  vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
  %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
  %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
  %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
  %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
  vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
  %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
  %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
  %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
  vm.cond_br %_device_query_0, ^bb1, ^bb4
^bb1:  // pred: ^bb0
  vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
  vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
  %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
  %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
  vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
  %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
  vm.cond_br %0, ^bb3, ^bb2
^bb2:  // pred: ^bb1
  %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
  vm.return %ref_12 : !vm.ref<!hal.buffer_view>
^bb3:  // pred: ^bb1
  vm.fail %0, "failed to wait on timepoint"
^bb4:  // pred: ^bb0
  vm.fail %c2, "device not supported in the compiled configuration"
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.func private @__init() {
  %null = vm.const.ref.zero : !vm.ref<!hal.executable>
  %null_0 = vm.const.ref.zero : !vm.buffer
  %c7 = vm.const.i32 7
  %zero = vm.const.i32.zero
  %c1 = vm.const.i32 1
  %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
  %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
  %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
  %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
  %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
  %2 = vm.and.i32 %1, %c1 : i32
  %3 = vm.select.i32 %0#0, %2, %zero : i32
  %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
  %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
  vm.global.store.i32 %3, @_device_query_0 : i32
  vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.cond_br %3, ^bb1, ^bb2
^bb1:  // pred: ^bb0
  %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
  %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
  vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
^bb2:  // pred: ^bb0
  vm.br ^bb3(%null : !vm.ref<!hal.executable>)
^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
  vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
  vm.return
}

// -----// IR Dump After Inliner (inline) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
  vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i64 1
    %c1_0 = vm.const.i32 1
    %c268435464 = vm.const.i32 268435464
    %zero_1 = vm.const.i64.zero
    %c2_2 = vm.const.i64 2
    %c128 = vm.const.i64 128
    %c-1 = vm.const.i64 -1
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @main attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After CSE (cse) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
  vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i64 1
    %c1_0 = vm.const.i32 1
    %c268435464 = vm.const.i32 268435464
    %zero_1 = vm.const.i64.zero
    %c2_2 = vm.const.i64 2
    %c128 = vm.const.i64 128
    %c-1 = vm.const.i64 -1
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @main attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
  vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i64 1
    %c1_0 = vm.const.i32 1
    %c268435464 = vm.const.i32 268435464
    %zero_1 = vm.const.i64.zero
    %c2_2 = vm.const.i64 2
    %c128 = vm.const.i64 128
    %c-1 = vm.const.i64 -1
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @main attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After DropCompilerHints (iree-util-drop-compiler-hints) //----- //
vm.module public @module {
  vm.global.i32 private mutable @_device_query_0 : i32
  vm.global.ref private mutable @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
  vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64} "hal.executable.format"
  vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64} "static"
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects}
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32)
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...)
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer>
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>)
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32)
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32)
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer>
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>)
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...)
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence>
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects}
  vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64} "input 0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64} "tensor"
  vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> {
    %c2 = vm.const.i32 2
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i64 1
    %c1_0 = vm.const.i32 1
    %c268435464 = vm.const.i32 268435464
    %zero_1 = vm.const.i64.zero
    %c2_2 = vm.const.i64 2
    %c128 = vm.const.i64 128
    %c-1 = vm.const.i64 -1
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @main attributes {iree.abi.stub}
  vm.export @__init
  vm.func private @__init() {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.return
  }
}

// -----// IR Dump After mlir::iree_compiler::IREE::VM::OrdinalAllocationPass (iree-vm-ordinal-allocation) //----- //
vm.module public @module attributes {ordinal_counts = #vm.ordinal_counts<import_funcs = 21, export_funcs = 2, internal_funcs = 2, global_bytes = 4, global_refs = 2, rodatas = 5, rwdatas = 0>} {
  vm.global.i32 private mutable @_device_query_0 {ordinal = 0 : i32} : i32
  vm.global.ref private mutable @_pipeline_layout_0 {ordinal = 0 : i32} : !vm.ref<!hal.pipeline_layout>
  vm.global.ref private mutable @_executable_llvm_module_linked_llvm_cpu {ordinal = 1 : i32} : !vm.ref<!hal.executable>
  vm.rodata private @llvm_module_linked_llvm_cpu_static {alignment = 16 : i64, ordinal = 0 : i32} dense<[108, 108, 118, 109, 95, 109, 111, 100, 117, 108, 101, 95, 108, 105, 110, 107, 101, 100, 95, 108, 108, 118, 109, 95, 99, 112, 117]> : vector<27xi8>
  vm.rodata private @_utf8_hal_executable_format_EAB228F999C2D3A1 {alignment = 1 : i64, ordinal = 1 : i32} "hal.executable.format"
  vm.rodata private @_utf8_static_96B31E405495E0B6 {alignment = 1 : i64, ordinal = 2 : i32} "static"
  vm.import private @hal.ex.shared_device() -> !vm.ref<!hal.device> attributes {nosideeffects, ordinal = 0 : i32}
  vm.import private @hal.allocator.allocate(%allocator : !vm.ref<!hal.allocator>, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {ordinal = 1 : i32}
  vm.import private @hal.buffer.assert(%buffer : !vm.ref<!hal.buffer>, %message : !vm.buffer, %allocator : !vm.ref<!hal.allocator>, %minimum_length : i64, %memory_types : i32, %buffer_usage : i32) attributes {ordinal = 2 : i32}
  vm.import private @hal.buffer_view.create(%buffer : !vm.ref<!hal.buffer>, %source_offset : i64, %source_length : i64, %element_type : i32, %encoding_type : i32, %shape : i64 ...) -> !vm.ref<!hal.buffer_view> attributes {nosideeffects, ordinal = 3 : i32}
  vm.import private @hal.buffer_view.assert(%buffer_view : !vm.ref<!hal.buffer_view>, %message : !vm.buffer, %element_type : i32, %encoding_type : i32, %shape : i64 ...) attributes {ordinal = 4 : i32}
  vm.import private @hal.buffer_view.buffer(%buffer_view : !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer> attributes {nosideeffects, ordinal = 5 : i32}
  vm.import private @hal.command_buffer.create(%device : !vm.ref<!hal.device>, %modes : i32, %command_categories : i32, %binding_capacity : i32) -> !vm.ref<!hal.command_buffer> attributes {ordinal = 6 : i32}
  vm.import private @hal.command_buffer.finalize(%command_buffer : !vm.ref<!hal.command_buffer>) attributes {ordinal = 7 : i32}
  vm.import private @hal.command_buffer.execution_barrier(%command_buffer : !vm.ref<!hal.command_buffer>, %source_stage_mask : i32, %target_stage_mask : i32, %flags : i32) attributes {ordinal = 8 : i32}
  vm.import private @hal.command_buffer.push_descriptor_set(%command_buffer : !vm.ref<!hal.command_buffer>, %pipeline_layout : !vm.ref<!hal.pipeline_layout>, %set : i32, %bindings : tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...) attributes {ordinal = 9 : i32}
  vm.import private @hal.command_buffer.dispatch(%command_buffer : !vm.ref<!hal.command_buffer>, %executable : !vm.ref<!hal.executable>, %entry_point : i32, %workgroup_x : i32, %workgroup_y : i32, %workgroup_z : i32) attributes {ordinal = 10 : i32}
  vm.import private @hal.descriptor_set_layout.create(%device : !vm.ref<!hal.device>, %flags : i32, %bindings : tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout> attributes {nosideeffects, ordinal = 11 : i32}
  vm.import private @hal.device.allocator(%device : !vm.ref<!hal.device>) -> !vm.ref<!hal.allocator> attributes {nosideeffects, ordinal = 12 : i32}
  vm.import private @hal.device.query.i64(%device : !vm.ref<!hal.device>, %category : !vm.buffer, %key : !vm.buffer) -> (i32, i64) attributes {nosideeffects, ordinal = 13 : i32}
  vm.import private @hal.device.queue.alloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %pool : i32, %memory_types : i32, %buffer_usage : i32, %allocation_size : i64) -> !vm.ref<!hal.buffer> attributes {ordinal = 14 : i32}
  vm.import private @hal.device.queue.dealloca(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %buffer : !vm.ref<!hal.buffer>) attributes {ordinal = 15 : i32}
  vm.import private @hal.device.queue.execute(%device : !vm.ref<!hal.device>, %queue_affinity : i64, %wait_fence : !vm.ref<!hal.fence>, %signal_fence : !vm.ref<!hal.fence>, %command_buffers : !vm.ref<!hal.command_buffer> ...) attributes {ordinal = 16 : i32}
  vm.import private @hal.executable.create(%device : !vm.ref<!hal.device>, %executable_format : !vm.buffer, %executable_data : !vm.buffer, %constants : !vm.buffer, %pipeline_layouts : !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable> attributes {nosideeffects, ordinal = 17 : i32}
  vm.import private @hal.fence.create(%device : !vm.ref<!hal.device>, %flags : i32) -> !vm.ref<!hal.fence> attributes {ordinal = 18 : i32}
  vm.import private @hal.fence.await(%timeout_millis : i32, %fences : !vm.ref<!hal.fence> ...) -> i32 attributes {ordinal = 19 : i32, vm.yield}
  vm.import private @hal.pipeline_layout.create(%device : !vm.ref<!hal.device>, %push_constants : i32, %set_layouts : !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout> attributes {nosideeffects, ordinal = 20 : i32}
  vm.rodata private @_utf8_input_0_5FD512E67BEFDEEC {alignment = 1 : i64, ordinal = 3 : i32} "input 0"
  vm.rodata private @_utf8_tensor_3C6209B4FD120BDC {alignment = 1 : i64, ordinal = 4 : i32} "tensor"
  vm.func private @main(%arg0: !vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer_view> attributes {ordinal = 0 : i32} {
    %c2 = vm.const.i32 2
    %c13 = vm.const.i32 13
    %c28 = vm.const.i32 28
    %c3 = vm.const.i32 3
    %c48 = vm.const.i32 48
    %zero = vm.const.i32.zero
    %null = vm.const.ref.zero : !vm.ref<!hal.fence>
    %c150998019 = vm.const.i32 150998019
    %c50 = vm.const.i32 50
    %c3075 = vm.const.i32 3075
    %c16 = vm.const.i32 16
    %c1 = vm.const.i64 1
    %c1_0 = vm.const.i32 1
    %c268435464 = vm.const.i32 268435464
    %zero_1 = vm.const.i64.zero
    %c2_2 = vm.const.i64 2
    %c128 = vm.const.i64 128
    %c-1 = vm.const.i64 -1
    %c-1_3 = vm.const.i32 -1
    %_device_query_0 = vm.global.load.i32 @_device_query_0 : i32
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %_executable_llvm_module_linked_llvm_cpu = vm.global.load.ref @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    %_utf8_input_0_5FD512E67BEFDEEC = vm.const.ref.rodata @_utf8_input_0_5FD512E67BEFDEEC : !vm.buffer
    vm.call.variadic @hal.buffer_view.assert(%arg0, %_utf8_input_0_5FD512E67BEFDEEC, %c268435464, %c1_0, [%c1, %c2_2]) : (!vm.ref<!hal.buffer_view>, !vm.buffer, i32, i32, i64 ...)
    %ref = vm.call @hal.buffer_view.buffer(%arg0) {nosideeffects} : (!vm.ref<!hal.buffer_view>) -> !vm.ref<!hal.buffer>
    %ref_4 = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %ref_5 = vm.call @hal.device.allocator(%ref_4) {nosideeffects} : (!vm.ref<!hal.device>) -> !vm.ref<!hal.allocator>
    %_utf8_tensor_3C6209B4FD120BDC = vm.const.ref.rodata @_utf8_tensor_3C6209B4FD120BDC : !vm.buffer
    vm.call @hal.buffer.assert(%ref, %_utf8_tensor_3C6209B4FD120BDC, %ref_5, %c2_2, %c16, %c3075) : (!vm.ref<!hal.buffer>, !vm.buffer, !vm.ref<!hal.allocator>, i64, i32, i32) -> ()
    %ref_6 = vm.call @hal.allocator.allocate(%ref_5, %c50, %c150998019, %c2_2) : (!vm.ref<!hal.allocator>, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_7 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    %ref_8 = vm.call @hal.device.queue.alloca(%ref_4, %c-1, %null, %ref_7, %zero, %c48, %c3075, %c128) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, i32, i32, i32, i64) -> !vm.ref<!hal.buffer>
    %ref_9 = vm.call @hal.command_buffer.create(%ref_4, %c1_0, %c3, %zero) : (!vm.ref<!hal.device>, i32, i32, i32) -> !vm.ref<!hal.command_buffer>
    vm.cond_br %_device_query_0, ^bb1, ^bb4
  ^bb1:  // pred: ^bb0
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref, %zero_1, %c2_2), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %zero, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_8, %zero_1, %c128)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c1_0, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call.variadic @hal.command_buffer.push_descriptor_set(%ref_9, %_pipeline_layout_0, %zero, [(%zero, %zero, %ref_8, %zero_1, %c128), (%c1_0, %zero, %ref_6, %zero_1, %c2_2)]) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.pipeline_layout>, i32, tuple<i32, i32, !vm.ref<!hal.buffer>, i64, i64> ...)
    vm.call @hal.command_buffer.dispatch(%ref_9, %_executable_llvm_module_linked_llvm_cpu, %c2, %c1_0, %c1_0, %c1_0) : (!vm.ref<!hal.command_buffer>, !vm.ref<!hal.executable>, i32, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.execution_barrier(%ref_9, %c28, %c13, %zero) : (!vm.ref<!hal.command_buffer>, i32, i32, i32) -> ()
    vm.call @hal.command_buffer.finalize(%ref_9) : (!vm.ref<!hal.command_buffer>) -> ()
    %ref_10 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call.variadic @hal.device.queue.execute(%ref_4, %c-1, %ref_7, %ref_10, [%ref_9]) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.command_buffer> ...)
    %ref_11 = vm.call @hal.fence.create(%ref_4, %zero) : (!vm.ref<!hal.device>, i32) -> !vm.ref<!hal.fence>
    vm.call @hal.device.queue.dealloca(%ref_4, %c-1, %ref_10, %ref_11, %ref_8) : (!vm.ref<!hal.device>, i64, !vm.ref<!hal.fence>, !vm.ref<!hal.fence>, !vm.ref<!hal.buffer>) -> ()
    %0 = vm.call.variadic @hal.fence.await(%c-1_3, [%ref_11]) : (i32, !vm.ref<!hal.fence> ...) -> i32
    vm.cond_br %0, ^bb3, ^bb2
  ^bb2:  // pred: ^bb1
    %ref_12 = vm.call.variadic @hal.buffer_view.create(%ref_6, %zero_1, %c2_2, %c268435464, %c1_0, [%c1, %c2_2]) {nosideeffects} : (!vm.ref<!hal.buffer>, i64, i64, i32, i32, i64 ...) -> !vm.ref<!hal.buffer_view>
    vm.return %ref_12 : !vm.ref<!hal.buffer_view>
  ^bb3:  // pred: ^bb1
    vm.fail %0, "failed to wait on timepoint"
  ^bb4:  // pred: ^bb0
    vm.fail %c2, "device not supported in the compiled configuration"
  }
  vm.export @main attributes {iree.abi.stub, ordinal = 0 : i32}
  vm.export @__init attributes {ordinal = 1 : i32}
  vm.func private @__init() attributes {ordinal = 1 : i32} {
    %null = vm.const.ref.zero : !vm.ref<!hal.executable>
    %null_0 = vm.const.ref.zero : !vm.buffer
    %c7 = vm.const.i32 7
    %zero = vm.const.i32.zero
    %c1 = vm.const.i32 1
    %ref = vm.call @hal.ex.shared_device() {nosideeffects} : () -> !vm.ref<!hal.device>
    %_utf8_hal_executable_format_EAB228F999C2D3A1 = vm.const.ref.rodata @_utf8_hal_executable_format_EAB228F999C2D3A1 : !vm.buffer
    %_utf8_static_96B31E405495E0B6 = vm.const.ref.rodata @_utf8_static_96B31E405495E0B6 : !vm.buffer
    %0:2 = vm.call @hal.device.query.i64(%ref, %_utf8_hal_executable_format_EAB228F999C2D3A1, %_utf8_static_96B31E405495E0B6) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer) -> (i32, i64)
    %1 = vm.trunc.i64.i32 %0#1 : i64 -> i32
    %2 = vm.and.i32 %1, %c1 : i32
    %3 = vm.select.i32 %0#0, %2, %zero : i32
    %ref_1 = vm.call.variadic @hal.descriptor_set_layout.create(%ref, %zero, [(%zero, %c7, %c1), (%c1, %c7, %zero)]) {nosideeffects} : (!vm.ref<!hal.device>, i32, tuple<i32, i32, i32> ...) -> !vm.ref<!hal.descriptor_set_layout>
    %ref_2 = vm.call.variadic @hal.pipeline_layout.create(%ref, %zero, [%ref_1]) {nosideeffects} : (!vm.ref<!hal.device>, i32, !vm.ref<!hal.descriptor_set_layout> ...) -> !vm.ref<!hal.pipeline_layout>
    vm.global.store.i32 %3, @_device_query_0 : i32
    vm.global.store.ref %ref_2, @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    vm.cond_br %3, ^bb1, ^bb2
  ^bb1:  // pred: ^bb0
    %_pipeline_layout_0 = vm.global.load.ref @_pipeline_layout_0 : !vm.ref<!hal.pipeline_layout>
    %llvm_module_linked_llvm_cpu_static = vm.const.ref.rodata @llvm_module_linked_llvm_cpu_static : !vm.buffer
    %ref_3 = vm.call.variadic @hal.executable.create(%ref, %_utf8_static_96B31E405495E0B6, %llvm_module_linked_llvm_cpu_static, %null_0, [%_pipeline_layout_0, %_pipeline_layout_0, %_pipeline_layout_0]) {nosideeffects} : (!vm.ref<!hal.device>, !vm.buffer, !vm.buffer, !vm.buffer, !vm.ref<!hal.pipeline_layout> ...) -> !vm.ref<!hal.executable>
    vm.br ^bb3(%ref_3 : !vm.ref<!hal.executable>)
  ^bb2:  // pred: ^bb0
    vm.br ^bb3(%null : !vm.ref<!hal.executable>)
  ^bb3(%4: !vm.ref<!hal.executable>):  // 2 preds: ^bb1, ^bb2
    vm.global.store.ref %4, @_executable_llvm_module_linked_llvm_cpu : !vm.ref<!hal.executable>
    vm.return
  }
}

